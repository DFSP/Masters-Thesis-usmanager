[2020-07-30 21:16:23,119] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:16:23,130] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:16:23,143] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:16:23,177] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:16:23,501] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:16:23,502] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:23,502] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:23,502] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:23,502] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:23,503] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:23,504] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:16:23,757] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:16:23,757] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:23,758] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:16:23,938] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:16:23,939] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:23,939] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,804] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:16:24,804] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,804] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,804] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,804] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,804] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,805] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,806] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,807] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,807] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,807] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,807] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,807] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,807] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,807] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,808] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,809] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,809] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,809] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,809] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:16:24,810] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,810] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,810] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,811] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,811] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,811] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,811] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,811] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,811] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,811] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,812] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,813] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,814] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,814] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,814] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,814] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:16:24,814] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,814] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:16:24,814] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:16:24,815] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:16:24,815] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:16:24,815] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,815] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,815] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:16:24,826] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:16:24,828] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:16:24,832] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:16:24,883] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:16:24,883] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:16:24,883] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:16:24,883] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:16:24,884] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:16:24,884] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:16:24,884] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:16:24,884] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:16:24,884] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:16:24,884] INFO Kafka startTimeMs: 1596140184884 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:16:25,148] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:16:25,170] INFO Logging initialized @2712ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:16:25,246] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:16:25,247] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:16:25,255] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:16:25,288] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:16:25,288] INFO Started @2829ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:16:25,315] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:16:25,315] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:16:25,315] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:16:25,316] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:16:25,316] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:16:25,316] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:16:25,330] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:16:25,331] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:16:25,331] INFO Kafka startTimeMs: 1596140185330 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:16:25,502] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:16:25,504] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:16:25,515] INFO Kafka Connect standalone worker initialization took 2394ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:16:25,516] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:16:25,517] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:16:25,517] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:16:25,518] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:16:25,536] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:16:25,536] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:16:25,536] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:16:25,593] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:16:25,688] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:16:25,688] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:16:25,690] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:16:26,101] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:16:26,101] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:16:26,101] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:16:26,122] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:16:26,200] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:16:26,207] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:16:26,208] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:16:26,208] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:16:26,211] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:16:26,212] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:16:26,212] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:16:26,213] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:16:26,217] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:16:26,217] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:16:26,218] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:16:26,219] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:16:26,235] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:16:26,238] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:16:26,239] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:16:26,241] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:16:26,242] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:16:26,243] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:16:26,243] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:16:26,244] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:16:26,244] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:16:26,244] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:16:26,250] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:16:26,257] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:16:26,278] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:16:26,278] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:16:26,279] INFO Kafka startTimeMs: 1596140186278 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:16:26,288] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:16:26,289] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:16:26,289] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:16:26,289] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:16:26,293] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:16:28,055] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:16:28,056] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:16:28,056] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:16:28,056] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:16:28,057] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:16:28,057] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:16:28,057] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:16:28,072] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:16:28,072] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:16:28,073] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:16:28,074] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:16:28,074] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:16:28,075] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:16:28,076] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:16:28,077] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:16:28,077] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:16:28,077] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:16:28,077] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:16:28,077] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:16:28,078] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:16:28,079] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:16:28,079] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:16:28,079] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:16:28,088] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:16:28,132] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:16:28,132] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:16:28,132] INFO Kafka startTimeMs: 1596140188132 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:16:28,137] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:16:28,139] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:16:28,139] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:16:28,139] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:16:28,143] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:16:28,144] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:16:28,162] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:16:28,163] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:16:28,165] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:16:28,173] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:16:28,174] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:16:28,179] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-24ad1a86-497d-47e4-a82b-ebedd9903d49=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:16:28,183] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:16:28,187] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:16:28,195] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:16:28,206] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:16:28,275] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:16:28,275] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:16:28,276] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,303] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:16:28,311] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,331] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,333] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,334] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,335] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,340] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,348] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,349] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,350] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,351] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,352] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,352] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,354] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,355] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,355] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,360] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,363] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,364] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,365] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,370] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,374] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,374] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,375] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,376] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,380] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,384] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,386] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,391] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,392] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 21:16:28,398] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,416] INFO Checking Generic dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:548)
[2020-07-30 21:16:28,421] INFO Using Generic dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:556)
[2020-07-30 21:16:28,422] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: null (STRING) type doesn't have a mapping to the SQL database column type (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: null (STRING) type doesn't have a mapping to the SQL database column type
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getSqlType(GenericDatabaseDialect.java:1818)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnSpec(GenericDatabaseDialect.java:1734)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.lambda$writeColumnsSpec$33(GenericDatabaseDialect.java:1723)
	at io.confluent.connect.jdbc.util.ExpressionBuilder.append(ExpressionBuilder.java:558)
	at io.confluent.connect.jdbc.util.ExpressionBuilder$BasicListBuilder.of(ExpressionBuilder.java:597)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnsSpec(GenericDatabaseDialect.java:1725)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.buildCreateTableStatement(GenericDatabaseDialect.java:1648)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:92)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:16:28,446] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,463] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: null (STRING) type doesn't have a mapping to the SQL database column type
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getSqlType(GenericDatabaseDialect.java:1818)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnSpec(GenericDatabaseDialect.java:1734)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.lambda$writeColumnsSpec$33(GenericDatabaseDialect.java:1723)
	at io.confluent.connect.jdbc.util.ExpressionBuilder.append(ExpressionBuilder.java:558)
	at io.confluent.connect.jdbc.util.ExpressionBuilder$BasicListBuilder.of(ExpressionBuilder.java:597)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnsSpec(GenericDatabaseDialect.java:1725)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.buildCreateTableStatement(GenericDatabaseDialect.java:1648)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:92)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
[2020-07-30 21:16:28,464] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 21:16:28,464] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 21:16:28,464] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:16:28,464] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,465] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 21:16:28,465] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-24ad1a86-497d-47e4-a82b-ebedd9903d49 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 21:16:28,465] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,533] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,534] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,538] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,539] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,539] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,568] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,576] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,577] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,578] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,578] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,579] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:28,583] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:16:36,288] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:16:36,289] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:16:36,297] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:16:46,298] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:16:46,299] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:16:46,303] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:16:56,304] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:16:56,304] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:16:56,308] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:17:06,309] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:17:06,309] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:17:06,312] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:17:16,313] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:17:16,313] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:17:16,316] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:17:26,317] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:17:26,318] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:17:26,321] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:17:36,322] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:17:36,322] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:17:36,326] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:17:46,327] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:17:46,328] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:17:46,331] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:17:56,331] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:17:56,332] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:17:56,335] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:18:06,336] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:18:06,337] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:18:06,339] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:18:16,340] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:18:16,341] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:18:16,344] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:18:26,344] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:18:26,345] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:18:26,347] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:18:36,979] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:18:36,980] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:18:36,982] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:18:46,983] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:18:46,985] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:18:46,987] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:18:56,988] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:18:56,989] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:18:56,991] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:19:06,992] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:19:06,993] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:19:06,996] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:19:16,997] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:19:16,997] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:19:17,000] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:19:27,001] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:19:27,001] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:19:27,003] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:19:37,004] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:19:37,004] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:19:37,006] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:19:47,007] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:19:47,008] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:19:47,010] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:19:57,011] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:19:57,011] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:19:57,014] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:19:58,195] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:19:58,196] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:19:58,203] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:19:58,203] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:19:58,205] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:19:58,205] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:19:58,205] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:19:58,207] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:19:58,208] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:19:58,208] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:19:58,208] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:19:58,230] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:19:58,231] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:19:58,231] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:19:58,231] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:19:58,232] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:19:58,234] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:19:58,234] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:19:58,234] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:19:58,234] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:19:58,235] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:19:58,235] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:19:58,236] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:19:58,236] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:19:58,236] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:19:58,236] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 21:20:03,736] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:20:03,744] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:20:03,749] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:20:03,772] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:20:04,048] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:20:04,048] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:04,049] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:04,049] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:04,049] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:04,049] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:04,050] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:20:04,254] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:20:04,255] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:04,255] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:20:04,408] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:20:04,408] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:04,409] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,257] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:20:05,257] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,257] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,258] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,259] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,260] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,261] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,262] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:20:05,262] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,262] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,263] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,263] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,263] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,263] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,263] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,263] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,263] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,264] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,265] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,266] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,266] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,266] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,266] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,266] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,266] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:20:05,266] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,266] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:20:05,267] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:20:05,267] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:20:05,267] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:20:05,267] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,267] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,267] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:20:05,278] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:20:05,279] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:20:05,283] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:20:05,334] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:20:05,334] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:20:05,334] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:20:05,334] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:20:05,335] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:20:05,335] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:20:05,335] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:20:05,335] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:20:05,335] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:20:05,335] INFO Kafka startTimeMs: 1596140405335 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:20:05,647] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:20:05,663] INFO Logging initialized @2222ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:20:05,709] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:20:05,710] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:20:05,716] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:20:05,737] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:20:05,737] INFO Started @2297ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:20:05,756] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:20:05,756] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:20:05,756] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:20:05,756] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:20:05,756] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:20:05,757] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:20:05,766] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:20:05,766] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:20:05,767] INFO Kafka startTimeMs: 1596140405766 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:20:05,895] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:20:05,896] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:20:05,902] INFO Kafka Connect standalone worker initialization took 2164ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:20:05,903] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:20:05,903] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:20:05,904] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:20:05,904] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:20:05,912] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:20:05,912] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:20:05,912] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:20:05,952] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:20:06,017] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:20:06,018] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:20:06,019] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:20:06,524] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:20:06,525] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:20:06,525] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:20:06,545] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:20:06,642] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:20:06,648] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:20:06,649] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:20:06,649] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:20:06,652] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:20:06,652] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:20:06,653] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:20:06,654] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:20:06,658] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:20:06,658] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:20:06,659] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:20:06,660] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:20:06,670] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:20:06,672] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:20:06,672] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:20:06,673] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:20:06,675] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:20:06,676] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:20:06,677] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:20:06,677] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:20:06,677] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:20:06,678] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:20:06,682] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:20:06,688] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:20:06,710] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:20:06,711] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:20:06,711] INFO Kafka startTimeMs: 1596140406710 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:20:06,719] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:20:06,719] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:20:06,720] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:20:06,720] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:20:06,721] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:20:06,722] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:20:06,726] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:20:06,726] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:20:06,727] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:20:06,727] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:20:06,727] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:20:06,727] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:20:06,728] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:20:06,728] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:20:06,729] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:20:06,729] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:20:06,730] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:20:06,731] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:20:06,731] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:20:06,731] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:20:06,732] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:20:06,732] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:20:06,732] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:20:06,732] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:20:06,732] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:20:06,733] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:20:06,734] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:20:06,734] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:20:06,743] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:20:06,784] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:20:06,784] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:20:06,785] INFO Kafka startTimeMs: 1596140406784 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:20:06,791] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:20:06,792] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:20:06,793] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:20:06,793] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:20:06,796] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:20:06,797] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:20:06,814] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:20:06,815] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:20:06,817] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:20:06,824] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:20:06,824] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:20:06,828] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 3: {connector-consumer-jdbc-sink-0-7e25d838-90ca-4f2c-b16c-7509489fc0b2=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:20:06,831] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:20:06,835] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:20:06,844] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:20:06,854] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:20:06,979] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:20:06,979] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:20:06,980] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:06,983] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:20:07,008] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,016] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,018] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,019] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,019] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,022] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,027] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,028] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,029] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,030] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,030] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,030] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,031] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,032] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,032] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,036] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,040] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,041] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,041] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,045] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,047] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,047] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,048] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,048] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,051] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,055] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,056] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,060] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,064] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,076] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,078] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,078] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,083] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 21:20:07,085] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,086] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,089] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,090] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,091] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,094] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,097] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,097] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,098] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,099] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,099] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,102] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:20:07,108] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 21:20:07,112] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 21:20:07,113] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 21:20:07,122] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,183] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,185] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,186] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,186] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,187] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:10 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,189] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,207] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,208] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,209] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,209] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,209] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:9 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,210] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,224] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,225] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,225] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,225] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,226] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:8 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,226] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,239] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,240] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,240] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,240] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,241] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:7 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,241] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,255] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,256] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,257] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,257] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,257] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:6 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,258] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,265] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,266] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,267] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,267] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,267] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:5 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,268] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,275] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,276] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,277] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,277] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,277] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:4 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,278] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,286] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,287] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,287] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,287] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,288] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:3 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,288] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,296] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,297] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,298] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,298] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,298] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:2 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,298] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,307] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,308] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,309] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,309] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,309] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:1 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,309] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:20:07,317] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:20:07,318] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:20:07,319] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:20:07,319] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:20:07,319] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] maxRetries:0 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:20:07,320] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}] (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:20:07,321] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "USERNAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:20:07,322] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 21:20:07,322] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 21:20:07,322] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:20:07,322] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 21:20:07,323] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-7e25d838-90ca-4f2c-b16c-7509489fc0b2 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 21:20:16,719] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:20:16,719] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:20:16,727] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:20:26,728] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:20:26,728] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:20:26,733] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:20:36,733] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:20:36,734] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:20:36,737] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:20:46,738] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:20:46,738] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:20:46,741] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:20:56,741] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:20:56,742] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:20:56,746] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:21:06,747] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:21:06,747] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:21:06,750] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:21:16,751] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:21:16,751] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:21:16,754] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:21:26,754] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:21:26,756] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:21:26,760] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:21:36,761] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:21:36,761] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:21:36,764] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:21:46,765] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:21:46,765] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:21:46,768] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:21:50,515] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:21:50,515] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:21:50,526] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:21:50,526] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:21:50,527] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:21:50,528] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:21:50,528] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:21:50,531] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:21:50,532] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:21:50,532] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:21:50,532] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:21:50,621] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:21:50,621] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:21:50,623] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:21:50,623] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:21:50,624] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:21:50,624] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:21:50,628] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:21:50,628] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:21:50,628] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:21:50,628] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:21:50,629] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:21:50,630] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:21:50,630] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:21:50,631] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:21:50,631] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:21:50,631] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 21:21:52,884] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:21:52,897] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:21:52,910] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:21:52,946] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:21:53,234] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:21:53,235] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:53,235] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:53,235] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:53,236] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:53,236] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:53,237] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:21:53,435] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:21:53,435] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:53,435] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:21:53,576] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:21:53,577] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:53,578] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,410] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,411] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,412] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,413] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,414] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,415] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,415] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:21:54,415] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,415] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,416] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,416] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,416] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,416] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,416] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,416] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,416] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,417] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,418] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,418] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,418] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,418] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,418] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,418] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,418] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,419] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,419] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,419] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,419] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,419] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,419] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:21:54,419] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,420] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:21:54,420] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:21:54,421] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:21:54,421] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:21:54,421] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,421] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,421] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:21:54,432] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:21:54,433] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:21:54,436] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:21:54,482] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:21:54,482] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:21:54,482] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:21:54,482] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:21:54,483] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:21:54,483] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:21:54,483] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:21:54,483] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:21:54,483] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:21:54,483] INFO Kafka startTimeMs: 1596140514483 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:21:54,718] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:21:54,734] INFO Logging initialized @2510ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:21:54,788] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:21:54,788] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:21:54,794] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:21:54,817] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:21:54,817] INFO Started @2593ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:21:54,832] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:21:54,832] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:21:54,832] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:21:54,832] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:21:54,833] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:21:54,833] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:21:54,841] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:21:54,841] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:21:54,841] INFO Kafka startTimeMs: 1596140514841 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:21:54,949] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:21:54,950] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:21:54,960] INFO Kafka Connect standalone worker initialization took 2072ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:21:54,960] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:21:54,961] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:21:54,961] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:21:54,962] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:21:54,971] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:21:54,971] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:21:54,972] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:21:55,025] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:21:55,101] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:21:55,101] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:21:55,102] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:21:55,541] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:21:55,542] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:21:55,542] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:21:55,559] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:21:55,639] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:21:55,645] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:21:55,646] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:21:55,646] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:21:55,649] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:21:55,649] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:21:55,650] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:21:55,651] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:21:55,654] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:21:55,655] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:21:55,655] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:21:55,656] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:21:55,667] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:21:55,669] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:21:55,670] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:21:55,671] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:21:55,672] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:21:55,673] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:21:55,673] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:21:55,674] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:21:55,674] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:21:55,674] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:21:55,678] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:21:55,684] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:21:55,707] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:21:55,707] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:21:55,707] INFO Kafka startTimeMs: 1596140515707 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:21:55,714] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:21:55,714] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:21:55,715] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:21:55,715] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:21:55,716] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:21:55,716] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:21:55,720] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:21:55,721] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:21:55,721] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:21:55,721] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:21:55,722] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:21:55,723] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:21:55,723] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:21:55,723] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:21:55,724] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:21:55,725] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:21:55,725] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:21:55,726] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:21:55,726] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:21:55,727] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:21:55,727] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:21:55,727] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:21:55,727] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:21:55,728] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:21:55,728] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:21:55,729] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:21:55,729] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:21:55,729] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:21:55,739] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:21:55,775] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:21:55,775] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:21:55,775] INFO Kafka startTimeMs: 1596140515774 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:21:55,781] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:21:55,782] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:21:55,783] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:21:55,783] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:21:55,786] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:21:55,787] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:21:55,802] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:21:55,803] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:21:55,806] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:21:55,813] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:21:55,814] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:21:55,818] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 5: {connector-consumer-jdbc-sink-0-26c768db-6a3a-4eae-9ce8-d2fca3ec5f52=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:21:55,822] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:21:55,826] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:21:55,833] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:21:55,845] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:21:55,939] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:21:55,940] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:21:55,941] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:21:55,945] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:55,983] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:55,992] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:55,994] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:55,995] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:55,996] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,004] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,009] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,011] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,013] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,013] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,014] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,014] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,015] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,016] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,017] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,021] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,033] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,034] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,035] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,038] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 21:21:56,041] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,043] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,044] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,045] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,046] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,049] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,052] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,054] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,062] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 21:21:56,062] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,068] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 21:21:56,070] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 21:21:56,071] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,080] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,081] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,085] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,086] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,095] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,096] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,099] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,099] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,100] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,103] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,107] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,107] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,108] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,109] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,110] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,114] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:21:56,144] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,145] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,146] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,147] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,148] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:10 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,149] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,167] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,168] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,169] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,169] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,169] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:9 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,169] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,184] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,186] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,186] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,186] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,186] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:8 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,187] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,199] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,200] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,200] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,200] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,201] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:7 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,201] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,212] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,213] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,213] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,214] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,214] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:6 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,214] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,226] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,227] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,227] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,227] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,228] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:5 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,228] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,236] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,236] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,237] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,237] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,237] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:4 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,237] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,248] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,249] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,249] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,250] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,250] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:3 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,250] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,260] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,261] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,261] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,261] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,261] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:2 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,261] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,269] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,270] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,270] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,270] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,270] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:1 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,270] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:21:56,278] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:21:56,279] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:21:56,280] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:21:56,280] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:21:56,280] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:0 with SQL: [ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:21:56,281] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:21:56,282] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""PASSWORD"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""ROLE"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "PASSWORD" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:21:56,283] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 21:21:56,284] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 21:21:56,284] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:21:56,284] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 21:21:56,285] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-26c768db-6a3a-4eae-9ce8-d2fca3ec5f52 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 21:22:05,714] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:22:05,715] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:22:05,724] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 9 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:22:13,943] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:22:13,943] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:22:13,950] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:22:13,951] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:22:13,953] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:22:13,953] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:22:13,953] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:22:13,955] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:22:13,955] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:22:13,956] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:22:13,956] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:22:14,025] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:22:14,025] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:22:14,025] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:22:14,025] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:22:14,027] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:22:14,027] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:22:14,030] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:22:14,030] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:22:14,030] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:22:14,030] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:22:14,031] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:22:14,031] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:22:14,032] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:22:14,032] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:22:14,033] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:22:14,033] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 21:22:15,492] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:22:15,497] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:22:15,502] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:22:15,522] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:22:15,792] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:22:15,793] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:15,793] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:15,794] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:15,794] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:15,794] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:15,795] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:22:15,977] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:22:15,978] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:15,978] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:22:16,115] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:22:16,115] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,116] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,965] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:22:16,966] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,966] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,966] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,966] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,966] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,966] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,967] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,967] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,967] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,967] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,967] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,967] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,968] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,968] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,968] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,968] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,968] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,968] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,968] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,969] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,969] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,969] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,969] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,969] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,969] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,970] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,970] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,970] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,970] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,970] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,970] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,970] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,971] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,971] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,971] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,971] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,971] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,971] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,971] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,972] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,972] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,972] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,972] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:22:16,973] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,973] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,973] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,974] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,974] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,974] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,974] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,974] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,974] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,975] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,975] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,975] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,975] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,975] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,976] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,976] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,976] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,976] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,976] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,976] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,976] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,977] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,978] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,978] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,978] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:22:16,978] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,978] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:22:16,979] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:22:16,979] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:22:16,979] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:22:16,979] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,979] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,980] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:22:16,995] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:22:16,996] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:22:17,001] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:22:17,047] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:22:17,048] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:22:17,048] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:22:17,048] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:22:17,048] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:22:17,048] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:22:17,048] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:22:17,048] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:22:17,049] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:22:17,049] INFO Kafka startTimeMs: 1596140537048 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:22:17,284] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:22:17,305] INFO Logging initialized @2117ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:22:17,354] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:22:17,354] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:22:17,361] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:22:17,386] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:22:17,386] INFO Started @2198ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:22:17,403] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:22:17,403] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:22:17,403] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:22:17,403] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:22:17,404] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:22:17,404] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:22:17,413] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:22:17,414] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:22:17,414] INFO Kafka startTimeMs: 1596140537413 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:22:17,516] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:22:17,518] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:22:17,524] INFO Kafka Connect standalone worker initialization took 2030ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:22:17,524] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:22:17,525] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:22:17,525] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:22:17,526] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:22:17,532] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:22:17,532] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:22:17,532] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:22:17,571] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:22:17,637] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:22:17,637] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:22:17,638] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:22:18,067] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:22:18,067] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:22:18,067] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:22:18,089] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:22:18,171] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:22:18,177] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:22:18,178] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:22:18,178] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:22:18,180] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:22:18,181] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:22:18,181] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:22:18,182] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:22:18,186] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:22:18,186] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:22:18,187] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:22:18,188] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:22:18,199] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:22:18,201] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:22:18,201] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:22:18,202] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:22:18,203] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:22:18,204] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:22:18,204] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:22:18,204] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:22:18,205] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:22:18,205] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:22:18,210] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:22:18,217] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:22:18,237] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:22:18,238] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:22:18,238] INFO Kafka startTimeMs: 1596140538237 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:22:18,243] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:22:18,244] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:22:18,245] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:22:18,246] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:22:18,251] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:22:18,251] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:22:18,256] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:22:18,256] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:22:18,257] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:22:18,257] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:22:18,257] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:22:18,257] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:22:18,258] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:22:18,258] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:22:18,259] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:22:18,259] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:22:18,260] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:22:18,260] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:22:18,261] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:22:18,261] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:22:18,261] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:22:18,262] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:22:18,262] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:22:18,262] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:22:18,262] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:22:18,263] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:22:18,263] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:22:18,264] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:22:18,270] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:22:18,307] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:22:18,307] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:22:18,307] INFO Kafka startTimeMs: 1596140538307 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:22:18,315] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:22:18,316] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:22:18,317] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:22:18,317] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:22:18,320] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:22:18,321] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:22:18,335] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:22:18,336] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:22:18,337] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:22:18,344] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:22:18,344] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:22:18,349] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 7: {connector-consumer-jdbc-sink-0-7faf5ffb-7f60-4f50-adc2-2369f3edf0ba=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:22:18,353] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:22:18,357] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:22:18,367] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:22:18,378] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:22:18,460] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:22:18,460] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:22:18,462] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,475] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:22:18,497] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,511] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,513] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,514] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,514] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,520] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,525] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,531] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,532] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,533] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,534] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,535] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,536] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,537] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,537] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,542] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,545] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,547] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,547] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,555] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,557] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,558] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,559] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,560] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,563] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,568] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,569] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,575] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 21:22:18,577] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,581] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,597] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,598] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,599] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,599] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 21:22:18,605] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,605] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 21:22:18,606] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,607] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 21:22:18,608] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,609] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,610] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,613] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,615] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,616] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,616] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,617] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,620] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,621] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,623] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:22:18,665] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,667] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,668] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,669] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,670] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:10 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,672] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,689] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,691] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,691] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,691] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,691] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:9 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,692] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,704] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,705] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,705] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,706] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,706] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:8 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,706] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,716] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,717] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,718] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,718] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,718] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:7 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,719] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,729] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,731] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,731] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,732] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,732] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:6 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,732] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,743] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,745] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,745] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,745] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,745] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:5 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,746] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,755] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,756] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,757] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,757] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,757] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:4 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,758] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,768] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,770] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,770] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,770] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,771] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:3 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,771] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,781] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,782] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,782] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,782] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,782] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:2 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,783] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,791] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,794] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,795] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,795] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,795] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:1 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,796] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:22:18,807] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:22:18,808] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:22:18,809] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:22:18,809] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:22:18,810] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:0 with SQL: [ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:22:18,810] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:22:18,812] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""EMAIL"" TEXT NULL,[*]
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:22:18,813] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 21:22:18,813] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 21:22:18,813] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:22:18,814] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 21:22:18,814] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-7faf5ffb-7f60-4f50-adc2-2369f3edf0ba sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 21:22:27,074] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:22:27,074] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:22:27,077] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:22:27,077] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:22:27,078] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:22:27,078] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:22:27,078] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:22:27,079] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:22:27,080] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:22:27,080] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:22:27,080] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:22:27,132] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:22:27,132] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:22:27,133] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:22:27,133] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:22:27,141] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:22:27,141] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:22:27,144] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:22:27,145] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:22:27,145] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:22:27,145] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:22:27,146] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:22:27,146] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:22:27,147] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:22:27,147] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:22:27,147] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:22:27,147] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 21:24:58,188] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:24:58,193] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:24:58,198] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:24:58,221] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:24:58,484] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:24:58,484] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:58,484] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:58,485] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:58,485] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:58,485] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:58,486] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:24:58,693] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:24:58,694] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:58,694] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:24:58,836] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:24:58,836] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:58,838] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,668] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:24:59,669] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,669] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,669] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,669] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,669] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,669] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,670] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,670] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,670] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,670] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,670] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,670] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,671] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,672] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,673] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,674] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,674] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,674] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,674] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,674] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:24:59,675] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,675] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,675] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,676] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,676] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,676] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,676] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,676] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,677] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,677] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,677] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,677] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,677] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,677] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,678] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,679] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:24:59,680] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,680] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:24:59,680] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:24:59,681] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:24:59,681] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:24:59,681] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,681] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,681] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:24:59,694] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:24:59,695] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:24:59,699] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:24:59,748] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:24:59,748] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:24:59,748] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:24:59,748] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:24:59,748] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:24:59,749] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:24:59,749] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:24:59,749] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:24:59,749] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:24:59,749] INFO Kafka startTimeMs: 1596140699749 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:24:59,985] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:25:00,002] INFO Logging initialized @2105ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:25:00,049] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:25:00,049] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:25:00,056] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:25:00,081] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:25:00,082] INFO Started @2184ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:25:00,100] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:25:00,100] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:25:00,100] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:25:00,100] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:25:00,101] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:25:00,101] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:25:00,111] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:25:00,111] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:25:00,111] INFO Kafka startTimeMs: 1596140700111 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:25:00,227] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:00,228] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:00,234] INFO Kafka Connect standalone worker initialization took 2045ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:25:00,235] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:25:00,235] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:25:00,236] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:25:00,236] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:25:00,242] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:25:00,242] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:25:00,242] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:25:00,281] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:25:00,351] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:25:00,351] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:25:00,352] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:25:00,840] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:25:00,841] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:25:00,841] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:25:00,859] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:25:00,945] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:25:00,952] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:00,953] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:00,953] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:25:00,956] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:25:00,957] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:25:00,957] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:25:00,958] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:25:00,963] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:25:00,963] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:25:00,964] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:25:00,965] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:00,974] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:25:00,977] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:00,977] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:00,979] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:25:00,979] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:25:00,981] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:25:00,981] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:25:00,981] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:00,981] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:25:00,981] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:25:00,985] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:25:00,991] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:25:01,011] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:25:01,011] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:25:01,011] INFO Kafka startTimeMs: 1596140701010 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:25:01,018] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:25:01,019] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:25:01,020] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:25:01,020] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:25:01,021] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:25:01,021] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:25:01,024] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:25:01,025] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:01,025] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:01,025] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:25:01,026] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:25:01,026] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:25:01,027] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:25:01,027] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:01,028] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:25:01,028] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:25:01,029] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:01,030] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:01,030] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:25:01,031] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:25:01,032] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:25:01,032] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:25:01,032] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:01,032] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:25:01,033] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:25:01,034] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:25:01,034] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:25:01,034] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:01,043] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:25:01,079] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:25:01,079] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:25:01,079] INFO Kafka startTimeMs: 1596140701079 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:25:01,085] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:25:01,087] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:25:01,087] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:25:01,087] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:25:01,090] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:25:01,091] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:25:01,107] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:25:01,108] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:25:01,109] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:25:01,116] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:25:01,117] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:25:01,120] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-fa32acba-70b0-42d9-8585-723659a03b4e=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:25:01,124] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:25:01,127] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:25:01,137] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:25:01,147] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:25:01,228] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:25:01,245] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:25:01,245] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:25:01,247] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,281] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,289] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,291] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,292] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,292] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,295] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,301] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,303] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,304] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,305] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,305] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,306] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,307] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,308] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,309] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,314] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,316] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 21:25:01,318] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,318] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,319] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,325] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,328] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,329] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,329] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,330] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,333] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,336] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 21:25:01,338] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,339] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,342] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,342] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 21:25:01,344] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 21:25:01,346] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,353] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,363] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,365] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,366] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,373] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,374] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,377] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,378] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,379] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,382] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,385] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,386] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,387] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,387] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,388] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,390] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:01,410] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,412] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,413] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,414] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,415] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:10 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,417] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,435] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,436] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,436] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,437] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,437] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:9 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,437] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,453] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,454] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,455] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,455] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,455] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:8 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,455] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,469] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,470] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,470] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,471] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,471] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:7 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,471] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,482] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,483] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,484] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,484] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,484] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:6 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,485] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,495] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,496] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,496] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,496] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,496] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:5 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,497] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,506] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,507] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,508] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,508] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,508] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:4 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,508] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,518] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,519] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,520] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,520] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,521] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:3 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,521] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,531] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,532] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,533] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,533] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,533] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:2 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,534] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,544] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,546] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,546] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,546] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,547] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:1 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,547] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:01,557] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 21:25:01,558] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 21:25:01,558] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 21:25:01,559] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 21:25:01,559] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:0 with SQL: [ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 21:25:01,560] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:25:01,561] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""LAST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "LAST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 21:25:01,562] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 21:25:01,562] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 21:25:01,562] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:25:01,563] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 21:25:01,563] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-fa32acba-70b0-42d9-8585-723659a03b4e sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 21:25:07,986] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:25:07,986] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:25:07,990] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:25:07,990] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:25:07,991] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:25:07,992] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:25:07,992] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:25:07,994] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:25:07,994] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:25:07,995] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:25:07,995] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:25:07,996] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:25:07,996] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:25:07,996] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:25:07,997] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:25:08,005] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:25:08,006] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:25:08,008] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:25:08,008] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:25:08,009] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:25:08,009] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:25:08,010] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:25:08,010] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:25:08,012] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:25:08,012] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:25:08,013] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:25:08,013] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 21:25:37,795] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:25:37,802] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:25:37,809] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:25:37,835] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:25:38,121] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:25:38,121] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:38,122] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:38,122] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:38,122] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:38,122] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:38,123] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:25:38,305] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:25:38,305] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:38,305] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:25:38,427] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:25:38,427] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:38,428] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,241] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:25:39,241] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,242] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,242] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,242] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,242] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,242] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,242] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,243] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,243] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,243] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,243] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,243] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,243] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,244] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,244] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,244] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,244] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,244] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,244] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,244] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,245] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,246] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,247] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,247] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,247] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,247] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:25:39,248] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,248] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,249] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,249] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,249] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,249] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,249] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,250] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,250] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,250] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,250] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,250] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,250] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,251] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,251] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,251] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,251] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,251] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,251] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,251] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,252] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,253] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,253] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,253] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:25:39,253] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,253] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:25:39,254] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:25:39,254] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:25:39,254] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:25:39,254] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,254] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,254] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:25:39,266] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:25:39,267] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:25:39,270] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:25:39,321] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:25:39,321] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:25:39,321] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:25:39,321] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:25:39,322] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:25:39,322] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:25:39,322] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:25:39,322] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:25:39,323] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:25:39,323] INFO Kafka startTimeMs: 1596140739322 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:25:39,553] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:25:39,572] INFO Logging initialized @2070ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:25:39,624] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:25:39,624] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:25:39,630] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:25:39,654] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:25:39,654] INFO Started @2152ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:25:39,669] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:25:39,670] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:25:39,670] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:25:39,670] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:25:39,670] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:25:39,671] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:25:39,679] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:25:39,680] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:25:39,680] INFO Kafka startTimeMs: 1596140739679 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:25:39,776] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:39,777] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:39,783] INFO Kafka Connect standalone worker initialization took 1987ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:25:39,783] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:25:39,784] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:25:39,784] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:25:39,784] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:25:39,790] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:25:39,791] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:25:39,791] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:25:39,833] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:25:39,906] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:25:39,906] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:25:39,907] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:25:40,358] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:25:40,358] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:25:40,359] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:25:40,385] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:25:40,461] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:25:40,467] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:40,468] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:40,468] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:25:40,471] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:25:40,472] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:25:40,472] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:25:40,473] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:25:40,476] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:25:40,477] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:25:40,478] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:25:40,478] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:40,490] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:25:40,492] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:40,492] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:40,494] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:25:40,494] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:25:40,496] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:25:40,496] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:25:40,496] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:40,496] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:25:40,496] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:25:40,501] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:25:40,507] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:25:40,525] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:25:40,526] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:25:40,526] INFO Kafka startTimeMs: 1596140740525 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:25:40,533] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:25:40,534] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:25:40,535] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:25:40,535] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:25:40,536] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:25:40,537] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:25:40,539] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:25:40,540] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:40,540] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:40,540] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:25:40,541] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:25:40,541] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:25:40,542] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:25:40,542] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:40,543] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:25:40,543] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:25:40,544] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:25:40,544] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:40,545] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:25:40,545] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:25:40,546] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:25:40,546] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:25:40,546] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:25:40,546] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:25:40,546] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:25:40,547] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:25:40,548] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:25:40,548] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:25:40,555] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:25:40,601] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:25:40,602] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:25:40,602] INFO Kafka startTimeMs: 1596140740601 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:25:40,610] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:25:40,612] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:25:40,612] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:25:40,613] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:postgresql:mem:manager-worker-db;
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:25:40,616] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:25:40,617] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:25:40,638] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:25:40,639] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:25:40,642] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:25:40,652] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:25:40,652] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:25:40,661] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 3: {connector-consumer-jdbc-sink-0-1a6dd954-e25e-4f71-b889-fd830dbdcc17=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:25:40,665] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:25:40,669] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:25:40,681] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:25:40,692] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:25:40,793] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:25:40,816] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:25:40,816] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:25:40,818] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,859] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,868] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,870] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,871] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,872] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,875] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,881] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,883] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,884] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,884] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,885] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,886] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,887] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,887] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,888] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,892] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,902] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,903] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,905] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,910] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,913] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,914] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,914] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,915] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,918] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,922] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,924] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,928] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,932] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,953] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,955] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,956] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,976] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,978] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,988] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,989] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,989] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,995] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:40,998] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:41,000] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:41,001] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:41,002] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:41,002] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:41,007] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:25:41,186] INFO Unable to connect to database on attempt 1/3. Will retry in 10000 ms. (io.confluent.connect.jdbc.util.CachedConnectionProvider:99)
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "sa"
	at org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:524)
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:145)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:196)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:211)
	at org.postgresql.Driver.makeConnection(Driver.java:459)
	at org.postgresql.Driver.connect(Driver.java:261)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getConnection(GenericDatabaseDialect.java:227)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.newConnection(CachedConnectionProvider.java:93)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.getConnection(CachedConnectionProvider.java:62)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:56)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:25:44,096] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:25:44,096] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:25:44,105] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:25:44,105] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:25:44,105] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:25:44,106] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:25:44,106] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:25:49,107] ERROR Graceful stop of task jdbc-sink-0 failed. (org.apache.kafka.connect.runtime.Worker:736)
[2020-07-30 21:25:49,108] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:25:49,109] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:25:49,109] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:25:49,109] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:25:49,116] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:25:49,116] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:25:49,117] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:25:49,118] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:25:49,127] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 9 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:25:49,127] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:25:49,129] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:25:49,129] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:25:49,130] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:25:49,130] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:25:49,130] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:25:49,131] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:25:49,131] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:25:49,131] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:25:49,132] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:25:49,132] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 21:26:05,602] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:26:05,608] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:26:05,612] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:26:05,633] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:26:05,900] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:26:05,900] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:05,901] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:05,901] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:05,901] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:05,901] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:05,902] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:26:06,088] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:26:06,088] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:06,089] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:26:06,231] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:26:06,231] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:06,232] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,077] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:26:07,077] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,077] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,078] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,078] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,078] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,078] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,078] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,078] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,078] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,079] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,079] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,079] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,079] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,079] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,079] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,080] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,081] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,082] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,082] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,082] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,082] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,082] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,082] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,082] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,083] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,083] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,083] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:26:07,084] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,084] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,084] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,085] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,085] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,085] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,085] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,085] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,086] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,086] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,086] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,086] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,086] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,086] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,087] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,088] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,089] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:26:07,089] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,089] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:26:07,090] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:26:07,090] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:26:07,090] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:26:07,090] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,090] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,090] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:26:07,102] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:26:07,103] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:26:07,107] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:26:07,166] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:26:07,166] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:26:07,166] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:26:07,166] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:26:07,166] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:26:07,166] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:26:07,167] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:26:07,167] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:26:07,167] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:26:07,167] INFO Kafka startTimeMs: 1596140767167 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:26:07,403] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:26:07,421] INFO Logging initialized @2112ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:26:07,470] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:26:07,470] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:26:07,478] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:26:07,503] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:26:07,503] INFO Started @2195ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:26:07,522] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:26:07,522] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:26:07,522] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:26:07,523] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:26:07,523] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:26:07,523] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:26:07,534] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:26:07,534] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:26:07,534] INFO Kafka startTimeMs: 1596140767534 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:26:07,654] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:26:07,655] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:26:07,662] INFO Kafka Connect standalone worker initialization took 2058ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:26:07,662] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:26:07,663] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:26:07,663] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:26:07,663] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:26:07,670] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:26:07,670] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:26:07,671] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:26:07,709] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:26:07,777] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:26:07,777] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:26:07,778] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:26:08,222] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:26:08,222] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:26:08,222] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:26:08,243] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:26:08,327] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:26:08,334] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:26:08,335] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:26:08,335] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:26:08,338] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:26:08,338] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:26:08,339] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:26:08,340] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:26:08,344] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:26:08,344] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:26:08,345] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:26:08,348] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:26:08,359] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:26:08,360] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:26:08,361] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:26:08,362] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:26:08,363] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:26:08,364] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:26:08,364] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:26:08,365] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:26:08,365] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:26:08,365] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:26:08,368] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:26:08,375] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:26:08,392] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:26:08,393] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:26:08,393] INFO Kafka startTimeMs: 1596140768392 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:26:08,400] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:26:08,401] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:26:08,401] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:26:08,401] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:26:08,402] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:26:08,402] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:26:08,405] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:26:08,405] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:26:08,405] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:26:08,405] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:26:08,406] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:26:08,406] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:26:08,406] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:26:08,406] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:26:08,407] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:26:08,407] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:26:08,408] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:26:08,408] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:26:08,408] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:26:08,409] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:26:08,409] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:26:08,409] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:26:08,409] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:26:08,409] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:26:08,409] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:26:08,410] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:26:08,410] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:26:08,410] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:26:08,418] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:26:08,452] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:26:08,452] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:26:08,453] INFO Kafka startTimeMs: 1596140768452 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:26:08,458] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:26:08,459] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:26:08,459] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:26:08,459] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:postgresql:mem:manager-worker-db;
	connection.user = admin
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:26:08,462] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:26:08,462] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:26:08,477] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:26:08,477] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:26:08,479] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:26:08,486] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:26:08,486] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:26:08,491] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 5: {connector-consumer-jdbc-sink-0-0df75ac3-807a-4fed-8d9c-91f647305c04=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:26:08,494] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:26:08,498] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:26:08,507] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:26:08,517] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:26:08,600] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:26:08,621] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:26:08,621] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:26:08,622] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,669] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,685] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,686] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,687] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,688] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,701] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,709] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,710] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,711] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,712] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,716] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,716] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,717] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,718] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,718] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,722] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,733] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,734] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,734] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,743] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,745] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,746] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,747] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,748] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,751] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,756] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,756] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,761] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,766] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,784] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,786] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,787] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,794] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,795] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,801] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,802] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,803] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,806] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,813] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,815] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,816] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,816] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,817] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,822] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:26:08,950] INFO Unable to connect to database on attempt 1/3. Will retry in 10000 ms. (io.confluent.connect.jdbc.util.CachedConnectionProvider:99)
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "admin"
	at org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:524)
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:145)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:196)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:211)
	at org.postgresql.Driver.makeConnection(Driver.java:459)
	at org.postgresql.Driver.connect(Driver.java:261)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getConnection(GenericDatabaseDialect.java:227)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.newConnection(CachedConnectionProvider.java:93)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.getConnection(CachedConnectionProvider.java:62)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:56)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:26:11,921] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:26:11,922] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:26:11,927] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:26:11,928] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:26:11,929] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:26:11,929] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:26:11,929] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:26:16,931] ERROR Graceful stop of task jdbc-sink-0 failed. (org.apache.kafka.connect.runtime.Worker:736)
[2020-07-30 21:26:16,932] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:26:16,933] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:26:16,934] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:26:16,934] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:26:17,033] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:26:17,033] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:26:17,034] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:26:17,034] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:26:17,043] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 9 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:26:17,044] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:26:17,047] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:26:17,047] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:26:17,047] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:26:17,047] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:26:17,048] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:26:17,048] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:26:17,049] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:26:17,049] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:26:17,049] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:26:17,050] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 21:39:10,217] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 21:39:10,229] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 21:39:10,242] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 21:39:10,278] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:39:10,594] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:39:10,595] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:10,595] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:10,595] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:10,596] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:10,596] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:10,597] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:39:10,883] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:39:10,884] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:10,884] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 21:39:11,032] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:39:11,033] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:11,034] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,917] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 21:39:51,917] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,918] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,918] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,918] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,918] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,918] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,918] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,918] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,919] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,919] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,919] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,919] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,919] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,919] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,920] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,920] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,920] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,920] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,920] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,920] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,921] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,921] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,921] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,921] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,921] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,921] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,921] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,922] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,922] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,922] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,922] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,922] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,922] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,922] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,923] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,923] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,923] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,923] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,923] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,923] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,923] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,924] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,924] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 21:39:51,925] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,925] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,925] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,926] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,926] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,926] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,926] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,926] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,926] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,927] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,927] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,927] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,927] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,927] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,928] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,929] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,929] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,929] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,929] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,929] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,929] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,930] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,930] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,930] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,930] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:39:51,930] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,931] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:39:51,931] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:39:51,931] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:39:51,932] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 21:39:51,932] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,932] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,932] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 21:39:51,948] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 21:39:51,949] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 21:39:51,955] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 21:39:52,022] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:39:52,023] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:39:52,023] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:39:52,023] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:39:52,023] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:39:52,023] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:39:52,023] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 21:39:52,023] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:39:52,023] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:39:52,024] INFO Kafka startTimeMs: 1596141592023 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:39:52,308] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 21:39:52,331] INFO Logging initialized @42841ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 21:39:52,389] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 21:39:52,389] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 21:39:52,398] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 21:39:52,427] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 21:39:52,427] INFO Started @42937ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 21:39:52,448] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:39:52,448] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 21:39:52,448] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:39:52,448] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 21:39:52,449] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 21:39:52,449] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 21:39:52,459] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:39:52,459] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:39:52,460] INFO Kafka startTimeMs: 1596141592459 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:39:52,583] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:39:52,585] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:39:52,594] INFO Kafka Connect standalone worker initialization took 42374ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 21:39:52,594] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 21:39:52,595] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 21:39:52,595] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 21:39:52,596] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 21:39:52,608] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 21:39:52,608] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 21:39:52,608] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 21:39:52,653] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 21:39:52,742] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 21:39:52,743] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 21:39:52,744] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 21:39:53,267] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 21:39:53,268] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 21:39:53,268] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 21:39:53,297] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:39:53,415] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:39:53,427] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:39:53,428] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:39:53,428] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:39:53,432] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:39:53,434] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 21:39:53,435] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 21:39:53,436] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:39:53,446] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 21:39:53,447] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:39:53,449] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 21:39:53,449] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:39:53,469] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:39:53,471] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:39:53,472] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:39:53,474] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:39:53,475] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:39:53,478] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:39:53,478] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:39:53,478] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:39:53,478] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:39:53,479] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:39:53,484] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 21:39:53,493] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 21:39:53,524] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:39:53,524] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:39:53,524] INFO Kafka startTimeMs: 1596141593523 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:39:53,539] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:39:53,540] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 21:39:53,541] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:39:53,541] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 21:39:53,546] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 21:39:53,547] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:39:53,549] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 21:39:53,549] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:39:53,550] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:39:53,550] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 21:39:53,550] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 21:39:53,551] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 21:39:53,551] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:39:53,552] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:39:53,552] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 21:39:53,553] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 21:39:53,554] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 21:39:53,554] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:39:53,555] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 21:39:53,555] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 21:39:53,556] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 21:39:53,556] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 21:39:53,556] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 21:39:53,556] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 21:39:53,556] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 21:39:53,557] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 21:39:53,558] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 21:39:53,558] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 21:39:53,569] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 21:39:53,617] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 21:39:53,617] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 21:39:53,617] INFO Kafka startTimeMs: 1596141593617 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 21:39:53,622] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 21:39:53,624] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 21:39:53,625] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 21:39:53,625] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:mysql://${MYSQL_HOST:localhost}:3306/db_example
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 21:39:53,628] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 21:39:53,629] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 21:39:53,644] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 21:39:53,645] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 21:39:53,647] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:39:53,656] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 21:39:53,657] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 21:39:53,661] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-1369bb90-41c4-4b6e-bd86-73142e002625=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 21:39:53,664] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 21:39:53,669] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 21:39:53,679] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 21:39:53,691] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 21:39:53,798] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 21:39:53,798] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 21:39:53,799] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,806] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:39:53,807] INFO Unable to connect to database on attempt 1/3. Will retry in 10000 ms. (io.confluent.connect.jdbc.util.CachedConnectionProvider:99)
java.sql.SQLException: No suitable driver found for jdbc:mysql://${MYSQL_HOST:localhost}:3306/db_example
	at java.sql.DriverManager.getConnection(DriverManager.java:689)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getConnection(GenericDatabaseDialect.java:227)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.newConnection(CachedConnectionProvider.java:93)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.getConnection(CachedConnectionProvider.java:62)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:56)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:39:53,842] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,851] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,853] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,854] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,854] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,858] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,863] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,864] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,865] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,865] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,866] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,867] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,868] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,869] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,869] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,874] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,877] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,878] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,878] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,882] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,884] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,884] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,885] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,885] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,888] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,891] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,892] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,897] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,900] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,910] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,911] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,911] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,917] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,918] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,920] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,921] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,921] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,924] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,927] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,928] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,929] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,930] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,931] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:39:53,933] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 21:40:00,583] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 21:40:00,583] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 21:40:00,588] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 21:40:00,589] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 21:40:00,589] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 21:40:00,590] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 21:40:00,590] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:40:03,539] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:40:03,540] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:40:03,599] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 59 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:40:03,811] INFO Attempting to open connection #2 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 21:40:03,812] INFO Unable to connect to database on attempt 2/3. Will retry in 10000 ms. (io.confluent.connect.jdbc.util.CachedConnectionProvider:99)
java.sql.SQLException: No suitable driver found for jdbc:mysql://${MYSQL_HOST:localhost}:3306/db_example
	at java.sql.DriverManager.getConnection(DriverManager.java:689)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getConnection(GenericDatabaseDialect.java:227)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.newConnection(CachedConnectionProvider.java:93)
	at io.confluent.connect.jdbc.util.CachedConnectionProvider.getConnection(CachedConnectionProvider.java:62)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:56)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 21:40:05,592] ERROR Graceful stop of task jdbc-sink-0 failed. (org.apache.kafka.connect.runtime.Worker:736)
[2020-07-30 21:40:05,592] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:40:05,593] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:40:05,593] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 21:40:05,593] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 21:40:05,639] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 21:40:05,639] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:40:05,641] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 21:40:05,641] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 21:40:05,643] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 21:40:05,643] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 21:40:05,648] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 21:40:05,648] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 21:40:05,648] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 21:40:05,649] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 21:40:05,650] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 21:40:05,650] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 21:40:05,651] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 21:40:05,651] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 21:40:05,651] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 21:40:05,652] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)

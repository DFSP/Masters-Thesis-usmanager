[2020-07-30 19:00:00,508] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:00:00,530] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:00:00,539] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:00,539] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:00:00,548] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:00:00,549] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:00:00,550] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [SERVICE_APPS]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:00:00,551] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:00:00,630] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:00:00,632] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:00:00,636] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:00,639] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:00:00,665] WARN No tasks will be run because no tables were found (io.confluent.connect.jdbc.JdbcSourceConnector:150)
[2020-07-30 19:00:00,667] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:00:24,683] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:00:24,684] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:00:24,693] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:00:24,694] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:00:24,700] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:00:24,700] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:00:24,701] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:00:24,701] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:00:24,701] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:00:24,701] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:00:24,702] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:00:24,702] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:00:24,703] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:00:24,703] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:00:24,704] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:00:24,704] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:00:26,403] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:00:26,413] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:00:26,428] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:00:26,459] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:00:26,808] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:26,808] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:26,809] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:26,809] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:26,809] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:26,809] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:26,810] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:00:27,082] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:27,083] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:27,083] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:00:27,265] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:27,266] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:27,267] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,201] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:28,201] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,201] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,202] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,203] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,204] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,205] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:28,206] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,206] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,206] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,206] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,207] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,208] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,209] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,210] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,210] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,210] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:28,210] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,210] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:28,210] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:28,210] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:28,211] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:28,211] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,211] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,211] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:28,224] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:00:28,225] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:00:28,228] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:00:28,277] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:28,277] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:28,277] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:28,278] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:28,278] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:28,278] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:28,278] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:28,278] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:00:28,278] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:00:28,278] INFO Kafka startTimeMs: 1596132028278 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:00:28,509] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:00:28,525] INFO Logging initialized @2716ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:00:28,588] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:00:28,588] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:00:28,597] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:00:28,622] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:00:28,622] INFO Started @2813ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:00:28,641] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:00:28,641] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:00:28,642] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:00:28,642] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:00:28,642] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:00:28,643] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:00:28,653] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:00:28,653] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:00:28,654] INFO Kafka startTimeMs: 1596132028653 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:00:28,759] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:00:28,760] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:00:28,768] INFO Kafka Connect standalone worker initialization took 2363ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:00:28,768] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:00:28,769] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:00:28,769] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:00:28,770] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:00:28,773] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:00:28,773] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:00:28,773] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:00:28,817] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:00:28,885] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:00:28,886] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:00:28,887] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:00:29,358] INFO Started o.e.j.s.ServletContextHandler@1c6e0a08{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:00:29,358] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:00:29,358] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:00:29,391] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:00:29,593] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:00:29,599] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:00:29,600] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:29,600] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:00:29,602] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:00:29,603] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:00:29,603] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:00:29,604] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:00:29,645] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:00:29,646] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:00:29,646] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:00:29,647] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:29,666] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:00:29,668] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:00:29,669] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:29,671] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:00:29,672] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:00:29,673] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:00:29,674] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:00:29,674] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:00:29,674] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:00:29,674] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:00:29,679] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:00:29,685] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:00:29,711] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:00:29,711] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:00:29,711] INFO Kafka startTimeMs: 1596132029711 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:00:29,719] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:00:29,720] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:00:29,720] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:00:29,721] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:00:29,721] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:00:29,722] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:00:30,137] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:00:30,137] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:00:30,138] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:30,152] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:00:30,152] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:00:30,153] ERROR WorkerSourceTask{id=jdbc-source-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:178)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:104)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.sendRecords(WorkerSourceTask.java:320)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:245)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.DataException: Field does not exist: id
	at org.apache.kafka.connect.transforms.ValueToKey.applyWithSchema(ValueToKey.java:89)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:67)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:128)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:162)
	... 11 more
[2020-07-30 19:00:30,156] ERROR WorkerSourceTask{id=jdbc-source-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:00:30,156] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:00:30,156] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:00:39,719] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:00:39,719] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:00:40,408] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:00:40,408] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:00:40,415] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:00:40,416] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:00:40,416] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:00:40,417] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:00:40,417] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:00:40,421] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:00:40,422] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:00:40,422] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:00:40,422] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:00:40,423] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:00:40,423] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:00:40,423] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:00:40,424] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:00:40,424] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:00:40,424] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:00:41,943] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:00:41,949] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:00:41,953] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:00:41,971] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:00:42,264] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:42,265] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:42,265] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:42,266] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:42,266] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:42,266] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:42,267] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:00:42,468] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:42,468] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:42,469] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:00:42,596] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:42,596] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:42,597] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,460] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:00:43,460] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,461] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,462] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,463] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,464] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,465] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,465] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,465] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,465] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,465] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:00:43,466] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,466] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,466] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,466] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,466] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,467] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,468] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,469] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:43,470] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,470] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:43,470] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:43,470] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:43,470] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:00:43,470] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,471] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,471] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:00:43,484] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:00:43,485] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:00:43,490] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:00:43,542] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:43,542] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:43,542] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:43,543] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:43,543] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:43,543] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:43,543] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:00:43,544] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:00:43,544] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:00:43,544] INFO Kafka startTimeMs: 1596132043543 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:00:43,865] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:00:43,888] INFO Logging initialized @2245ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:00:43,955] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:00:43,955] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:00:43,964] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:00:43,986] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:00:43,987] INFO Started @2344ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:00:44,004] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:00:44,004] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:00:44,004] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:00:44,004] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:00:44,005] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:00:44,005] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:00:44,015] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:00:44,016] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:00:44,016] INFO Kafka startTimeMs: 1596132044015 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:00:44,172] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:00:44,173] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:00:44,181] INFO Kafka Connect standalone worker initialization took 2236ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:00:44,181] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:00:44,182] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:00:44,182] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:00:44,183] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:00:44,186] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:00:44,186] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:00:44,186] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:00:44,227] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:00:44,298] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:00:44,299] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:00:44,300] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:00:44,839] INFO Started o.e.j.s.ServletContextHandler@1c6e0a08{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:00:44,840] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:00:44,840] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:00:44,863] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:00:45,094] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:00:45,101] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:00:45,102] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:45,102] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:00:45,105] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:00:45,106] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:00:45,106] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:00:45,108] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:00:45,149] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:00:45,149] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:00:45,150] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:00:45,151] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:45,171] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:00:45,174] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:00:45,174] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:00:45,176] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:00:45,176] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:00:45,178] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:00:45,178] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:00:45,179] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:00:45,179] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:00:45,179] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:00:45,185] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:00:45,191] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:00:45,213] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:00:45,213] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:00:45,213] INFO Kafka startTimeMs: 1596132045213 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:00:45,220] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:00:45,221] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:00:45,222] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:00:45,223] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:00:45,224] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:00:45,227] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:00:45,570] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:00:45,571] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:00:45,571] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:45,611] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 3 : {APPS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:45,731] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:45,747] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:45,749] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:45,751] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:45,751] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:45,780] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 9 : {COMPONENT_TYPES=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:45,883] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:45,906] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 12 : {CONDITIONS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,009] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,010] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,010] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,011] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,012] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,012] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,013] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,014] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,015] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,026] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 17 : {DECISIONS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,130] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,153] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 20 : {EDGE_HOSTS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,256] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,257] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,257] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,279] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 24 : {FIELDS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,386] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,389] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,390] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,391] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,392] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,405] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 28 : {HOST_RULES=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,508] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,529] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 31 : {HOST_RULE_CONDITIONS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,633] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,635] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,636] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,636] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,637] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,656] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 35 : {OPERATORS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,763] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,778] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 38 : {REGIONS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:46,882] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:46,902] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 42 : {SERVICES=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:47,008] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,009] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,010] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,029] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 48 : {SERVICE_DEPENDENCIES=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:47,138] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,139] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,142] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,144] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,144] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,156] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 52 : {SERVICE_RULES=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:47,259] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,276] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 55 : {SERVICE_RULE_CONDITIONS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:47,381] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,382] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,383] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,384] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,385] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,403] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 58 : {USERS=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:47,505] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:00:47,519] WARN [Producer clientId=connector-producer-jdbc-source-0] Error while fetching metadata with correlation id 61 : {VALUE_MODES=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1070)
[2020-07-30 19:00:55,221] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:00:55,221] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:00:55,235] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 14 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:01:05,235] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:01:05,236] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:01:05,240] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:01:15,241] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:01:15,242] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:01:15,245] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:01:18,618] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:01:18,619] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:01:18,623] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:01:18,623] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:01:18,624] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:01:18,624] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:01:18,624] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:01:18,624] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:01:18,712] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:01:18,712] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:01:18,712] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:01:18,712] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:01:18,714] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:01:18,714] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:01:18,719] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:01:18,720] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:01:18,720] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:01:18,720] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:01:18,721] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:01:18,721] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:01:18,722] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:01:18,722] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:01:18,723] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:01:18,723] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:01:19,926] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:01:19,933] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:01:19,937] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:01:19,959] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:01:20,278] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:01:20,279] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:20,279] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:20,280] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:20,280] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:20,280] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:20,281] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:01:20,511] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:01:20,511] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:20,512] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:01:20,661] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:01:20,661] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:20,662] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,534] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:01:21,534] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,534] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,535] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,536] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,537] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,538] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:01:21,539] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,539] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,540] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,540] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,540] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,540] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,540] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,540] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,540] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,541] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,542] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,542] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,542] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,542] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,542] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,542] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,542] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,543] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,543] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,543] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,543] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,543] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,543] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,543] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,544] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:01:21,544] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,544] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:01:21,544] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:01:21,545] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:01:21,545] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:01:21,545] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,545] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,545] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:01:21,557] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:01:21,558] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:01:21,562] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:01:21,614] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:01:21,614] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:01:21,614] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:01:21,614] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:01:21,614] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:01:21,614] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:01:21,614] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:01:21,615] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:01:21,615] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:01:21,615] INFO Kafka startTimeMs: 1596132081614 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:01:21,854] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:01:21,871] INFO Logging initialized @2261ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:01:21,925] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:01:21,925] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:01:21,931] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:01:21,955] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:01:21,955] INFO Started @2346ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:01:21,972] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:01:21,972] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:01:21,973] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:01:21,973] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:01:21,973] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:01:21,973] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:01:21,983] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:01:21,983] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:01:21,983] INFO Kafka startTimeMs: 1596132081983 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:01:22,116] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:01:22,117] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:01:22,125] INFO Kafka Connect standalone worker initialization took 2197ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:01:22,126] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:01:22,127] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:01:22,127] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:01:22,128] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:01:22,137] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:01:22,138] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:01:22,138] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:01:22,187] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:01:22,258] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:01:22,259] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:01:22,260] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:01:22,694] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:01:22,694] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:01:22,695] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:01:22,713] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [APP_SERVICES]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:01:22,905] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:01:22,910] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:01:22,911] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:01:22,911] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:01:22,914] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:01:22,915] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:01:22,915] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [APP_SERVICES]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:01:22,916] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:01:22,944] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:01:22,944] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:01:22,945] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:01:22,945] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:01:22,962] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:01:22,965] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:01:22,966] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [id]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = id
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:01:22,967] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:01:22,968] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:01:22,969] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:01:22,969] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:01:22,970] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:01:22,970] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:01:22,970] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:01:22,976] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:01:22,984] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:01:23,006] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:01:23,006] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:01:23,006] INFO Kafka startTimeMs: 1596132083006 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:01:23,014] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:01:23,015] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [APP_SERVICES]
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:01:23,015] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:01:23,015] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:01:23,016] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:01:23,016] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:01:23,071] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:01:23,071] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:01:23,072] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:01:23,088] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:01:23,088] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:01:23,089] ERROR WorkerSourceTask{id=jdbc-source-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:178)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:104)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.sendRecords(WorkerSourceTask.java:320)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:245)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.DataException: Field does not exist: id
	at org.apache.kafka.connect.transforms.ValueToKey.applyWithSchema(ValueToKey.java:89)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:67)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:128)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:162)
	... 11 more
[2020-07-30 19:01:23,092] ERROR WorkerSourceTask{id=jdbc-source-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:01:23,092] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:01:23,092] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:01:25,176] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:01:25,177] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:01:25,179] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:01:25,180] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:01:25,180] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:01:25,180] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:01:25,181] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:01:25,182] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:01:25,182] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:01:25,182] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:01:25,183] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:01:25,183] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:01:25,183] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:01:25,183] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:01:25,183] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:01:25,184] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:01:25,184] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:02:00,032] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:02:00,045] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:02:00,054] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:02:00,090] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:02:00,472] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:02:00,473] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:00,473] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:00,473] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:00,474] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:00,474] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:00,475] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:02:00,727] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:02:00,728] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:00,728] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:02:00,892] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:02:00,893] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:00,894] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,506] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:02:02,506] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,507] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,507] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,508] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,508] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,509] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,509] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,509] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,509] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,509] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,509] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,510] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,510] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,510] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,510] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,510] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,511] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,511] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,511] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,511] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,511] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,511] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,512] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,512] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,512] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,512] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,512] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,512] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,512] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,513] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,513] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,513] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,513] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,513] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,513] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,514] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,514] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,514] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,514] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,514] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,514] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,514] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,515] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:02:02,515] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,516] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,516] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,516] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,516] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,516] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,516] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,517] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,517] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,517] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,517] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,517] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,517] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,517] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,518] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,518] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,518] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,518] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,518] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,519] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,519] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,519] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,519] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,520] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,520] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,520] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,521] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,521] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,521] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,521] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,522] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,522] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:02:02,522] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,523] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:02:02,524] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:02:02,524] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:02:02,524] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:02:02,524] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,524] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,525] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:02:02,541] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:02:02,543] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:02:02,549] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:02:02,626] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:02:02,626] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:02:02,626] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:02:02,626] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:02:02,627] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:02:02,627] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:02:02,627] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:02:02,627] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:02:02,628] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:02:02,628] INFO Kafka startTimeMs: 1596132122627 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:02:03,025] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:02:03,051] INFO Logging initialized @3384ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:02:03,120] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:02:03,122] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:02:03,134] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:02:03,176] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:02:03,177] INFO Started @3510ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:02:03,201] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:02:03,201] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:02:03,202] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:02:03,202] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:02:03,202] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:02:03,203] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:02:03,218] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:02:03,218] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:02:03,218] INFO Kafka startTimeMs: 1596132123217 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:02:03,405] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:02:03,408] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:02:03,419] INFO Kafka Connect standalone worker initialization took 3382ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:02:03,419] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:02:03,421] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:02:03,421] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:02:03,422] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:02:03,435] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:02:03,435] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:02:03,435] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:02:03,499] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:02:03,605] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:02:03,606] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:02:03,608] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:02:04,254] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:02:04,254] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:02:04,254] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:02:04,280] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [APP_SERVICES]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:02:04,543] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:02:04,555] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:02:04,555] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:02:04,556] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:02:04,560] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:02:04,561] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:02:04,561] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [APP_SERVICES]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:02:04,563] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:02:04,613] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:02:04,614] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:02:04,615] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:02:04,616] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:02:04,649] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:02:04,652] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:02:04,653] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:02:04,655] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:02:04,655] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:02:04,657] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:02:04,658] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:02:04,658] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:02:04,658] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:02:04,658] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:02:04,667] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:02:04,675] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:02:04,712] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:02:04,713] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:02:04,713] INFO Kafka startTimeMs: 1596132124712 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:02:04,743] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:02:04,753] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:02:04,758] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:02:04,759] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [APP_SERVICES]
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:02:04,761] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:02:04,767] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:02:04,851] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:02:04,851] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:02:04,852] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:02:14,743] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:02:14,745] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:02:14,759] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 15 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:02:24,760] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:02:24,760] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:02:24,763] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:02:34,764] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:02:34,764] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:02:34,766] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:02:44,766] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:02:44,767] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:02:44,770] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:02:54,771] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:02:54,771] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:02:54,772] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:02:57,719] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:02:57,719] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:02:57,727] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:02:57,728] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:02:57,729] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:02:57,729] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:02:57,729] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:02:57,729] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:02:57,787] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:02:57,788] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:02:57,788] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:02:57,788] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:02:57,790] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:02:57,790] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:02:57,794] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:02:57,794] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:02:57,794] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:02:57,795] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:02:57,795] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:02:57,795] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:02:57,797] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:02:57,797] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:02:57,798] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:02:57,798] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:03:06,163] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:03:06,170] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:03:06,180] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:03:06,206] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:03:06,528] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:03:06,530] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:06,530] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:06,530] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:06,530] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:06,530] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:06,531] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:03:06,733] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:03:06,733] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:06,734] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:03:06,894] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:03:06,894] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:06,895] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,894] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:03:07,895] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,895] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,895] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,895] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,895] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,896] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,896] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,896] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,896] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,897] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,898] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,899] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,900] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,900] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,900] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:03:07,900] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,901] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,901] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,901] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,901] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,901] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,901] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,902] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,903] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,904] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,904] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,904] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,904] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,904] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,904] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,904] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,905] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,905] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:03:07,905] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,905] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:03:07,905] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:03:07,906] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:03:07,906] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:03:07,906] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,906] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,906] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:03:07,920] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:03:07,921] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:03:07,925] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:03:07,981] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:03:07,981] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:03:07,981] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:03:07,981] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:03:07,982] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:03:07,982] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:03:07,982] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:03:07,982] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:03:07,982] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:03:07,983] INFO Kafka startTimeMs: 1596132187982 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:03:08,252] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:03:08,274] INFO Logging initialized @2444ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:03:08,349] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:03:08,350] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:03:08,358] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:03:08,386] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:03:08,386] INFO Started @2556ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:03:08,407] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:03:08,408] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:03:08,408] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:03:08,408] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:03:08,408] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:03:08,409] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:03:08,420] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:03:08,420] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:03:08,420] INFO Kafka startTimeMs: 1596132188419 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:03:08,546] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:03:08,547] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:03:08,553] INFO Kafka Connect standalone worker initialization took 2388ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:03:08,553] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:03:08,554] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:03:08,554] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:03:08,554] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:03:08,563] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:03:08,563] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:03:08,563] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:03:08,603] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:03:08,670] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:03:08,671] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:03:08,672] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:03:09,139] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:03:09,139] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:03:09,139] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:03:09,165] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:03:09,398] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:03:09,405] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:03:09,406] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:03:09,406] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:03:09,409] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:03:09,410] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:03:09,411] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:03:09,412] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:03:09,450] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:03:09,451] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:03:09,452] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:03:09,453] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:03:09,475] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:03:09,478] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:03:09,478] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:03:09,480] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:03:09,480] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:03:09,481] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:03:09,481] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:03:09,482] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:03:09,482] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:03:09,482] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:03:09,490] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:03:09,499] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:03:09,527] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:03:09,527] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:03:09,527] INFO Kafka startTimeMs: 1596132189527 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:03:09,534] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:03:09,534] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:03:09,535] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:03:09,535] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:03:09,536] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:03:09,537] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:03:09,911] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:03:09,911] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:03:09,912] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:09,971] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:09,985] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:09,987] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:09,989] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:09,990] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:09,996] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,003] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,005] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,006] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,007] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,007] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,008] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,010] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,011] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,012] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,016] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,021] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,022] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,023] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,029] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,032] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,033] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,033] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,034] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,039] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,047] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,049] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,050] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,050] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,052] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,057] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,061] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,075] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,076] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,077] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,087] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,088] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,092] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,092] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,093] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,096] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,101] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,102] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,103] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,104] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,105] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:10,110] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:03:19,534] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:03:19,534] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:03:19,540] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:03:29,541] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:03:29,541] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:03:29,545] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:03:36,395] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:03:36,395] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:03:36,399] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:03:36,400] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:03:36,401] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:03:36,401] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:03:36,401] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:03:36,401] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:03:36,420] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:03:36,421] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:03:36,421] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:03:36,421] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:03:36,422] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:03:36,423] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:03:36,426] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:03:36,426] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:03:36,426] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:03:36,427] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:03:36,428] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:03:36,428] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:03:36,429] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:03:36,429] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:03:36,430] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:03:36,430] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:04:18,506] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:04:18,512] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:04:18,516] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:04:18,536] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:04:18,885] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:18,886] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:18,886] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:18,887] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:18,887] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:18,887] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:18,888] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:04:19,132] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:19,133] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:19,134] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:04:19,290] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:19,290] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:19,291] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,844] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:20,845] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,845] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,845] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,845] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,845] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,845] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,846] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,846] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,846] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,846] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,846] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,846] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,847] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,847] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,847] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,847] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,847] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,847] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,848] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,848] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,848] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,848] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,848] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,849] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,850] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,850] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,850] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,850] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,850] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,850] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,850] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,851] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,851] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,851] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,851] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:20,852] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,852] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,853] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,853] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,853] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,853] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,853] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,854] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,854] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,854] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,854] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,854] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,854] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,855] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,856] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,857] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,857] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:20,857] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,857] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:20,858] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:20,858] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:20,858] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:20,858] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,858] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,859] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:20,875] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:04:20,876] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:04:20,880] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:04:20,955] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:20,955] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:20,955] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:20,955] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:20,955] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:20,955] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:20,955] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:20,956] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:04:20,956] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:04:20,956] INFO Kafka startTimeMs: 1596132260955 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:04:21,216] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:04:21,245] INFO Logging initialized @3039ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:04:21,334] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:04:21,335] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:04:21,346] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:04:21,395] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:04:21,395] INFO Started @3189ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:04:21,417] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:04:21,417] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:04:21,417] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:04:21,417] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:04:21,418] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:04:21,418] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:04:21,431] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:04:21,431] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:04:21,432] INFO Kafka startTimeMs: 1596132261431 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:04:21,561] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:04:21,563] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:04:21,573] INFO Kafka Connect standalone worker initialization took 3065ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:04:21,573] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:04:21,574] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:04:21,574] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:04:21,575] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:04:21,588] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:04:21,588] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:04:21,589] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:04:21,641] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:04:21,758] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:04:21,758] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:04:21,760] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:04:22,567] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:04:22,568] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:04:22,568] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:04:22,611] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:04:22,717] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:04:22,722] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:04:22,732] ERROR Failed to create job for config/connect-jdbc-source.properties (org.apache.kafka.connect.cli.ConnectStandalone:110)
[2020-07-30 19:04:22,733] ERROR Stopping after connector error (org.apache.kafka.connect.cli.ConnectStandalone:121)
java.util.concurrent.ExecutionException: org.apache.kafka.connect.runtime.rest.errors.BadRequestException: Connector configuration is invalid and contains the following 2 error(s):
Invalid value org.h2.jdbc.JdbcSQLNonTransientConnectionException: Database may be already in use: null. Possible solutions: close all other connection(s); use the server mode [90020-199] for configuration Couldn't open connection to jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
Invalid value org.h2.jdbc.JdbcSQLNonTransientConnectionException: Database may be already in use: null. Possible solutions: close all other connection(s); use the server mode [90020-199] for configuration Couldn't open connection to jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
You can also find the above list of errors at the endpoint `/connector-plugins/{connectorType}/config/validate`
	at org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:115)
	at org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:99)
	at org.apache.kafka.connect.cli.ConnectStandalone.main(ConnectStandalone.java:118)
Caused by: org.apache.kafka.connect.runtime.rest.errors.BadRequestException: Connector configuration is invalid and contains the following 2 error(s):
Invalid value org.h2.jdbc.JdbcSQLNonTransientConnectionException: Database may be already in use: null. Possible solutions: close all other connection(s); use the server mode [90020-199] for configuration Couldn't open connection to jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
Invalid value org.h2.jdbc.JdbcSQLNonTransientConnectionException: Database may be already in use: null. Possible solutions: close all other connection(s); use the server mode [90020-199] for configuration Couldn't open connection to jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL
You can also find the above list of errors at the endpoint `/connector-plugins/{connectorType}/config/validate`
	at org.apache.kafka.connect.runtime.AbstractHerder.maybeAddConfigErrors(AbstractHerder.java:581)
	at org.apache.kafka.connect.runtime.standalone.StandaloneHerder.putConnectorConfig(StandaloneHerder.java:194)
	at org.apache.kafka.connect.cli.ConnectStandalone.main(ConnectStandalone.java:115)
[2020-07-30 19:04:22,735] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:04:22,735] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:04:22,742] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:04:22,742] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:04:22,743] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:04:22,743] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:04:22,743] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:04:22,743] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:04:22,744] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:04:22,745] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:04:22,745] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:04:48,478] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:04:48,490] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:04:48,505] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:04:48,539] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:04:48,840] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:48,841] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:48,841] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:48,841] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:48,841] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:48,841] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:48,842] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:04:49,053] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:49,053] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:49,054] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:04:49,228] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:49,228] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:49,229] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,140] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:04:50,140] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,140] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,141] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,141] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,141] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,141] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,141] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,141] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,142] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,143] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,144] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,145] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,145] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,145] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,145] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,145] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:04:50,146] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,146] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,146] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,146] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,147] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,148] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,149] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,150] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:50,150] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,150] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:50,150] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:50,150] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:50,150] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:04:50,151] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,151] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,151] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:04:50,161] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:04:50,162] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:04:50,166] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:04:50,221] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:50,221] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:50,221] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:50,221] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:50,221] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:50,221] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:50,222] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:04:50,222] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:04:50,223] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:04:50,223] INFO Kafka startTimeMs: 1596132290222 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:04:50,477] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:04:50,499] INFO Logging initialized @2692ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:04:50,564] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:04:50,565] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:04:50,573] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:04:50,610] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:04:50,610] INFO Started @2804ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:04:50,630] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:04:50,630] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:04:50,631] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:04:50,631] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:04:50,631] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:04:50,631] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:04:50,641] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:04:50,641] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:04:50,641] INFO Kafka startTimeMs: 1596132290641 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:04:50,778] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:04:50,780] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:04:50,788] INFO Kafka Connect standalone worker initialization took 2307ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:04:50,788] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:04:50,789] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:04:50,789] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:04:50,790] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:04:50,798] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:04:50,798] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:04:50,798] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:04:50,839] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:04:50,914] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:04:50,914] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:04:50,915] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:04:51,378] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:04:51,378] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:04:51,378] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:04:51,417] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:04:51,544] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:04:51,556] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:04:51,559] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:04:51,559] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:04:51,565] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:04:51,567] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:04:51,568] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:04:51,570] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:04:51,585] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:04:51,586] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:04:51,588] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:04:51,589] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:04:51,616] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:04:51,618] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:04:51,619] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:04:51,620] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:04:51,621] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:04:51,622] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:04:51,623] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:04:51,623] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:04:51,623] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:04:51,623] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:04:51,631] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:04:51,641] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:04:51,672] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:04:51,672] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:04:51,672] INFO Kafka startTimeMs: 1596132291671 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:04:51,682] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:04:51,682] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:04:51,683] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:04:51,685] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:04:51,686] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:04:51,688] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:04:52,284] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:04:52,285] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:04:52,286] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,354] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,369] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,371] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,372] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,373] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,377] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,386] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,388] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,389] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,391] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,391] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,392] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,394] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,395] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,395] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,400] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,408] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,409] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,411] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,418] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,421] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,422] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,423] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,424] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,430] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,435] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,438] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,439] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,440] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,441] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,455] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,464] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,487] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,489] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,490] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,504] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,505] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,509] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,510] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,511] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,516] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,525] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,527] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,527] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,529] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,530] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:04:52,537] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:05:01,682] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:05:01,682] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:05:01,692] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 10 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:05:08,517] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:05:08,518] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:05:08,523] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:05:08,524] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:05:08,525] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:05:08,525] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:05:08,526] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:05:08,526] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:05:08,542] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:05:08,543] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:05:08,544] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:05:08,544] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:05:08,547] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:05:08,548] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:05:08,553] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:05:08,553] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:05:08,554] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:05:08,554] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:05:08,555] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:05:08,555] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:05:08,556] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:05:08,556] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:05:08,557] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:05:08,557] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:05:56,804] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:05:56,811] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:05:56,816] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:05:56,847] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:05:57,457] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:05:57,458] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:57,458] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:57,458] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:57,458] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:57,459] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:57,460] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:05:58,060] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:05:58,060] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:58,061] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:05:58,717] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:05:58,718] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:58,719] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,736] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:05:59,737] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,737] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,737] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,737] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,737] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,737] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,737] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,738] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,738] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,738] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,738] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,738] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,738] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,739] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,739] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,739] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,739] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,739] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,739] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,739] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,740] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,741] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,742] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,742] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,742] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,742] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:05:59,743] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,743] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,743] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,743] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,744] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,744] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,744] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,744] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,745] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,746] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,747] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,747] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,747] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,747] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,747] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,747] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:05:59,747] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,747] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:05:59,748] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:05:59,748] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:05:59,748] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:05:59,748] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,748] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,748] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:05:59,762] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:05:59,763] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:05:59,767] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:05:59,817] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:05:59,817] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:05:59,817] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:05:59,817] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:05:59,817] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:05:59,818] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:05:59,818] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:05:59,818] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:05:59,818] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:05:59,818] INFO Kafka startTimeMs: 1596132359818 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:06:00,084] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:06:00,107] INFO Logging initialized @3612ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:06:00,174] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:06:00,174] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:06:00,181] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:06:00,209] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:06:00,209] INFO Started @3714ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:06:00,228] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:06:00,228] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:06:00,228] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:06:00,228] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:06:00,228] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:06:00,229] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:06:00,239] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:06:00,240] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:06:00,240] INFO Kafka startTimeMs: 1596132360239 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:06:00,358] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:06:00,359] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:06:00,366] INFO Kafka Connect standalone worker initialization took 3561ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:06:00,367] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:06:00,368] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:06:00,368] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:06:00,368] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:06:00,377] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:06:00,377] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:06:00,377] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:06:00,423] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:06:00,510] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:06:00,510] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:06:00,511] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:06:01,052] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:06:01,052] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:06:01,052] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:06:01,074] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:06:01,171] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:06:01,179] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:06:01,180] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:06:01,181] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:06:01,185] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:06:01,186] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:06:01,186] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:06:01,187] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:06:01,192] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:06:01,192] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:06:01,194] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:06:01,195] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:06:01,209] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:06:01,211] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:06:01,212] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:06:01,213] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:06:01,214] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:06:01,216] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:06:01,216] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:06:01,216] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:06:01,217] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:06:01,217] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:06:01,221] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:06:01,228] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:06:01,250] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:06:01,250] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:06:01,251] INFO Kafka startTimeMs: 1596132361250 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:06:01,259] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:06:01,262] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:06:01,263] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:06:01,263] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:06:01,266] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:06:01,267] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:06:01,271] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:06:01,272] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:06:01,272] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:06:01,272] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:06:01,273] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:06:01,273] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:06:01,274] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:06:01,275] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:06:01,276] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:06:01,276] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:06:01,277] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:06:01,278] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:06:01,279] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:06:01,279] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:06:01,280] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:06:01,280] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:06:01,280] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:06:01,280] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:06:01,280] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:06:01,291] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:06:01,293] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:06:01,293] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:06:01,308] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:06:01,355] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:06:01,356] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:06:01,356] INFO Kafka startTimeMs: 1596132361355 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:06:01,368] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:06:01,371] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): SERVICE_APPS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:06:01,372] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:06:01,373] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-worker/manager-worker-db;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:06:01,379] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:06:01,381] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:06:01,414] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:06:01,416] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:06:01,419] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:06:01,428] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:06:01,428] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:06:01,434] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-6b22cfcc-4b7b-4b5c-b5f0-178c5b8450da=Assignment(partitions=[SERVICE_APPS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:06:01,440] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:06:01,444] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: SERVICE_APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:06:01,459] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition SERVICE_APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:06:01,475] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition SERVICE_APPS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:06:01,612] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:06:01,613] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:06:01,613] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,669] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,680] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,682] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,683] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,684] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,690] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,697] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,698] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,699] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,700] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,701] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,701] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,704] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,705] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,707] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,711] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,716] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,717] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,718] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,723] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,727] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,728] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,728] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,729] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,732] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,736] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,738] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,739] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,740] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,744] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,750] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,758] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,759] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,760] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,767] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,768] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,771] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,772] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,773] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,776] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,781] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,781] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,782] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,783] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,784] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:01,788] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:06:08,374] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:06:08,374] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:06:08,380] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:06:08,380] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:06:08,381] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:06:08,381] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:06:08,382] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:06:08,382] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:06:08,382] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions SERVICE_APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:06:08,383] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-6b22cfcc-4b7b-4b5c-b5f0-178c5b8450da sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:06:08,388] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:06:08,389] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:06:08,389] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:06:08,390] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:06:08,393] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:06:08,394] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:06:08,394] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:06:08,394] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:06:08,399] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:06:08,400] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:06:08,402] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:06:08,403] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:06:08,403] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:06:08,403] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:06:08,406] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:06:08,406] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:06:08,407] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:06:08,407] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:06:08,407] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:06:08,408] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:12:41,358] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:12:41,371] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:12:41,387] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:12:41,425] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:12:41,875] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:12:41,876] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:41,876] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:41,876] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:41,877] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:41,877] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:41,907] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:12:42,243] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:12:42,243] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:42,244] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:12:42,408] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:12:42,409] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:42,410] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,534] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:12:43,534] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,534] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,535] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,536] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,537] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,538] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,539] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,539] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:12:43,539] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,540] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,540] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,540] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,540] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,540] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,540] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,540] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,541] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,541] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,541] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,541] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,541] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,541] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,541] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,542] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,543] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:12:43,543] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,544] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:12:43,544] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:12:43,544] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:12:43,544] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:12:43,544] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,544] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,545] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:12:43,555] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:12:43,556] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:12:43,560] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:12:43,614] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:12:43,614] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:12:43,614] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:12:43,614] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:12:43,614] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:12:43,614] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:12:43,614] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:12:43,615] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:12:43,615] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:12:43,615] INFO Kafka startTimeMs: 1596132763614 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:12:43,879] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:12:43,898] INFO Logging initialized @3335ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:12:43,978] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:12:43,978] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:12:43,986] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:12:44,021] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:12:44,021] INFO Started @3458ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:12:44,047] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:12:44,048] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:12:44,048] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:12:44,048] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:12:44,048] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:12:44,049] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:12:44,062] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:12:44,062] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:12:44,062] INFO Kafka startTimeMs: 1596132764062 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:12:44,193] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:12:44,195] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:12:44,202] INFO Kafka Connect standalone worker initialization took 2841ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:12:44,202] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:12:44,203] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:12:44,203] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:12:44,204] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:12:44,216] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:12:44,216] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:12:44,216] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:12:44,273] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:12:44,374] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:12:44,375] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:12:44,377] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:12:44,970] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:12:44,970] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:12:44,970] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:12:44,993] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:12:45,106] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:12:45,114] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:12:45,114] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:12:45,115] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:12:45,117] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:12:45,117] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:12:45,118] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:12:45,119] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:12:45,128] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:12:45,128] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:12:45,130] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:12:45,131] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:12:45,145] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:12:45,147] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:12:45,148] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:12:45,149] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:12:45,151] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:12:45,152] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:12:45,152] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:12:45,153] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:12:45,153] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:12:45,153] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:12:45,159] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:12:45,165] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:12:45,191] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:12:45,191] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:12:45,191] INFO Kafka startTimeMs: 1596132765191 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:12:45,199] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:12:45,199] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:12:45,199] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:12:45,200] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:12:45,200] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:12:45,201] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:12:45,208] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:12:45,209] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:12:45,209] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:12:45,209] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:12:45,210] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:12:45,210] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:12:45,211] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:12:45,212] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:12:45,213] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:12:45,213] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:12:45,214] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:12:45,214] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:12:45,215] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:12:45,215] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:12:45,216] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:12:45,216] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:12:45,216] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:12:45,216] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:12:45,216] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:12:45,217] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:12:45,218] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:12:45,218] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [SERVICE_APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:12:45,228] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:12:45,278] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:12:45,278] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:12:45,278] INFO Kafka startTimeMs: 1596132765278 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:12:45,286] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:12:45,289] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): SERVICE_APPS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:12:45,290] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:12:45,290] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:12:45,294] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:12:45,295] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:12:45,327] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:12:45,329] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:12:45,332] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:12:45,349] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:12:45,350] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:12:45,358] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-61885fd5-b825-4bfc-9e73-636acb70d57f=Assignment(partitions=[SERVICE_APPS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:12:45,373] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:12:45,378] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: SERVICE_APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:12:45,403] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition SERVICE_APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:12:45,431] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition SERVICE_APPS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:12:45,557] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:12:45,558] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:12:45,559] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,630] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,655] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,658] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,659] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,660] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,665] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,677] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,679] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,680] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,682] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,683] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,684] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,687] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,688] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,689] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,697] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,701] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,703] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,704] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,711] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,713] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,715] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,716] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,716] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,721] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,726] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,728] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,729] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,730] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,734] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,738] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,752] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,754] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,754] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,761] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,763] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,767] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,768] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,769] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,772] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,777] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,777] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,778] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,779] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,780] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:45,784] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:12:55,199] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:12:55,199] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:12:55,211] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 11 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:13:05,211] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:13:05,211] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:13:05,213] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:13:15,213] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:13:15,214] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:13:15,215] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:13:25,216] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:13:25,216] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:13:25,218] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:13:35,218] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:13:35,219] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:13:35,221] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:13:45,221] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:13:45,222] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:13:45,224] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:13:55,225] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:13:55,225] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:13:55,228] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:14:05,228] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:14:05,229] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:14:05,231] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:14:15,232] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:14:15,232] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:14:15,234] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:14:25,235] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:14:25,235] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:14:25,237] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:14:35,237] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:14:35,237] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:14:35,239] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:14:45,239] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:14:45,240] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:14:45,242] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:14:55,243] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:14:55,243] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:14:55,245] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:15:05,246] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:15:05,246] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:15:05,248] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:15:09,099] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:15:09,099] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:15:09,105] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:15:09,105] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:15:09,106] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:15:09,106] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:15:09,106] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:15:09,107] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:15:09,107] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions SERVICE_APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:15:09,108] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-61885fd5-b825-4bfc-9e73-636acb70d57f sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:15:09,112] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:15:09,113] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:15:09,113] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:15:09,114] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:15:09,208] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:15:09,208] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:15:09,209] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:15:09,209] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:15:09,211] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:15:09,211] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:15:09,213] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:15:09,214] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:15:09,214] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:15:09,214] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:15:09,215] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:15:09,215] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:15:09,215] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:15:09,216] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:15:09,216] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:15:09,216] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:15:10,734] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:15:10,740] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:15:10,755] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:15:10,782] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:15:11,081] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:15:11,082] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:11,082] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:11,082] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:11,083] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:11,083] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:11,084] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:15:11,313] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:15:11,313] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:11,313] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:15:11,468] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:15:11,468] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:11,469] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,552] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:15:12,552] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,553] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,553] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,553] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,553] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,553] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,553] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,554] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,554] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,554] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,554] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,554] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,554] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,555] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,555] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,555] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,555] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,555] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,555] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,556] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,556] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,556] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,556] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,556] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,556] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,556] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,557] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,558] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,558] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,558] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,558] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,558] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,558] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,559] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,559] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,559] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:15:12,560] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,560] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,560] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,561] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,561] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,561] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,561] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,561] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,561] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,562] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,563] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,564] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,564] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,564] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,564] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,564] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:15:12,564] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,564] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:15:12,565] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:15:12,565] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:15:12,565] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:15:12,565] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,565] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,565] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:15:12,579] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:15:12,632] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:15:12,636] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:15:12,694] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:15:12,694] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:15:12,694] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:15:12,694] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:15:12,694] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:15:12,694] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:15:12,694] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:15:12,695] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:15:12,695] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:15:12,695] INFO Kafka startTimeMs: 1596132912694 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:15:12,974] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:15:12,993] INFO Logging initialized @2560ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:15:13,057] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:15:13,057] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:15:13,065] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:15:13,099] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:15:13,099] INFO Started @2667ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:15:13,120] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:15:13,121] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:15:13,121] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:15:13,121] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:15:13,121] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:15:13,122] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:15:13,135] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:15:13,135] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:15:13,135] INFO Kafka startTimeMs: 1596132913135 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:15:13,260] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:15:13,262] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:15:13,271] INFO Kafka Connect standalone worker initialization took 2535ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:15:13,271] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:15:13,272] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:15:13,272] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:15:13,273] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:15:13,282] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:15:13,282] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:15:13,282] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:15:13,327] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:15:13,414] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:15:13,414] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:15:13,415] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:15:13,975] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:15:13,975] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:15:13,975] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:15:13,996] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:15:14,095] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:15:14,108] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:15:14,110] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:15:14,110] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:15:14,114] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:15:14,115] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:15:14,116] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:15:14,117] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:15:14,128] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:15:14,129] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:15:14,130] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:15:14,132] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:15:14,155] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:15:14,158] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:15:14,158] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:15:14,160] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:15:14,161] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:15:14,166] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:15:14,167] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:15:14,167] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:15:14,167] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:15:14,168] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:15:14,173] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:15:14,182] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:15:14,214] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:15:14,214] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:15:14,214] INFO Kafka startTimeMs: 1596132914214 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:15:14,226] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:15:14,228] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:15:14,229] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION", "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:15:14,231] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:15:14,232] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:15:14,232] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:15:14,235] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:15:14,235] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:15:14,236] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:15:14,236] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:15:14,236] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:15:14,237] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:15:14,237] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:15:14,237] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:15:14,238] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:15:14,239] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:15:14,240] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:15:14,240] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:15:14,241] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:15:14,241] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:15:14,241] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:15:14,242] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:15:14,242] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:15:14,242] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:15:14,242] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:15:14,243] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:15:14,244] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:15:14,244] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [APPS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:15:14,255] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:15:14,304] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:15:14,304] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:15:14,304] INFO Kafka startTimeMs: 1596132914304 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:15:14,312] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:15:14,313] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): APPS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:15:14,314] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:15:14,314] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:15:14,318] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:15:14,319] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:15:14,347] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:15:14,348] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:15:14,353] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:15:14,366] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:15:14,366] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:15:14,374] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 3: {connector-consumer-jdbc-sink-0-086de2e1-d976-4959-8cbf-e9600fcb5536=Assignment(partitions=[APPS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:15:14,381] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:15:14,386] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:15:14,403] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:15:14,434] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition APPS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:15:14,551] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:15:14,624] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:15:14,625] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:15:14,626] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,675] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:15:14,684] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='APPS',partition=0,offset=0,timestamp=1596132045717) with a null key and string key schema. (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='APPS',partition=0,offset=0,timestamp=1596132045717) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:15:14,692] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='APPS',partition=0,offset=0,timestamp=1596132045717) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
[2020-07-30 19:15:14,693] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:15:14,693] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:15:14,693] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:15:14,693] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions APPS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:15:14,694] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-086de2e1-d976-4959-8cbf-e9600fcb5536 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:15:14,685] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,718] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,720] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,721] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,722] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,737] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,754] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,756] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,757] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,758] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,759] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,760] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,761] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,762] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,763] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,772] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,777] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,779] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,780] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,790] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,793] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,794] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,794] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,795] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,799] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,808] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_EXCEPTION" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,810] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."LOGGING_EVENT_PROPERTY" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,811] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,812] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,817] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,822] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,835] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,836] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,837] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,849] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,851] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,853] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,854] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,855] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,860] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,865] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,870] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,871] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,872] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,873] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:14,879] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:15:24,225] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:15:24,226] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:15:24,238] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 12 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:15:34,238] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:15:34,238] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:15:34,241] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:15:44,242] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:15:44,242] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:15:44,246] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:15:54,246] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:15:54,247] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:15:54,249] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:16:04,249] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:16:04,250] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:16:04,252] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:16:14,253] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:16:14,253] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:16:14,255] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:16:24,256] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:16:24,256] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:16:24,257] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:16:34,258] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:16:34,258] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:16:34,260] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:16:44,261] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:16:44,261] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:16:44,264] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:16:54,264] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:16:54,265] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:16:54,267] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:17:04,267] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:17:04,268] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:17:04,270] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:17:14,270] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:17:14,271] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:17:14,273] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:17:24,273] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:17:24,273] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:17:24,274] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:17:34,275] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:17:34,275] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:17:34,277] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:17:44,278] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:17:44,278] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:17:44,280] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:17:54,280] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:17:54,280] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:17:54,282] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:18:04,283] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:18:04,283] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:18:04,285] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:18:14,286] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:18:14,286] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:18:14,288] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:18:24,289] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:18:24,289] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:18:24,291] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:18:34,292] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:18:34,292] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:18:34,294] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:18:44,294] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:18:44,294] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:18:44,295] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:18:54,296] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:18:54,296] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:18:54,297] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:19:04,298] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:19:04,298] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:19:04,300] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:19:14,300] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:19:14,301] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:19:14,302] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:19:24,303] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:19:24,303] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:19:24,305] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:19:34,305] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:19:34,306] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:19:34,307] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:19:44,307] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:19:44,308] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:19:44,309] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:19:54,309] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:19:54,310] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:19:54,311] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:20:04,311] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:20:04,312] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:20:04,313] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:20:14,314] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:20:14,314] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:20:14,315] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:20:24,316] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:20:24,316] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:20:24,318] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:20:34,318] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:20:34,318] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:20:34,319] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:20:44,320] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:20:44,320] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:20:44,322] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:20:54,322] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:20:54,323] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:20:54,324] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:21:04,325] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:21:04,325] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:21:04,328] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:21:14,329] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:21:14,329] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:21:14,331] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:21:24,332] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:21:24,332] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:21:24,334] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:21:34,335] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:21:34,336] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:21:34,338] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:21:44,339] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:21:44,339] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:21:44,341] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:21:54,342] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:21:54,343] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:21:54,345] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:22:04,346] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:22:04,346] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:22:04,348] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:22:14,348] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:22:14,349] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:22:14,351] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:22:24,352] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:22:24,352] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:22:24,355] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:22:34,355] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:22:34,356] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:22:34,358] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:22:44,358] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:22:44,359] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:22:44,361] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:22:54,361] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:22:54,362] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:22:54,363] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:23:04,364] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:23:04,364] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:23:04,366] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:23:14,367] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:23:14,368] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:23:14,370] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:23:24,370] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:23:24,371] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:23:24,373] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:23:34,374] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:23:34,374] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:23:34,376] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:23:44,377] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:23:44,378] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:23:44,379] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:23:54,380] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:23:54,381] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:23:54,383] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:24:04,383] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:24:04,384] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:24:04,386] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:24:14,386] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:24:14,386] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:24:14,388] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:24:24,388] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:24:24,388] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:24:24,389] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:24:34,390] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:24:34,390] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:24:34,391] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:24:44,391] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:24:44,392] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:24:44,393] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:24:54,393] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:24:54,394] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:24:54,395] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:25:04,396] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:25:04,396] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:25:04,397] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:25:14,398] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:25:14,398] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:25:14,399] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:25:24,399] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:25:24,400] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:25:24,401] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:25:34,402] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:25:34,402] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:25:34,403] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:25:44,404] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:25:44,404] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:25:44,406] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:25:47,956] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:25:47,957] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:25:47,967] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:25:47,967] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:25:47,968] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:25:47,969] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:25:47,969] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:25:47,971] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:25:47,971] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:25:47,971] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:25:47,971] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:25:47,973] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:25:47,973] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:25:47,974] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:25:47,974] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:25:47,975] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:25:47,975] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:25:47,976] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:25:47,977] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:25:47,977] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:25:47,977] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:25:47,977] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:25:47,977] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:25:47,978] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:25:47,978] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:25:47,978] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:25:47,978] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:25:49,968] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:25:49,980] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:25:49,995] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:25:50,029] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:25:50,344] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:25:50,345] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:50,345] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:50,345] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:50,345] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:50,346] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:50,347] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:25:50,557] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:25:50,557] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:50,557] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:25:50,721] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:25:50,721] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:50,723] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,744] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:25:51,744] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,744] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,744] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,744] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,744] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,745] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,746] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,747] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,748] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:25:51,749] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,749] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,749] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,750] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,751] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,751] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,751] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,751] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,751] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,751] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,751] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,752] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,753] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,753] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,753] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,753] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,753] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,753] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:25:51,753] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,754] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:25:51,754] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:25:51,754] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:25:51,754] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:25:51,755] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,755] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,755] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:25:51,767] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:25:51,768] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:25:51,772] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:25:51,830] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:25:51,831] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:25:51,831] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:25:51,831] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:25:51,831] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:25:51,831] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:25:51,831] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:25:51,832] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:25:51,832] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:25:51,832] INFO Kafka startTimeMs: 1596133551831 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:25:52,095] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:25:52,115] INFO Logging initialized @2820ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:25:52,176] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:25:52,176] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:25:52,186] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:25:52,215] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:25:52,215] INFO Started @2920ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:25:52,239] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:25:52,239] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:25:52,239] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:25:52,240] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:25:52,240] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:25:52,240] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:25:52,258] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:25:52,258] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:25:52,259] INFO Kafka startTimeMs: 1596133552258 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:25:52,379] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:25:52,380] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:25:52,389] INFO Kafka Connect standalone worker initialization took 2418ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:25:52,389] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:25:52,390] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:25:52,390] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:25:52,391] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:25:52,398] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:25:52,398] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:25:52,398] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:25:52,440] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:25:52,511] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:25:52,511] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:25:52,512] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:25:53,004] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:25:53,004] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:25:53,004] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:25:53,024] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:25:53,137] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:25:53,146] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:25:53,146] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:25:53,147] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:25:53,149] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:25:53,150] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:25:53,150] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:25:53,151] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:25:53,156] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:25:53,156] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:25:53,157] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:25:53,158] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:25:53,172] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:25:53,174] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:25:53,175] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:25:53,176] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:25:53,177] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:25:53,178] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:25:53,179] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:25:53,179] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:25:53,179] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:25:53,179] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:25:53,184] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:25:53,189] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:25:53,211] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:25:53,211] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:25:53,211] INFO Kafka startTimeMs: 1596133553211 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:25:53,218] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:25:53,219] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:25:53,220] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:25:53,220] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:25:53,221] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:25:53,222] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:25:53,229] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:25:53,229] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:25:53,230] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:25:53,230] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:25:53,231] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:25:53,231] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:25:53,231] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:25:53,231] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:25:53,232] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:25:53,233] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:25:53,233] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:25:53,234] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:25:53,234] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:25:53,235] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:25:53,235] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:25:53,235] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:25:53,235] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:25:53,235] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:25:53,235] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:25:53,236] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:25:53,236] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:25:53,237] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:25:53,245] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:25:53,286] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:25:53,286] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:25:53,286] INFO Kafka startTimeMs: 1596133553286 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:25:53,292] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:25:53,294] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:25:53,297] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:25:53,298] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:25:53,302] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:25:53,303] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:25:53,323] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:25:53,325] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:25:53,327] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:25:53,335] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:25:53,336] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:25:53,342] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-f401aa16-f769-4676-ab8f-2f3ab8d51845=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:25:53,348] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:25:53,352] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:25:53,366] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:25:53,380] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:25:53,509] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:25:53,556] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:25:53,557] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:25:53,560] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,597] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,610] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,612] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,613] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,615] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,618] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:25:53,623] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,629] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema. (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:25:53,630] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,632] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
[2020-07-30 19:25:53,633] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:25:53,633] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:25:53,633] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:25:53,633] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,633] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:25:53,634] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-f401aa16-f769-4676-ab8f-2f3ab8d51845 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:25:53,634] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,637] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,638] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,639] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,641] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,642] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,643] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,650] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,655] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,655] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,656] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,664] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,666] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,667] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,668] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,669] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,672] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,676] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,677] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,686] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,690] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,702] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,704] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,705] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,713] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,714] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,717] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,718] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,719] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,723] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,727] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,728] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,729] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,730] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,731] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:25:53,734] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:26:03,219] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:26:03,219] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:26:03,225] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:26:13,226] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:26:13,227] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:26:13,230] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:26:23,230] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:26:23,230] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:26:23,232] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:26:33,232] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:26:33,232] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:26:33,234] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:26:43,235] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:26:43,235] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:26:43,237] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:26:53,238] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:26:53,238] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:26:53,239] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:26:53,316] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:26:53,317] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:26:53,322] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:26:53,323] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:26:53,324] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:26:53,325] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:26:53,325] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:26:53,326] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:26:53,327] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:26:53,327] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:26:53,327] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:26:53,355] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:26:53,356] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:26:53,356] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:26:53,356] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:26:53,357] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:26:53,358] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:26:53,359] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:26:53,359] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:26:53,359] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:26:53,360] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:26:53,360] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:26:53,361] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:26:53,361] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:26:53,362] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:26:53,362] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:27:00,236] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:27:00,242] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:27:00,248] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:27:00,270] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:27:00,583] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:27:00,584] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:00,585] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:00,585] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:00,585] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:00,585] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:00,586] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:27:00,779] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:27:00,780] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:00,781] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:27:00,935] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:27:00,936] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:00,937] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,834] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:27:01,834] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,834] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,834] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,834] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,834] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,835] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,835] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,835] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,835] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,835] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,836] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,836] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,836] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,836] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,836] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,836] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,836] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,837] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,838] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,839] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,839] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,839] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,839] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,839] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,839] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,839] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,840] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,840] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,840] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:27:01,841] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,842] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,842] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,842] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,842] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,843] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,843] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,843] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,843] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,843] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,843] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,844] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,844] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,844] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,844] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,844] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,845] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,845] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,845] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,845] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,845] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,845] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,845] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,846] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,846] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,846] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,846] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,846] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,846] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,847] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,847] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,847] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:27:01,847] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,848] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:27:01,848] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:27:01,848] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:27:01,848] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:27:01,849] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,849] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,849] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:27:01,865] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:27:01,866] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:27:01,871] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:27:01,931] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:27:01,931] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:27:01,931] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:27:01,931] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:27:01,931] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:27:01,931] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:27:01,931] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:27:01,932] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:27:01,932] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:27:01,932] INFO Kafka startTimeMs: 1596133621932 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:27:02,191] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:27:02,216] INFO Logging initialized @2295ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:27:02,271] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:27:02,272] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:27:02,279] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:27:02,304] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:27:02,305] INFO Started @2384ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:27:02,323] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:27:02,324] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:27:02,324] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:27:02,324] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:27:02,324] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:27:02,325] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:27:02,336] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:27:02,336] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:27:02,336] INFO Kafka startTimeMs: 1596133622336 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:27:02,450] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:27:02,451] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:27:02,458] INFO Kafka Connect standalone worker initialization took 2221ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:27:02,458] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:27:02,459] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:27:02,459] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:27:02,459] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:27:02,470] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:27:02,470] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:27:02,470] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:27:02,515] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:27:02,599] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:27:02,599] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:27:02,600] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:27:03,128] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:27:03,128] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:27:03,128] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:27:03,150] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:27:03,159] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:27:03,160] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:27:03,160] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:27:03,165] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:27:03,166] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:27:03,167] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:27:03,168] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:27:03,169] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:27:03,170] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:27:03,173] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:27:03,173] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:27:03,174] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:27:03,175] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:27:03,176] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:27:03,176] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:27:03,177] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:27:03,177] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:27:03,177] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:27:03,182] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:27:03,182] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:27:03,183] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:27:03,192] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:27:03,229] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:27:03,229] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:27:03,229] INFO Kafka startTimeMs: 1596133623229 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:27:03,236] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:27:03,236] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:27:03,237] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:27:03,237] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:27:03,242] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:27:03,244] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:27:03,262] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:27:03,263] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:27:03,265] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:27:03,273] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:27:03,273] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:27:03,278] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 3: {connector-consumer-jdbc-sink-0-d9d454f1-31f2-46a4-ad25-fb8eb8f80d48=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:27:03,282] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:27:03,286] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:27:03,297] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:27:03,310] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:27:03,390] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:27:03,501] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:27:03,506] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema. (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:27:03,508] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
[2020-07-30 19:27:03,508] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:27:03,509] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:27:03,509] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:27:03,509] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:27:03,509] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-d9d454f1-31f2-46a4-ad25-fb8eb8f80d48 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:29:29,645] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:29:29,646] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:29:29,651] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:29:29,652] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:29:29,653] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:29:29,653] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:29:29,653] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:29:29,654] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:29:29,655] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:29:29,655] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:29:29,655] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:29:29,655] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:29:29,656] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:29:29,656] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:29:31,148] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:29:31,153] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:29:31,158] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:29:31,180] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:29:31,467] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:29:31,467] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:31,468] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:31,468] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:31,468] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:31,468] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:31,469] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:29:31,641] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:29:31,642] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:31,643] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:29:31,778] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:29:31,778] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:31,779] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,732] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:29:32,732] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,732] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,733] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,733] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,733] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,733] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,733] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,733] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,733] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,734] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,734] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,734] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,734] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,734] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,734] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,735] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,736] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,737] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,738] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:29:32,738] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,739] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,739] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,739] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,739] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,739] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,739] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,739] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,740] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,740] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,740] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,740] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,740] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,740] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,740] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,741] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,742] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:29:32,742] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,743] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:29:32,743] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:29:32,743] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:29:32,743] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:29:32,743] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,743] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,744] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:29:32,755] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:29:32,756] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:29:32,760] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:29:32,815] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:29:32,816] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:29:32,816] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:29:32,816] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:29:32,816] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:29:32,816] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:29:32,816] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:29:32,816] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:29:32,817] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:29:32,817] INFO Kafka startTimeMs: 1596133772816 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:29:33,068] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:29:33,090] INFO Logging initialized @2248ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:29:33,146] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:29:33,146] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:29:33,153] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:29:33,180] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:29:33,180] INFO Started @2338ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:29:33,198] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:29:33,198] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:29:33,198] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:29:33,198] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:29:33,198] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:29:33,199] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:29:33,210] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:29:33,210] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:29:33,210] INFO Kafka startTimeMs: 1596133773210 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:29:33,322] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:29:33,324] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:29:33,331] INFO Kafka Connect standalone worker initialization took 2181ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:29:33,331] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:29:33,332] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:29:33,332] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:29:33,333] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:29:33,342] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:29:33,342] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:29:33,342] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:29:33,384] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:29:33,455] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:29:33,455] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:29:33,456] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:29:34,009] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:29:34,009] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:29:34,009] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:29:34,034] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:29:34,040] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:29:34,041] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:29:34,041] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:29:34,044] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:29:34,045] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:29:34,046] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:29:34,046] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:29:34,047] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:29:34,048] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:29:34,050] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:29:34,050] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:29:34,051] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:29:34,052] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:29:34,053] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:29:34,053] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:29:34,054] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:29:34,054] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:29:34,054] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:29:34,057] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:29:34,058] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:29:34,058] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:29:34,067] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:29:34,101] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:29:34,101] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:29:34,101] INFO Kafka startTimeMs: 1596133774101 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:29:34,110] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:29:34,110] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:29:34,111] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:29:34,111] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:29:34,115] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:29:34,116] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:29:34,137] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:29:34,138] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:29:34,140] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:29:34,148] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:29:34,149] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:29:34,154] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-7265a219-2254-44d6-8432-33970107e549=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:29:34,157] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:29:34,161] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:29:34,172] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:29:34,181] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:29:34,256] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:29:34,377] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:29:34,383] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema. (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:29:34,386] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Sink connector 'jdbc-sink' is configured with 'delete.enabled=true' and 'pk.mode=record_key' and therefore requires records with a non-null key and non-null Struct or primitive key schema, but found record at (topic='USERS',partition=0,offset=0,timestamp=1596132047505) with a null key and string key schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresKey$3(RecordValidator.java:113)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:82)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
[2020-07-30 19:29:34,386] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:29:34,387] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:29:34,387] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:29:34,387] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:29:34,388] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-7265a219-2254-44d6-8432-33970107e549 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:30:33,884] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:30:33,884] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:30:33,888] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:30:33,888] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:30:33,889] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:30:33,889] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:30:33,889] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:30:33,891] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:30:33,891] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:30:33,891] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:30:33,891] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:30:33,892] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:30:33,892] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:30:33,892] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:30:35,424] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:30:35,430] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:30:35,435] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:30:35,459] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:30:35,754] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:35,755] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:35,755] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:35,755] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:35,756] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:35,756] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:35,757] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:30:35,965] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:35,965] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:35,966] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:30:36,115] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:36,115] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,116] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,985] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:36,985] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,985] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,985] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,985] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,986] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,987] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,988] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,989] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,989] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,989] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,989] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,989] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,989] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,989] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,990] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:36,990] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,991] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,991] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,991] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,991] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,992] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,992] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,992] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,992] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,992] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,992] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,992] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,993] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,994] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,995] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,995] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,995] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:36,995] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,995] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:36,996] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:36,996] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:36,996] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:36,996] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,996] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:36,996] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:37,011] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:30:37,012] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:30:37,015] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:30:37,066] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:37,067] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:37,067] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:37,067] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:37,067] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:37,067] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:37,067] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:37,068] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:30:37,068] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:30:37,068] INFO Kafka startTimeMs: 1596133837067 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:30:37,308] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:30:37,328] INFO Logging initialized @2224ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:30:37,388] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:30:37,388] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:30:37,397] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:30:37,428] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:30:37,428] INFO Started @2325ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:30:37,452] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:30:37,452] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:30:37,453] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:30:37,453] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:30:37,453] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:30:37,453] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:30:37,465] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:30:37,465] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:30:37,465] INFO Kafka startTimeMs: 1596133837465 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:30:37,578] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:30:37,580] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:30:37,590] INFO Kafka Connect standalone worker initialization took 2164ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:30:37,590] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:30:37,592] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:30:37,592] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:30:37,592] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:30:37,601] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:30:37,601] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:30:37,601] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:30:37,652] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:30:37,736] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:30:37,737] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:30:37,738] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:30:38,215] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:30:38,215] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:30:38,215] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:30:38,252] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:30:38,258] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:30:38,258] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:38,258] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:30:38,267] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:30:38,269] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:30:38,270] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:30:38,270] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:38,271] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:30:38,272] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:30:38,274] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:30:38,274] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:38,275] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:30:38,275] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:30:38,276] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:30:38,276] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:30:38,276] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:30:38,277] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:30:38,277] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:30:38,280] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:30:38,280] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:30:38,281] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:38,289] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:30:38,325] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:30:38,325] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:30:38,325] INFO Kafka startTimeMs: 1596133838325 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:30:38,331] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:30:38,332] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:30:38,332] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:30:38,333] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:30:38,337] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:30:38,337] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:30:38,356] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:30:38,357] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:30:38,358] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:30:38,367] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:30:38,367] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:30:38,371] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-86e8ba0b-6955-4461-9b4d-e02e7745898e=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:30:38,374] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:30:38,376] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:30:38,385] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:30:38,395] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:30:39,876] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:30:39,876] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:30:39,882] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:30:39,883] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:30:39,884] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:30:39,884] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:30:39,885] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:30:39,885] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:30:39,886] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:30:39,886] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-86e8ba0b-6955-4461-9b4d-e02e7745898e sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:30:39,891] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:30:39,892] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:30:39,892] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:30:39,892] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:30:39,892] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:30:39,893] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:30:39,893] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:30:42,133] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:30:42,139] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:30:42,143] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:30:42,165] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:30:42,461] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:42,463] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:42,463] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:42,463] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:42,463] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:42,463] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:42,465] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:30:42,658] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:42,658] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:42,658] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:30:42,792] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:42,792] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:42,793] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,691] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:30:43,692] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,692] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,692] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,692] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,692] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,693] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,693] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,693] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,693] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,693] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,694] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,694] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,694] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,694] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,694] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,694] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,694] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,695] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,695] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,695] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,695] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,699] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,699] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,699] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,699] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,699] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,700] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,700] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,700] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,700] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,700] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,700] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,700] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,701] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,701] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,701] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,701] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,701] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,701] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,701] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,702] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,702] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,702] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:30:43,703] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,704] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,704] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,704] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,705] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,705] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,705] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,705] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,705] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,706] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,706] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,706] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,706] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,706] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,707] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,707] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,707] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,707] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,707] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,707] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,708] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,708] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,708] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,708] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,708] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,708] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,709] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,709] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,709] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,709] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,709] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,709] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:43,709] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,710] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:43,710] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:43,711] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:43,711] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:30:43,711] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,711] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,711] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:30:43,728] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:30:43,730] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:30:43,734] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:30:43,790] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:43,790] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:43,790] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:43,790] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:43,790] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:43,790] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:43,790] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:30:43,791] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:30:43,791] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:30:43,791] INFO Kafka startTimeMs: 1596133843790 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:30:44,042] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:30:44,064] INFO Logging initialized @2237ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:30:44,118] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:30:44,118] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:30:44,128] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:30:44,158] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:30:44,158] INFO Started @2332ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:30:44,175] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:30:44,176] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:30:44,176] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:30:44,176] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:30:44,176] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:30:44,177] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:30:44,187] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:30:44,187] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:30:44,187] INFO Kafka startTimeMs: 1596133844187 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:30:44,291] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:30:44,292] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:30:44,299] INFO Kafka Connect standalone worker initialization took 2164ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:30:44,299] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:30:44,300] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:30:44,300] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:30:44,300] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:30:44,308] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:30:44,308] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:30:44,308] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:30:44,348] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:30:44,428] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:30:44,428] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:30:44,429] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:30:44,898] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:30:44,898] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:30:44,898] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:30:44,936] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:30:45,036] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:30:45,046] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:30:45,048] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:45,048] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:30:45,052] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:30:45,053] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:30:45,053] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:30:45,055] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:30:45,066] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:30:45,066] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:30:45,067] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:30:45,068] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:45,080] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:30:45,084] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:30:45,084] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:45,086] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:30:45,086] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:30:45,088] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:30:45,088] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:30:45,088] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:30:45,088] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:30:45,089] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:30:45,093] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:30:45,101] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:30:45,127] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:30:45,127] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:30:45,127] INFO Kafka startTimeMs: 1596133845127 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:30:45,137] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:30:45,137] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:30:45,138] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:30:45,138] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:30:45,139] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:30:45,140] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:30:45,144] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:30:45,145] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:30:45,145] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:45,146] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:30:45,147] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:30:45,147] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:30:45,148] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:30:45,148] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:45,149] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:30:45,149] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:30:45,150] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:30:45,150] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:45,151] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:30:45,151] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:30:45,151] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:30:45,151] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:30:45,152] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:30:45,152] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:30:45,152] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:30:45,152] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:30:45,153] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:30:45,153] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:30:45,162] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:30:45,212] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:30:45,212] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:30:45,212] INFO Kafka startTimeMs: 1596133845212 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:30:45,220] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:30:45,222] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:30:45,222] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:30:45,223] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:30:45,226] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:30:45,227] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:30:45,249] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:30:45,250] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:30:45,252] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:30:45,263] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:30:45,263] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:30:45,269] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 3: {connector-consumer-jdbc-sink-0-c0200bc9-deb1-4bc3-b1b1-2f0b6b33730a=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:30:45,272] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:30:45,276] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:30:45,288] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:30:45,301] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:30:45,380] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:30:45,380] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:30:45,381] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,431] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,441] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,443] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,444] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,445] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,449] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,453] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,455] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,455] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,456] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,457] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,457] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,458] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,459] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,459] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,464] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,468] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,469] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,470] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,475] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,477] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,478] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,479] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,479] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,484] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,488] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,489] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,493] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,497] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,507] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,508] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,509] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,515] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,516] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,518] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,519] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,519] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,523] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,526] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,527] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,528] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,528] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,529] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,531] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:30:45,556] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:30:45,634] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:30:45,652] INFO Checking Generic dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:548)
[2020-07-30 19:30:45,657] INFO Using Generic dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:556)
[2020-07-30 19:30:45,658] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: null (STRING) type doesn't have a mapping to the SQL database column type (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: null (STRING) type doesn't have a mapping to the SQL database column type
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getSqlType(GenericDatabaseDialect.java:1818)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnSpec(GenericDatabaseDialect.java:1734)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.lambda$writeColumnsSpec$33(GenericDatabaseDialect.java:1723)
	at io.confluent.connect.jdbc.util.ExpressionBuilder.append(ExpressionBuilder.java:558)
	at io.confluent.connect.jdbc.util.ExpressionBuilder$BasicListBuilder.of(ExpressionBuilder.java:597)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnsSpec(GenericDatabaseDialect.java:1725)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.buildCreateTableStatement(GenericDatabaseDialect.java:1648)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:92)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:30:45,659] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: null (STRING) type doesn't have a mapping to the SQL database column type
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getSqlType(GenericDatabaseDialect.java:1818)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnSpec(GenericDatabaseDialect.java:1734)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.lambda$writeColumnsSpec$33(GenericDatabaseDialect.java:1723)
	at io.confluent.connect.jdbc.util.ExpressionBuilder.append(ExpressionBuilder.java:558)
	at io.confluent.connect.jdbc.util.ExpressionBuilder$BasicListBuilder.of(ExpressionBuilder.java:597)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnsSpec(GenericDatabaseDialect.java:1725)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.buildCreateTableStatement(GenericDatabaseDialect.java:1648)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:92)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
[2020-07-30 19:30:45,660] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:30:45,660] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:30:45,660] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:30:45,660] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:30:45,660] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-c0200bc9-deb1-4bc3-b1b1-2f0b6b33730a sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:30:55,136] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:30:55,137] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:30:55,147] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 10 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:31:05,147] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:31:05,148] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:31:05,150] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:31:15,151] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:31:15,151] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:31:15,154] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:31:19,003] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:31:19,003] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:31:19,007] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:31:19,007] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:31:19,008] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:31:19,008] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:31:19,008] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:31:19,009] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:31:19,010] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:31:19,010] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:31:19,010] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:31:19,043] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:31:19,043] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:19,044] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:31:19,044] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:31:19,046] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:31:19,046] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:31:19,048] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:31:19,048] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:31:19,048] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:31:19,049] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:19,049] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:31:19,049] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:31:19,050] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:31:19,050] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:31:19,051] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:31:19,051] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:31:20,481] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:31:20,487] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:31:20,491] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:31:20,513] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:31:20,822] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:31:20,823] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:20,824] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:20,824] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:20,824] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:20,825] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:20,826] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:31:21,015] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:31:21,015] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:21,015] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:31:21,158] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:31:21,158] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:21,159] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,023] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,024] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,025] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,025] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,025] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,025] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,025] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,026] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,026] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,026] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,026] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,026] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,026] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,026] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,027] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,027] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,027] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,027] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,027] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,027] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,027] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,028] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,028] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,028] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,028] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,028] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,028] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,028] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,029] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,029] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,029] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,029] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,029] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,029] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,029] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,030] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:31:22,030] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,031] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,031] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,031] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,031] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,031] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,032] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,032] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,032] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,032] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,032] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,033] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,034] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,035] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,035] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:31:22,035] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,035] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:31:22,035] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:31:22,035] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:31:22,035] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:31:22,036] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,036] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,036] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:31:22,047] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:31:22,048] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:31:22,051] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:31:22,106] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:31:22,107] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:31:22,107] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:31:22,107] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:31:22,107] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:31:22,107] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:31:22,107] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:31:22,108] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:31:22,108] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:31:22,108] INFO Kafka startTimeMs: 1596133882107 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:31:22,338] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:31:22,358] INFO Logging initialized @2174ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:31:22,409] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:31:22,409] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:31:22,415] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:31:22,443] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:31:22,444] INFO Started @2260ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:31:22,470] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:31:22,471] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:31:22,471] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:31:22,471] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:31:22,471] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:31:22,472] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:31:22,481] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:31:22,482] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:31:22,482] INFO Kafka startTimeMs: 1596133882481 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:31:22,590] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:31:22,591] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:31:22,597] INFO Kafka Connect standalone worker initialization took 2114ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:31:22,597] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:31:22,598] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:31:22,598] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:31:22,599] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:31:22,606] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:31:22,606] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:31:22,606] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:31:22,649] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:31:22,724] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:31:22,724] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:31:22,725] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:31:23,225] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:31:23,225] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:31:23,225] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:31:23,260] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:31:23,352] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:31:23,360] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:31:23,361] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:31:23,361] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:31:23,365] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:31:23,365] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:31:23,366] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:31:23,367] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:23,371] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:31:23,372] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:31:23,373] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:31:23,373] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:31:23,383] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:31:23,385] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:31:23,386] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:31:23,387] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:31:23,388] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:31:23,389] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:31:23,389] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:31:23,390] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:31:23,390] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:31:23,390] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:31:23,394] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:31:23,401] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:31:23,422] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:31:23,423] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:31:23,423] INFO Kafka startTimeMs: 1596133883422 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:31:23,430] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:31:23,430] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:31:23,430] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:31:23,431] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:31:23,432] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:31:23,432] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:23,435] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:31:23,435] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:31:23,436] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:31:23,436] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:31:23,436] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:31:23,436] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:31:23,437] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:31:23,437] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:31:23,438] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:31:23,439] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:31:23,440] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:31:23,440] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:31:23,441] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:31:23,441] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:31:23,441] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:31:23,441] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:31:23,441] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:31:23,442] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:31:23,442] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:31:23,442] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:31:23,443] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:31:23,443] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:31:23,450] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:31:23,494] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:31:23,494] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:31:23,494] INFO Kafka startTimeMs: 1596133883494 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:31:23,505] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:31:23,507] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:31:23,508] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:31:23,508] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:31:23,511] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:23,512] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:31:23,531] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:31:23,532] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:31:23,535] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:31:23,545] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:31:23,545] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:31:23,550] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 5: {connector-consumer-jdbc-sink-0-baf7bb6b-2cd9-45db-8a28-d6631e3b1740=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:31:23,554] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:31:23,558] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:31:23,570] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:31:23,588] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:31:23,650] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:23,694] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:31:23,695] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:31:23,696] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,733] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,742] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,744] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,745] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:23,745] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,746] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,750] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,755] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,756] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,758] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,758] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,759] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,760] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,762] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,763] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,767] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:23,767] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,773] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:23,773] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,774] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:23,777] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,778] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,780] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,783] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:23,789] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,791] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,792] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,793] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,795] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,799] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,804] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,805] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,811] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,817] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,850] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,852] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,853] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,868] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:23,870] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:23,872] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:23,874] WARN Write of 7 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:23,876] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:23,876] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:23,877] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:23,878] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,879] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,882] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,883] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,884] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,887] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,891] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,891] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,892] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,893] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,893] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:23,897] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:31:26,878] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:26,879] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:26,882] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:26,884] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:26,884] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:26,887] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:26,907] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:26,908] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:26,909] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:26,909] WARN Write of 7 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:26,909] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:26,910] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:26,910] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:29,912] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:29,913] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:29,915] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:29,916] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:29,916] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:29,918] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:29,936] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:29,937] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:29,937] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:29,938] WARN Write of 7 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:29,938] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:29,939] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:29,939] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:32,940] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:32,942] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:32,944] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:32,947] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:32,947] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:32,949] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:32,959] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:32,961] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:32,961] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:32,961] WARN Write of 7 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:32,962] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:32,963] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:32,963] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:33,430] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:31:33,430] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:31:33,438] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:31:33,498] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:33,499] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:33,500] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:33,502] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:33,502] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:33,504] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:33,520] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:33,522] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:33,522] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:33,523] WARN Write of 7 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:33,523] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:33,524] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:33,524] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:36,526] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:36,527] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:36,529] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:36,530] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:36,531] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:36,533] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:36,546] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:36,548] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:36,548] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:36,548] WARN Write of 7 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:36,549] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:36,549] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:36,549] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:39,551] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:39,552] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:39,553] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:39,554] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:39,554] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:39,556] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:39,565] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:39,566] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:39,567] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:39,567] WARN Write of 7 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:39,568] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:39,568] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:39,568] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:42,569] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:42,570] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:42,572] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:42,573] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:42,573] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:42,575] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:42,588] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:42,588] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:42,589] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:42,589] WARN Write of 7 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:42,589] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:42,590] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:42,590] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:43,438] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:31:43,439] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:31:43,441] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:31:43,525] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:43,526] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:43,528] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:43,530] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:43,530] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:43,532] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:43,541] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:43,541] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:43,542] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:43,542] WARN Write of 7 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:43,542] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:43,543] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:43,543] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:46,544] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:46,545] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:46,547] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:46,547] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:46,548] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:46,549] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:46,556] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:46,557] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:46,558] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:46,558] WARN Write of 7 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:46,559] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:46,559] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:31:46,559] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:49,559] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:31:49,561] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:31:49,563] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:31:49,565] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:31:49,565] INFO Creating table with sql: CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:31:49,567] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""id"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"id" TEXT NOT NULL,
PRIMARY KEY("id")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:49,576] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:31:49,577] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:31:49,577] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:31:49,577] WARN Write of 7 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:31:49,578] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]
 (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:89)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:49,578] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:89)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("id") VALUES (?) ON CONFLICT ("id") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:31:49,579] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:31:49,579] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:31:49,579] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:31:49,579] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:31:49,579] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-baf7bb6b-2cd9-45db-8a28-d6631e3b1740 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:31:53,441] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:31:53,442] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:31:53,445] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:03,445] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:32:03,446] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:32:03,448] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:13,449] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:32:13,449] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:32:13,451] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:23,452] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:32:23,452] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:32:23,454] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:33,454] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:32:33,455] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:32:33,458] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:43,458] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:32:43,458] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:32:43,460] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:44,804] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:32:44,804] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:32:44,808] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:32:44,809] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:32:44,809] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:32:44,809] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:32:44,810] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:32:44,811] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:32:44,811] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:32:44,811] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:32:44,811] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:32:44,812] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:32:44,812] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:32:44,813] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:32:44,813] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:32:44,814] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:44,814] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:32:44,816] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:32:44,816] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:32:44,816] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:32:44,816] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:32:44,817] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:32:44,818] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:32:44,818] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:32:44,819] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:32:44,819] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:32:44,819] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:32:46,183] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:32:46,189] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:32:46,193] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:32:46,214] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:32:46,498] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:32:46,499] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:46,499] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:46,500] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:46,500] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:46,500] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:46,501] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:32:46,683] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:32:46,683] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:46,684] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:32:46,842] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:32:46,842] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:46,844] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,863] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,864] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,865] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,866] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,867] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,868] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,868] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,868] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,868] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,868] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:32:47,869] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,869] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,869] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,869] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,870] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,870] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,870] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,870] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,870] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,871] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,871] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,871] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,871] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,871] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,872] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,872] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,872] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,872] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,872] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,873] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,873] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,873] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,873] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,873] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,873] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,874] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,874] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,874] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,874] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,874] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,874] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,874] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:32:47,875] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,875] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:32:47,876] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:32:47,876] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:32:47,876] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:32:47,876] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,876] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,876] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:32:47,890] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:32:47,890] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:32:47,895] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:32:47,955] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:32:47,955] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:32:47,955] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:32:47,955] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:32:47,955] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:32:47,955] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:32:47,956] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:32:47,956] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:32:47,956] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:32:47,956] INFO Kafka startTimeMs: 1596133967956 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:32:48,237] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:32:48,261] INFO Logging initialized @2389ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:32:48,332] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:32:48,332] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:32:48,342] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:32:48,374] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:32:48,375] INFO Started @2503ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:32:48,394] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:32:48,394] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:32:48,395] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:32:48,395] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:32:48,395] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:32:48,395] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:32:48,405] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:32:48,405] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:32:48,405] INFO Kafka startTimeMs: 1596133968405 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:32:48,523] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:32:48,525] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:32:48,533] INFO Kafka Connect standalone worker initialization took 2348ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:32:48,533] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:32:48,535] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:32:48,535] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:32:48,536] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:32:48,544] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:32:48,544] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:32:48,544] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:32:48,588] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:32:48,678] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:32:48,678] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:32:48,679] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:32:49,210] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:32:49,211] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:32:49,211] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:32:49,241] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:32:49,330] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:32:49,340] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:32:49,341] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:32:49,341] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:32:49,345] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:32:49,346] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:32:49,346] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:32:49,347] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:32:49,352] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:32:49,352] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:32:49,353] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:32:49,354] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:32:49,371] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:32:49,374] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:32:49,374] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:32:49,376] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:32:49,377] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:32:49,378] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:32:49,378] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:32:49,379] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:32:49,379] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:32:49,379] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:32:49,386] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:32:49,392] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:32:49,417] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:32:49,417] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:32:49,417] INFO Kafka startTimeMs: 1596133969417 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:32:49,427] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:32:49,428] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:32:49,428] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:32:49,429] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:32:49,430] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:32:49,430] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:32:49,432] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:32:49,433] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:32:49,433] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:32:49,433] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:32:49,434] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:32:49,434] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:32:49,434] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:32:49,435] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:32:49,444] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:32:49,445] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:32:49,446] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:32:49,447] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:32:49,447] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:32:49,448] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:32:49,448] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:32:49,448] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:32:49,448] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:32:49,449] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:32:49,449] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:32:49,450] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:32:49,450] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:32:49,450] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:32:49,460] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:32:49,499] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:32:49,499] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:32:49,499] INFO Kafka startTimeMs: 1596133969499 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:32:49,507] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:32:49,508] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:32:49,508] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:32:49,508] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:32:49,511] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:32:49,511] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:32:49,529] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:32:49,530] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:32:49,532] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:32:49,541] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:32:49,541] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:32:49,547] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 7: {connector-consumer-jdbc-sink-0-8b9ada03-0af1-4e56-9d43-eed66f5692c7=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:32:49,552] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:32:49,555] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:32:49,567] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:32:49,579] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:32:49,649] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:32:49,683] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:32:49,683] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:32:49,684] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,721] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,732] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,733] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,735] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,736] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,740] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,746] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,747] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,748] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,749] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,749] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,750] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,750] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:32:49,751] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,752] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,752] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,756] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,763] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,764] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,764] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,769] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,771] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,772] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,772] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,772] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:32:49,773] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,776] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,776] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:32:49,778] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:32:49,781] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,783] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,788] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:49,789] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,794] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,805] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,806] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,806] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,817] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,818] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,827] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,828] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,829] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,846] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,854] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,854] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,854] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:32:49,855] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,856] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,856] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,856] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:32:49,858] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:32:49,861] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:32:49,862] WARN Write of 24 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:49,863] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:32:49,864] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:32:49,864] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:32:52,866] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:32:52,867] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:32:52,869] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:32:52,871] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:32:52,871] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:32:52,872] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:52,893] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:32:52,894] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:32:52,894] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:32:52,895] WARN Write of 24 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:52,895] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:32:52,896] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:32:52,896] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:32:55,897] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:32:55,898] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:32:55,900] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:32:55,903] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:32:55,904] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:32:55,906] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:55,925] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:32:55,926] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:32:55,927] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:32:55,927] WARN Write of 24 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:55,928] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:32:55,928] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:32:55,928] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:32:58,930] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:32:58,931] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:32:58,933] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:32:58,935] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:32:58,935] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:32:58,938] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:58,952] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:32:58,953] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:32:58,953] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:32:58,954] WARN Write of 24 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:58,955] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:32:58,955] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:32:58,955] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:32:59,427] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:32:59,427] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:32:59,437] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 10 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:32:59,504] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:32:59,506] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:32:59,509] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:32:59,511] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:32:59,511] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:32:59,513] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:59,525] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:32:59,526] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:32:59,526] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:32:59,526] WARN Write of 24 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:32:59,527] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:32:59,527] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:32:59,527] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:33:02,528] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:33:02,530] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:33:02,531] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:33:02,532] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:33:02,533] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:33:02,534] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:33:02,549] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:33:02,550] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:33:02,550] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:33:02,550] WARN Write of 24 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:33:02,551] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:33:02,551] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:33:02,551] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:33:03,318] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:33:03,318] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:33:03,326] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:33:03,327] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:33:03,327] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:33:03,328] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:33:03,328] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:33:03,328] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:33:03,328] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:33:03,329] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-8b9ada03-0af1-4e56-9d43-eed66f5692c7 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:33:03,333] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:33:03,334] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:33:03,334] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:33:03,334] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:33:03,369] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:33:03,369] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:33:03,370] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:33:03,370] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:33:03,371] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:33:03,371] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:33:03,374] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:33:03,374] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:33:03,374] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:33:03,375] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:33:03,375] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:33:03,376] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:33:03,376] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:33:03,377] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:33:03,377] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:33:03,377] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:33:18,298] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:33:18,306] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:33:18,313] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:33:18,336] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:33:18,646] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:18,647] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:18,647] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:18,647] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:18,647] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:18,648] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:18,648] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:33:18,855] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:18,856] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:18,856] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:33:19,019] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:19,019] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,020] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,971] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:19,971] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,971] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,972] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,973] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,974] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,975] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,976] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,976] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:19,976] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,976] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,977] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,978] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,979] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:19,980] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,980] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:19,980] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:19,981] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:19,981] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:19,981] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,981] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,981] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:19,993] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:33:19,993] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:33:19,997] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:33:20,046] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:20,047] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:20,047] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:20,047] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:20,047] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:20,047] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:20,047] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:20,048] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:20,048] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:20,048] INFO Kafka startTimeMs: 1596134000047 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:20,298] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:33:20,318] INFO Logging initialized @2433ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:33:20,384] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:33:20,384] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:33:20,392] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:33:20,420] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:33:20,421] INFO Started @2536ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:33:20,439] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:33:20,439] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:33:20,440] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:33:20,440] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:33:20,440] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:33:20,440] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:33:20,451] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:20,451] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:20,451] INFO Kafka startTimeMs: 1596134000451 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:20,586] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:20,588] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:20,597] INFO Kafka Connect standalone worker initialization took 2297ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:33:20,597] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:33:20,598] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:33:20,598] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:33:20,599] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:33:20,608] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:33:20,608] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:33:20,608] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:33:20,655] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:33:20,731] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:33:20,731] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:33:20,732] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:33:21,243] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:33:21,244] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:33:21,244] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:33:21,266] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:33:21,357] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:33:21,369] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:21,371] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:21,371] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:33:21,375] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:33:21,376] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:33:21,377] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:33:21,378] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:33:21,385] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:33:21,385] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:33:21,387] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:33:21,387] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:21,404] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:33:21,407] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:21,408] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:21,410] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:33:21,411] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:33:21,412] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:33:21,413] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:33:21,413] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:21,413] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:33:21,413] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:33:21,420] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:33:21,431] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:33:21,462] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:21,462] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:21,463] INFO Kafka startTimeMs: 1596134001462 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:21,474] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:33:21,475] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:33:21,475] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:33:21,475] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:33:21,476] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:33:21,475] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:33:21,489] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:33:21,490] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:21,490] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:21,490] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:33:21,491] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:33:21,491] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:33:21,492] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:33:21,492] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:21,493] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:33:21,493] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:33:21,494] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:21,494] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:21,495] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:33:21,495] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:33:21,496] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:33:21,496] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:33:21,497] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:21,497] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:33:21,497] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:33:21,498] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:33:21,499] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:33:21,499] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:21,509] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:33:21,540] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:33:21,541] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:33:21,560] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:21,561] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:21,561] INFO Kafka startTimeMs: 1596134001560 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:21,568] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:33:21,570] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:33:21,570] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:33:21,570] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:33:21,571] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:33:21,571] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:33:21,575] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:33:21,576] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:33:21,577] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:33:21,578] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:33:21,578] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:33:21,582] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:33:21,587] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:33:21,588] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:33:21,588] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:33:21,775] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:33:21,775] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:33:21,776] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:33:21,776] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:33:21,776] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:33:21,776] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:33:21,779] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:33:21,780] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:33:21,780] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:33:21,780] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:33:21,782] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:33:21,782] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:33:21,782] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:33:21,783] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:33:21,784] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:33:21,784] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:33:26,095] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:33:26,100] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:33:26,107] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:33:26,127] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:33:26,419] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:26,420] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:26,420] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:26,420] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:26,421] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:26,421] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:26,422] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:33:26,623] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:26,623] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:26,624] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:33:26,782] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:26,783] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:26,785] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,647] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:33:27,648] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,648] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,648] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,648] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,648] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,648] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,648] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,649] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,650] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,651] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,652] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,652] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,652] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,652] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:33:27,653] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,653] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,653] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,654] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,654] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,654] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,654] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,654] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,655] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,655] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,655] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,655] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,655] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,656] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,656] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,656] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,656] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,656] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,656] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,656] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,657] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,657] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,657] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,657] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,657] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,657] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,657] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,658] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,658] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,658] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,658] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,658] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:27,658] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,659] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:27,659] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:27,659] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:27,659] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:33:27,660] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,660] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,660] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:33:27,677] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:33:27,678] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:33:27,683] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:33:27,738] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:27,738] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:27,739] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:27,739] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:27,739] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:27,739] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:27,739] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:33:27,739] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:27,740] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:27,740] INFO Kafka startTimeMs: 1596134007739 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:28,029] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:33:28,052] INFO Logging initialized @2254ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:33:28,120] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:33:28,121] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:33:28,131] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:33:28,157] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:33:28,157] INFO Started @2359ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:33:28,178] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:33:28,178] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:33:28,178] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:33:28,178] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:33:28,179] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:33:28,179] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:33:28,192] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:28,192] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:28,192] INFO Kafka startTimeMs: 1596134008192 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:28,327] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:28,329] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:28,338] INFO Kafka Connect standalone worker initialization took 2242ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:33:28,338] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:33:28,340] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:33:28,340] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:33:28,341] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:33:28,350] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:33:28,351] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:33:28,351] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:33:28,400] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:33:28,477] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:33:28,477] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:33:28,479] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:33:28,940] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:33:28,940] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:33:28,940] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:33:28,962] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:33:29,050] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:33:29,058] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:29,058] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:29,059] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:33:29,061] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:33:29,062] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:33:29,063] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:33:29,064] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:33:29,068] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:33:29,068] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:33:29,070] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:33:29,070] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:29,080] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:33:29,083] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:29,083] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:29,085] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:33:29,085] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:33:29,087] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:33:29,087] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:33:29,088] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:29,088] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:33:29,088] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:33:29,091] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:33:29,097] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:33:29,118] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:29,118] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:29,118] INFO Kafka startTimeMs: 1596134009118 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:29,126] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:33:29,128] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:33:29,128] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:33:29,128] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:33:29,129] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:33:29,130] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:33:29,134] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:33:29,134] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:29,135] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:29,135] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:33:29,136] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:33:29,136] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:33:29,136] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:33:29,136] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:29,137] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:33:29,137] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:33:29,138] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:33:29,138] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:29,139] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:33:29,139] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:33:29,140] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:33:29,140] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:33:29,140] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:33:29,140] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:33:29,140] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:33:29,141] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:33:29,141] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:33:29,142] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:33:29,149] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:33:29,191] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:33:29,191] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:33:29,191] INFO Kafka startTimeMs: 1596134009191 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:33:29,197] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:33:29,199] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:33:29,199] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:33:29,200] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:33:29,203] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:33:29,205] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:33:29,227] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:33:29,229] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:33:29,231] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:33:29,239] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:33:29,239] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:33:29,246] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 9: {connector-consumer-jdbc-sink-0-28d50998-71d8-423d-b8ac-13403c39bd42=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:33:29,251] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:33:29,254] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:33:29,263] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:33:29,274] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:33:29,337] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:33:29,372] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:33:29,372] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:33:29,373] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,404] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,413] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,415] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,416] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,416] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,419] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:33:29,419] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,432] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,433] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,434] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,436] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,436] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,437] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,438] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,438] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,439] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,443] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,446] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:33:29,447] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,448] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,448] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,452] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:33:29,454] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:33:29,455] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,457] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,458] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,459] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,459] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,464] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:33:29,467] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,471] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,472] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,478] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,485] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,500] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,501] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,503] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,511] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,512] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,516] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,517] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,518] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,525] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:33:29,525] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,527] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:33:29,528] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:33:29,532] WARN Write of 27 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:76)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.command.Parser.readTableOrView(Parser.java:7146)
	at org.h2.command.Parser.readTableOrView(Parser.java:7117)
	at org.h2.command.Parser.parseInsert(Parser.java:1682)
	at org.h2.command.Parser.parsePrepared(Parser.java:891)
	at org.h2.command.Parser.parse(Parser.java:788)
	at org.h2.command.Parser.parse(Parser.java:760)
	at org.h2.command.Parser.prepareCommand(Parser.java:683)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:77)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:349)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.createPreparedStatement(GenericDatabaseDialect.java:331)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:139)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:33:29,533] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:33:29,533] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,534] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:33:29,534] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:559)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:95)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception chain:
org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "USERS" not found; SQL statement:
INSERT INTO "USERS" ("ID") VALUES (?) ON CONFLICT ("ID") DO NOTHING [42102-199]

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:86)
	... 11 more
[2020-07-30 19:33:29,535] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,536] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,537] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,538] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:29,542] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:33:30,720] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:33:30,720] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:33:30,727] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:33:30,727] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:33:30,728] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:33:30,728] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:33:30,728] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:33:30,729] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:33:30,729] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:33:30,729] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-28d50998-71d8-423d-b8ac-13403c39bd42 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:33:30,734] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:33:30,734] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:33:30,734] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:33:30,735] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:33:30,749] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:33:30,750] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:33:30,750] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:33:30,750] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:33:30,755] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:33:30,756] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:33:30,758] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:33:30,758] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:33:30,758] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:33:30,759] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:33:30,759] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:33:30,760] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:33:30,761] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:33:30,762] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:33:30,762] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:33:30,763] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:36:43,027] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:36:43,033] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:36:43,038] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:36:43,061] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:36:43,331] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:36:43,332] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:43,333] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:43,333] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:43,333] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:43,333] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:43,334] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:36:43,548] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:36:43,548] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:43,549] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:36:43,679] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:36:43,679] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:43,680] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,618] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:36:44,618] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,618] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,618] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,619] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,619] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,619] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,619] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,619] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,619] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,620] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,620] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,620] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,620] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,620] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,620] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,620] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,621] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,622] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,623] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,623] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,623] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,623] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,623] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,623] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,623] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,624] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,624] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:36:44,625] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,625] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,625] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,626] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,626] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,626] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,626] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,626] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,626] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,627] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,627] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,627] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,627] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,627] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,627] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,628] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,629] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:36:44,630] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,630] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:36:44,630] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:36:44,631] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:36:44,631] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:36:44,631] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,631] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,631] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:36:44,642] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:36:44,643] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:36:44,646] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:36:44,694] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:36:44,694] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:36:44,694] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:36:44,694] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:36:44,694] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:36:44,694] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:36:44,694] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:36:44,695] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:36:44,695] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:36:44,695] INFO Kafka startTimeMs: 1596134204694 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:36:44,946] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:36:44,968] INFO Logging initialized @2264ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:36:45,031] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:36:45,031] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:36:45,038] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:36:45,141] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:36:45,141] INFO Started @2437ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:36:45,158] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:36:45,158] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:36:45,158] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:36:45,159] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:36:45,159] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:36:45,159] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:36:45,170] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:36:45,170] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:36:45,170] INFO Kafka startTimeMs: 1596134205170 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:36:45,277] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:36:45,279] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:36:45,290] INFO Kafka Connect standalone worker initialization took 2262ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:36:45,291] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:36:45,292] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:36:45,292] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:36:45,293] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:36:45,302] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:36:45,302] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:36:45,302] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:36:45,356] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:36:45,444] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:36:45,444] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:36:45,445] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:36:45,948] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:36:45,948] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:36:45,948] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:36:45,970] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:36:46,069] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:36:46,077] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:36:46,078] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:36:46,078] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:36:46,081] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:36:46,082] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:36:46,082] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:36:46,084] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:36:46,089] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:36:46,089] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:36:46,090] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:36:46,091] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:36:46,101] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:36:46,105] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:36:46,106] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:36:46,108] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:36:46,109] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:36:46,110] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:36:46,110] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:36:46,111] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:36:46,111] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:36:46,111] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:36:46,117] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:36:46,124] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:36:46,147] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:36:46,148] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:36:46,148] INFO Kafka startTimeMs: 1596134206147 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:36:46,155] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:36:46,156] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:36:46,156] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:36:46,157] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:36:46,157] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:36:46,157] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:36:46,161] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:36:46,162] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:36:46,162] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:36:46,163] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:36:46,163] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:36:46,164] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:36:46,164] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:36:46,164] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:36:46,166] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:36:46,166] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:36:46,167] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:36:46,168] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:36:46,168] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:36:46,168] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:36:46,169] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:36:46,169] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:36:46,170] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:36:46,170] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:36:46,170] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:36:46,171] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:36:46,171] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:36:46,172] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:36:46,179] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:36:46,221] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:36:46,221] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:36:46,222] INFO Kafka startTimeMs: 1596134206221 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:36:46,238] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:36:46,240] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:36:46,240] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:36:46,241] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:36:46,244] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:36:46,245] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:36:46,264] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:36:46,265] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:36:46,267] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:36:46,275] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:36:46,275] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:36:46,280] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 11: {connector-consumer-jdbc-sink-0-50ff87d6-0ce9-44fa-8939-a2eca5dd0894=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:36:46,284] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:36:46,288] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:36:46,299] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:36:46,311] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:36:46,380] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:178)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:104)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:492)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:469)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:325)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:359)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$2(WorkerSinkTask.java:492)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:128)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:162)
	... 13 more
[2020-07-30 19:36:46,390] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:36:46,390] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:36:46,390] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:36:46,391] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-50ff87d6-0ce9-44fa-8939-a2eca5dd0894 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:36:46,457] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:36:46,457] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:36:46,458] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,503] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,517] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,518] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,519] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,520] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,524] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,529] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,530] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,531] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,532] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,532] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,533] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,534] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,535] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,536] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,540] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,544] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,545] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,546] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,552] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,555] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,555] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,556] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,557] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,560] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,566] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,567] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,573] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,577] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,604] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,606] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,607] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,623] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,625] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,630] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,631] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,632] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,635] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,640] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,641] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,642] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,643] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,644] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:46,652] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:36:56,155] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:36:56,155] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:36:56,163] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:37:06,164] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:37:06,164] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:37:06,167] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:37:12,939] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 19:37:12,939] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 19:37:12,946] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 19:37:12,946] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 19:37:12,947] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 19:37:12,947] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 19:37:12,948] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:37:12,949] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:37:12,950] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:37:12,950] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 19:37:12,950] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 19:37:12,966] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 19:37:12,966] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:37:12,967] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:37:12,967] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:37:12,969] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:37:12,970] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 19:37:12,972] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 19:37:12,972] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 19:37:12,973] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 19:37:12,973] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:37:12,973] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 19:37:12,973] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 19:37:12,974] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 19:37:12,974] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 19:37:12,974] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 19:37:12,974] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 19:37:25,200] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 19:37:25,207] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 19:37:25,213] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 19:37:25,236] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:37:25,544] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:37:25,545] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:25,545] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:25,545] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:25,545] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:25,545] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:25,546] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:37:25,742] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:37:25,742] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:25,743] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 19:37:25,885] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:37:25,885] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:25,886] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,781] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 19:37:26,781] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,781] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,781] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,781] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,782] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,782] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,782] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,782] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,782] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,783] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,783] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,783] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,783] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,783] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,783] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,783] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,784] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,785] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,786] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,786] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,786] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,786] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,786] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,786] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,786] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 19:37:26,787] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,787] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,787] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,787] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,787] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,788] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,789] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,790] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:37:26,790] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,791] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:37:26,791] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:37:26,791] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:37:26,791] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 19:37:26,791] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,791] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,791] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 19:37:26,803] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 19:37:26,803] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 19:37:26,807] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 19:37:26,856] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:37:26,856] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:37:26,856] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:37:26,856] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:37:26,856] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:37:26,856] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:37:26,856] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 19:37:26,857] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:37:26,857] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:37:26,857] INFO Kafka startTimeMs: 1596134246856 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:37:27,106] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 19:37:27,125] INFO Logging initialized @2243ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 19:37:27,179] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 19:37:27,179] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 19:37:27,186] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 19:37:27,212] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 19:37:27,212] INFO Started @2330ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 19:37:27,233] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:37:27,233] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 19:37:27,233] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:37:27,233] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 19:37:27,234] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 19:37:27,234] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 19:37:27,248] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:37:27,248] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:37:27,248] INFO Kafka startTimeMs: 1596134247247 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:37:27,366] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:37:27,368] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:37:27,377] INFO Kafka Connect standalone worker initialization took 2175ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 19:37:27,378] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 19:37:27,379] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 19:37:27,379] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 19:37:27,380] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 19:37:27,389] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 19:37:27,389] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 19:37:27,389] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 19:37:27,439] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 19:37:27,509] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 19:37:27,509] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 19:37:27,510] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 19:37:27,997] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 19:37:27,997] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 19:37:27,997] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 19:37:28,018] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:37:28,104] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:37:28,111] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:37:28,111] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:37:28,112] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:37:28,114] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:37:28,115] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 19:37:28,115] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 19:37:28,116] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:37:28,120] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 19:37:28,120] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:37:28,122] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 19:37:28,123] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:37:28,134] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:37:28,136] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:37:28,137] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:37:28,138] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:37:28,139] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:37:28,140] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:37:28,140] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:37:28,141] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:37:28,141] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:37:28,141] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:37:28,147] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 19:37:28,152] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 19:37:28,174] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:37:28,174] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:37:28,174] INFO Kafka startTimeMs: 1596134248174 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:37:28,182] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:37:28,183] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 19:37:28,184] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:37:28,184] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 19:37:28,185] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 19:37:28,186] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:37:28,189] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 19:37:28,190] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:37:28,190] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:37:28,190] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 19:37:28,191] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 19:37:28,191] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 19:37:28,191] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:37:28,192] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:37:28,192] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 19:37:28,193] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 19:37:28,194] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 19:37:28,194] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:37:28,195] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 19:37:28,195] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 19:37:28,195] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 19:37:28,196] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 19:37:28,196] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 19:37:28,196] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 19:37:28,196] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 19:37:28,197] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 19:37:28,197] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 19:37:28,197] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 19:37:28,205] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 19:37:28,246] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 19:37:28,246] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 19:37:28,246] INFO Kafka startTimeMs: 1596134248245 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 19:37:28,251] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 19:37:28,252] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 19:37:28,253] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 19:37:28,253] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 19:37:28,255] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 19:37:28,255] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 19:37:28,272] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 19:37:28,273] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 19:37:28,274] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:37:28,283] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 19:37:28,283] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 19:37:28,288] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-2c635563-433f-4f67-bb04-f3e10a9b154c=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 19:37:28,292] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 19:37:28,295] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 19:37:28,308] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 19:37:28,320] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 19:37:28,410] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 19:37:28,410] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 19:37:28,411] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,463] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,472] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,474] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,474] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,475] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,478] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,485] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,486] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,487] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,487] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,488] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,488] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,489] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,489] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,490] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,493] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,496] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,497] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,498] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,502] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,505] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,506] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,506] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,507] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,509] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,514] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,515] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,519] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,525] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,537] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,539] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,539] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,546] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,547] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,550] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,550] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,551] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,554] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,557] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,558] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,559] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,559] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,560] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,563] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 19:37:28,587] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 19:37:28,652] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 19:37:28,670] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 19:37:28,673] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 19:37:28,675] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 19:37:28,684] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,733] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,734] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,735] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,735] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,737] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:10 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,737] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,753] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,754] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,754] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,754] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,754] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:9 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,755] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,767] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,768] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,768] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,768] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,768] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:8 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,769] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,779] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,780] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,781] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,781] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,781] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:7 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,782] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,794] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,795] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,795] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,795] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,796] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:6 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,796] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,808] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,810] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,810] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,810] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,811] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:5 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,811] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,821] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,822] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,823] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,823] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,823] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:4 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,824] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,832] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,833] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,834] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,834] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,834] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:3 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,835] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,844] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,845] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,846] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,846] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,846] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:2 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,847] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,856] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,857] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,858] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,858] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,858] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:1 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,858] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 19:37:28,867] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 19:37:28,868] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 19:37:28,869] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 19:37:28,869] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 19:37:28,869] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] maxRetries:0 with SQL: [ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 19:37:28,870] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}] (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 19:37:28,887] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""USERNAME"" TEXT NULL,[*]
ADD ""ROLE"" TEXT NULL,
ADD ""EMAIL"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""FIRST_NAME"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "USERNAME" TEXT NULL,
ADD "ROLE" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "FIRST_NAME" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 19:37:28,955] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 19:37:28,955] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 19:37:28,955] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 19:37:28,956] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 19:37:28,956] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-2c635563-433f-4f67-bb04-f3e10a9b154c sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 19:37:38,183] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:37:38,183] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:37:38,190] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:37:48,191] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:37:48,191] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:37:48,193] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:37:58,193] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:37:58,194] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:37:58,195] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:38:08,195] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:38:08,196] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:38:08,197] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:38:18,197] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:38:18,198] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:38:18,200] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 19:38:28,200] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 19:38:28,201] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 19:38:28,203] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)

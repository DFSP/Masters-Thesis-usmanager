[2020-07-30 20:44:25,240] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:44:25,247] INFO WorkerSourceTask{id=jdbc-source-0} flushing 2 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:44:25,264] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 16 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:44:35,264] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:44:35,264] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:44:35,266] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:44:45,266] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:44:45,267] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:44:45,269] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:44:55,269] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:44:55,269] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:44:55,271] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:45:05,271] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:45:05,271] INFO WorkerSourceTask{id=jdbc-source-0} flushing 1 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:45:05,273] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:45:15,274] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:45:15,274] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:45:15,275] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:45:25,276] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:45:25,276] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:45:25,278] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:45:35,278] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:45:35,279] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:45:35,280] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:45:45,280] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:45:45,281] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:45:45,282] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:45:55,282] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:45:55,282] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:45:55,284] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:46:05,285] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:46:05,285] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:46:05,286] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:46:15,287] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:46:15,287] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:46:15,288] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:46:25,288] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:46:25,288] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:46:25,289] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:46:35,290] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:46:35,290] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:46:35,292] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:46:45,292] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:46:45,292] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:46:45,294] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:46:55,294] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:46:55,295] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:46:55,296] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:47:05,296] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:47:05,296] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:47:05,298] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:47:15,298] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:47:15,298] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:47:15,299] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:47:25,299] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:47:25,300] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:47:25,301] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:47:35,302] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:47:35,303] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:47:35,305] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:47:45,305] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:47:45,305] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:47:45,306] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:47:55,307] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:47:55,307] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:47:55,308] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:48:05,309] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:48:05,309] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:48:05,310] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:48:15,311] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:48:15,311] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:48:15,312] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:48:18,526] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 20:48:18,526] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 20:48:18,531] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 20:48:18,531] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 20:48:18,532] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 20:48:18,532] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 20:48:18,532] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 20:48:18,533] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 20:48:18,533] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 20:48:18,533] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 20:48:18,533] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 20:48:18,609] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 20:48:18,609] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:48:18,610] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:48:18,610] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:48:18,611] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 1 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:48:18,612] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 20:48:18,614] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 20:48:18,614] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 20:48:18,614] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 20:48:18,615] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:48:18,615] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 20:48:18,615] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 20:48:18,616] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 20:48:18,616] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 20:48:18,617] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 20:48:18,617] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 20:48:52,580] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 20:48:52,588] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 20:48:52,593] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 20:48:52,615] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 20:48:52,901] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:48:52,902] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:52,903] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:52,903] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:52,903] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:52,903] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:52,904] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 20:48:53,159] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:48:53,159] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:53,159] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 20:48:53,319] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:48:53,319] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:53,320] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,718] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,719] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,720] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,721] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,722] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,723] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,723] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:48:54,723] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,724] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,724] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,724] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,724] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,724] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,725] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,726] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,727] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:48:54,727] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,728] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:48:54,728] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:48:54,728] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:48:54,728] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:48:54,728] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,728] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,729] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:48:54,740] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 20:48:54,742] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 20:48:54,746] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 20:48:54,801] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:48:54,802] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:48:54,802] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:48:54,802] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:48:54,802] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:48:54,802] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:48:54,802] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:48:54,803] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:48:54,803] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:48:54,803] INFO Kafka startTimeMs: 1596138534802 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:48:55,066] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 20:48:55,090] INFO Logging initialized @2826ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 20:48:55,154] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 20:48:55,154] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 20:48:55,163] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 20:48:55,193] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 20:48:55,194] INFO Started @2930ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 20:48:55,212] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 20:48:55,212] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 20:48:55,213] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 20:48:55,213] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 20:48:55,213] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 20:48:55,214] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 20:48:55,225] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:48:55,226] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:48:55,226] INFO Kafka startTimeMs: 1596138535225 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:48:55,354] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:48:55,355] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:48:55,363] INFO Kafka Connect standalone worker initialization took 2780ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 20:48:55,363] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 20:48:55,364] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 20:48:55,364] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 20:48:55,364] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 20:48:55,374] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 20:48:55,374] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 20:48:55,374] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 20:48:55,412] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 20:48:55,485] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 20:48:55,486] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 20:48:55,487] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 20:48:56,093] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 20:48:56,093] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 20:48:56,093] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 20:48:56,460] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 20:48:56,670] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 20:48:56,677] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:48:56,678] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:48:56,678] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 20:48:56,682] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 20:48:56,683] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 20:48:56,683] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 20:48:56,684] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 20:48:56,692] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 20:48:56,693] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 20:48:56,694] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 20:48:56,695] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:48:56,714] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 20:48:56,716] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:48:56,717] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:48:56,719] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 20:48:56,774] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 20:48:56,775] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 20:48:56,775] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 20:48:56,775] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:48:56,775] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 20:48:56,776] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 20:48:56,781] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 20:48:56,787] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 20:48:56,844] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:48:56,844] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:48:56,844] INFO Kafka startTimeMs: 1596138536844 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:48:56,852] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 20:48:56,853] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 20:48:56,854] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 20:48:56,855] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 20:48:56,855] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 20:48:56,857] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 20:48:56,862] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 20:48:56,863] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:48:56,863] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:48:56,863] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 20:48:56,864] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 20:48:56,865] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 20:48:56,865] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 20:48:56,866] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:48:56,867] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 20:48:56,867] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 20:48:56,868] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:48:56,869] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:48:56,870] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 20:48:56,870] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 20:48:56,871] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 20:48:56,871] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 20:48:56,872] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:48:56,872] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 20:48:56,872] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 20:48:56,873] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 20:48:56,873] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 20:48:56,874] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:48:56,884] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 20:48:56,931] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:48:56,931] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:48:56,931] INFO Kafka startTimeMs: 1596138536931 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:48:56,940] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 20:48:56,943] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 20:48:56,943] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 20:48:56,944] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = PostgreSqlDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 20:48:56,948] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 20:48:56,949] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 20:48:56,970] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 20:48:56,971] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 20:48:56,973] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 20:48:56,981] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 20:48:56,982] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 20:48:56,994] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-0f4336ca-bed3-4844-afb8-3b4c41730bf1=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 20:48:57,003] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 20:48:57,007] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 20:48:57,018] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 20:48:57,034] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 20:48:57,133] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 20:48:57,137] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 20:48:57,138] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 20:48:57,139] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,177] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,196] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,197] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,204] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,205] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,211] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,219] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,220] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,220] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,221] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,222] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,223] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,226] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,227] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,228] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,233] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,237] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,238] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,239] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,243] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 20:48:57,249] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,255] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,258] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,259] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,260] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,271] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,271] INFO Checking PostgreSql dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:548)
[2020-07-30 20:48:57,277] INFO Using PostgreSql dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:556)
[2020-07-30 20:48:57,279] INFO Creating table with sql: CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) (io.confluent.connect.jdbc.sink.DbStructure:93)
[2020-07-30 20:48:57,282] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,283] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,293] WARN Create failed, will attempt amend if table already exists (io.confluent.connect.jdbc.sink.DbStructure:64)
org.h2.jdbc.JdbcSQLFeatureNotSupportedException: Feature not supported: "Index on BLOB or CLOB column: ""ID"" TEXT NOT NULL"; SQL statement:
CREATE TABLE "USERS" (
"ID" TEXT NOT NULL,
"EMAIL" TEXT NULL,
"FIRST_NAME" TEXT NULL,
"LAST_NAME" TEXT NULL,
"PASSWORD" TEXT NULL,
"ROLE" TEXT NULL,
"USERNAME" TEXT NULL,
PRIMARY KEY("ID")) [50100-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:504)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getUnsupportedException(DbException.java:269)
	at org.h2.index.BaseIndex.checkIndexColumnTypes(BaseIndex.java:76)
	at org.h2.mvstore.db.MVSecondaryIndex.<init>(MVSecondaryIndex.java:55)
	at org.h2.mvstore.db.MVTable.addIndex(MVTable.java:381)
	at org.h2.command.ddl.AlterTableAddConstraint.tryUpdate(AlterTableAddConstraint.java:151)
	at org.h2.command.ddl.AlterTableAddConstraint.update(AlterTableAddConstraint.java:78)
	at org.h2.command.ddl.CommandWithColumns.createConstraints(CommandWithColumns.java:83)
	at org.h2.command.ddl.CreateTable.update(CreateTable.java:129)
	at org.h2.command.CommandContainer.update(CommandContainer.java:133)
	at org.h2.command.Command.executeUpdate(Command.java:267)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:169)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:94)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,297] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,312] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,336] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,341] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,342] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,356] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,357] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,362] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,366] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,367] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,378] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,382] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,385] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,386] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,387] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,389] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,389] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,391] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,393] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,394] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,395] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:48:57,395] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:10 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,406] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,424] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,425] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,425] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,425] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,426] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:9 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,426] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,437] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,438] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,439] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,439] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,439] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:8 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,440] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,453] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,455] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,455] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,455] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,455] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:7 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,456] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,530] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,531] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,531] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,531] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,532] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:6 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,532] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,557] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,558] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,559] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,559] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,560] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:5 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,560] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,570] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,573] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,573] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,573] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,574] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:4 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,575] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,585] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,587] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,587] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,588] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,588] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:3 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,588] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,599] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,601] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,601] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,601] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,601] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:2 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,602] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,612] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,613] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,614] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,614] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,614] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:1 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,615] WARN Amend failed, re-attempting (io.confluent.connect.jdbc.sink.DbStructure:194)
org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:48:57,625] INFO Checking PostgreSql dialect for type of TABLE "USERS" (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:818)
[2020-07-30 20:48:57,626] WARN PostgreSql dialect did not find type for TABLE "USERS"; using TABLE (io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect:844)
[2020-07-30 20:48:57,627] INFO Refreshing metadata for table "USERS" to Table{name='"USERS"', type=TABLE columns=[Column{'ADMIN', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'ID', isPrimaryKey=false, allowsNull=true, sqlType=INTEGER}, Column{'REMARKS', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}, Column{'NAME', isPrimaryKey=false, allowsNull=true, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions:86)
[2020-07-30 20:48:57,627] INFO Unable to find fields [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] among column names [ADMIN, ID, REMARKS, NAME] (io.confluent.connect.jdbc.sink.DbStructure:247)
[2020-07-30 20:48:57,628] INFO Amending TABLE to add missing fields:[SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] maxRetries:0 with SQL: [ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL] (io.confluent.connect.jdbc.sink.DbStructure:173)
[2020-07-30 20:48:57,629] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}] (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 20:48:57,630] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to amend TABLE '"USERS"' to add missing fields: [SinkRecordField{schema=Schema{STRING}, name='FIRST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='EMAIL', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='USERNAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='PASSWORD', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='LAST_NAME', isPrimaryKey=false}, SinkRecordField{schema=Schema{STRING}, name='ROLE', isPrimaryKey=false}]
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:185)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:197)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:75)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "ALTER TABLE ""USERS"" 
ADD ""FIRST_NAME"" TEXT NULL,[*]
ADD ""EMAIL"" TEXT NULL,
ADD ""USERNAME"" TEXT NULL,
ADD ""PASSWORD"" TEXT NULL,
ADD ""LAST_NAME"" TEXT NULL,
ADD ""ROLE"" TEXT NULL"; SQL statement:
ALTER TABLE "USERS" 
ADD "FIRST_NAME" TEXT NULL,
ADD "EMAIL" TEXT NULL,
ADD "USERNAME" TEXT NULL,
ADD "PASSWORD" TEXT NULL,
ADD "LAST_NAME" TEXT NULL,
ADD "ROLE" TEXT NULL [42000-199]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:451)
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
	at org.h2.message.DbException.get(DbException.java:205)
	at org.h2.message.DbException.get(DbException.java:181)
	at org.h2.message.DbException.getSyntaxError(DbException.java:229)
	at org.h2.command.Parser.getSyntaxError(Parser.java:989)
	at org.h2.command.Parser.prepareCommand(Parser.java:686)
	at org.h2.engine.Session.prepareLocal(Session.java:627)
	at org.h2.engine.Session.prepareCommand(Session.java:565)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1292)
	at org.h2.jdbc.JdbcStatement.executeUpdateInternal(JdbcStatement.java:165)
	at org.h2.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:126)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.applyDdlStatements(GenericDatabaseDialect.java:1167)
	at io.confluent.connect.jdbc.sink.DbStructure.amendIfNecessary(DbStructure.java:181)
	... 25 more
[2020-07-30 20:48:57,631] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 20:48:57,631] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 20:48:57,632] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:48:57,633] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 20:48:57,633] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-0f4336ca-bed3-4844-afb8-3b4c41730bf1 sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 20:49:06,852] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:49:06,853] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:49:06,862] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 9 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:49:15,523] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 20:49:15,524] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 20:49:15,527] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 20:49:15,527] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 20:49:15,528] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 20:49:15,528] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 20:49:15,529] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 20:49:15,530] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 20:49:15,530] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 20:49:15,530] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 20:49:15,531] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 20:49:15,603] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 20:49:15,604] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:49:15,604] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:49:15,604] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:49:15,606] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:49:15,607] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 20:49:15,610] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 20:49:15,611] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 20:49:15,611] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 20:49:15,612] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:49:15,613] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 20:49:15,613] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 20:49:15,620] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 20:49:15,620] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 20:49:15,621] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 20:49:15,621] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-07-30 20:49:22,958] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2020-07-30 20:49:22,964] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241, 25.241-b07
	jvm.classpath = bin/../libs/activation-1.1.1.jar:bin/../libs/aopalliance-repackaged-2.5.0.jar:bin/../libs/argparse4j-0.7.0.jar:bin/../libs/audience-annotations-0.5.0.jar:bin/../libs/commons-cli-1.4.jar:bin/../libs/commons-lang3-3.8.1.jar:bin/../libs/connect-api-2.5.0.jar:bin/../libs/connect-basic-auth-extension-2.5.0.jar:bin/../libs/connect-file-2.5.0.jar:bin/../libs/connect-json-2.5.0.jar:bin/../libs/connect-mirror-2.5.0.jar:bin/../libs/connect-mirror-client-2.5.0.jar:bin/../libs/connect-runtime-2.5.0.jar:bin/../libs/connect-transforms-2.5.0.jar:bin/../libs/hk2-api-2.5.0.jar:bin/../libs/hk2-locator-2.5.0.jar:bin/../libs/hk2-utils-2.5.0.jar:bin/../libs/jackson-annotations-2.10.2.jar:bin/../libs/jackson-core-2.10.2.jar:bin/../libs/jackson-databind-2.10.2.jar:bin/../libs/jackson-dataformat-csv-2.10.2.jar:bin/../libs/jackson-datatype-jdk8-2.10.2.jar:bin/../libs/jackson-jaxrs-base-2.10.2.jar:bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:bin/../libs/jackson-module-paranamer-2.10.2.jar:bin/../libs/jackson-module-scala_2.12-2.10.2.jar:bin/../libs/jakarta.activation-api-1.2.1.jar:bin/../libs/jakarta.annotation-api-1.3.4.jar:bin/../libs/jakarta.inject-2.5.0.jar:bin/../libs/jakarta.ws.rs-api-2.1.5.jar:bin/../libs/jakarta.xml.bind-api-2.3.2.jar:bin/../libs/javassist-3.22.0-CR2.jar:bin/../libs/javassist-3.26.0-GA.jar:bin/../libs/javax.servlet-api-3.1.0.jar:bin/../libs/javax.ws.rs-api-2.1.1.jar:bin/../libs/jaxb-api-2.3.0.jar:bin/../libs/jersey-client-2.28.jar:bin/../libs/jersey-common-2.28.jar:bin/../libs/jersey-container-servlet-2.28.jar:bin/../libs/jersey-container-servlet-core-2.28.jar:bin/../libs/jersey-hk2-2.28.jar:bin/../libs/jersey-media-jaxb-2.28.jar:bin/../libs/jersey-server-2.28.jar:bin/../libs/jetty-client-9.4.24.v20191120.jar:bin/../libs/jetty-continuation-9.4.24.v20191120.jar:bin/../libs/jetty-http-9.4.24.v20191120.jar:bin/../libs/jetty-io-9.4.24.v20191120.jar:bin/../libs/jetty-security-9.4.24.v20191120.jar:bin/../libs/jetty-server-9.4.24.v20191120.jar:bin/../libs/jetty-servlet-9.4.24.v20191120.jar:bin/../libs/jetty-servlets-9.4.24.v20191120.jar:bin/../libs/jetty-util-9.4.24.v20191120.jar:bin/../libs/jopt-simple-5.0.4.jar:bin/../libs/kafka_2.12-2.5.0.jar:bin/../libs/kafka_2.12-2.5.0-sources.jar:bin/../libs/kafka-clients-2.5.0.jar:bin/../libs/kafka-log4j-appender-2.5.0.jar:bin/../libs/kafka-streams-2.5.0.jar:bin/../libs/kafka-streams-examples-2.5.0.jar:bin/../libs/kafka-streams-scala_2.12-2.5.0.jar:bin/../libs/kafka-streams-test-utils-2.5.0.jar:bin/../libs/kafka-tools-2.5.0.jar:bin/../libs/log4j-1.2.17.jar:bin/../libs/lz4-java-1.7.1.jar:bin/../libs/maven-artifact-3.6.3.jar:bin/../libs/metrics-core-2.2.0.jar:bin/../libs/netty-buffer-4.1.45.Final.jar:bin/../libs/netty-codec-4.1.45.Final.jar:bin/../libs/netty-common-4.1.45.Final.jar:bin/../libs/netty-handler-4.1.45.Final.jar:bin/../libs/netty-resolver-4.1.45.Final.jar:bin/../libs/netty-transport-4.1.45.Final.jar:bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:bin/../libs/osgi-resource-locator-1.0.1.jar:bin/../libs/paranamer-2.8.jar:bin/../libs/plexus-utils-3.2.1.jar:bin/../libs/reflections-0.9.12.jar:bin/../libs/rocksdbjni-5.18.3.jar:bin/../libs/scala-collection-compat_2.12-2.1.3.jar:bin/../libs/scala-java8-compat_2.12-0.9.0.jar:bin/../libs/scala-library-2.12.10.jar:bin/../libs/scala-logging_2.12-3.9.2.jar:bin/../libs/scala-reflect-2.12.10.jar:bin/../libs/slf4j-api-1.7.30.jar:bin/../libs/slf4j-log4j12-1.7.30.jar:bin/../libs/snappy-java-1.1.7.3.jar:bin/../libs/validation-api-2.0.1.Final.jar:bin/../libs/zookeeper-3.5.7.jar:bin/../libs/zookeeper-jute-3.5.7.jar:bin/../libs/zstd-jni-1.4.4-7.jar
	os.spec = Linux, amd64, 5.4.0-42-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-07-30 20:49:22,973] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2020-07-30 20:49:23,003] INFO Loading plugin from: /home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 20:49:23,300] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/mongodb-kafka-connect-mongodb-1.2.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:49:23,301] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:23,301] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:23,301] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:23,301] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:23,302] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:23,302] INFO Loading plugin from: /home/daniel/plugins/avroconverter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 20:49:23,506] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/avroconverter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:49:23,507] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:23,507] INFO Loading plugin from: /home/daniel/plugins/jdbcconnector (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:239)
[2020-07-30 20:49:23,674] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/daniel/plugins/jdbcconnector/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:49:23,674] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:23,675] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,578] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:262)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,579] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,580] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,581] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,581] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,581] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,581] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,584] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,585] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,585] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,585] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,585] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,585] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,585] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,585] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,586] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,587] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,587] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,587] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,587] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:191)
[2020-07-30 20:49:24,588] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,588] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,589] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,589] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,589] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,589] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,589] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,590] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,591] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,592] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,592] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,592] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,592] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,592] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,592] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,592] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:49:24,592] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,593] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:49:24,593] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:49:24,593] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:49:24,594] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:416)
[2020-07-30 20:49:24,594] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,594] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,594] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:419)
[2020-07-30 20:49:24,606] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [/home/daniel/plugins]
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:347)
[2020-07-30 20:49:24,607] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2020-07-30 20:49:24,611] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2020-07-30 20:49:24,677] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:49:24,677] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:49:24,677] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:49:24,677] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:49:24,677] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:49:24,677] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:49:24,677] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:355)
[2020-07-30 20:49:24,678] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:49:24,678] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:49:24,678] INFO Kafka startTimeMs: 1596138564677 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:49:24,985] INFO Kafka cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.connect.util.ConnectUtils:59)
[2020-07-30 20:49:25,012] INFO Logging initialized @2380ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-07-30 20:49:25,087] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:131)
[2020-07-30 20:49:25,087] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2020-07-30 20:49:25,098] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_241-b07 (org.eclipse.jetty.server.Server:359)
[2020-07-30 20:49:25,132] INFO Started http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-07-30 20:49:25,132] INFO Started @2501ms (org.eclipse.jetty.server.Server:399)
[2020-07-30 20:49:25,159] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 20:49:25,159] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:218)
[2020-07-30 20:49:25,159] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 20:49:25,160] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-07-30 20:49:25,160] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:365)
[2020-07-30 20:49:25,160] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-07-30 20:49:25,175] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:49:25,175] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:49:25,175] INFO Kafka startTimeMs: 1596138565174 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:49:25,297] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:49:25,299] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:49:25,305] INFO Kafka Connect standalone worker initialization took 2345ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2020-07-30 20:49:25,305] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-07-30 20:49:25,306] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:93)
[2020-07-30 20:49:25,306] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:184)
[2020-07-30 20:49:25,306] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2020-07-30 20:49:25,315] INFO Worker started (org.apache.kafka.connect.runtime.Worker:191)
[2020-07-30 20:49:25,315] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:95)
[2020-07-30 20:49:25,315] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2020-07-30 20:49:25,359] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:240)
[2020-07-30 20:49:25,436] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-07-30 20:49:25,437] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-07-30 20:49:25,438] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:140)
[2020-07-30 20:49:25,892] INFO Started o.e.j.s.ServletContextHandler@78c7f9b3{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-07-30 20:49:25,892] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:313)
[2020-07-30 20:49:25,892] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-07-30 20:49:25,937] INFO AbstractConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 20:49:26,039] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 20:49:26,048] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:49:26,050] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:49:26,051] INFO Creating connector jdbc-source of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 20:49:26,055] INFO Instantiated connector jdbc-source with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 20:49:26,056] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2020-07-30 20:49:26,057] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:347)
[2020-07-30 20:49:26,059] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 20:49:26,064] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2020-07-30 20:49:26,064] INFO Finished creating connector jdbc-source (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 20:49:26,066] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:347)
[2020-07-30 20:49:26,067] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:49:26,087] INFO Creating task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 20:49:26,089] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:49:26,090] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source
	tasks.max = 1
	transforms = [createKey, extractInt]
	transforms.createKey.fields = [ID]
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	transforms.extractInt.field = ID
	transforms.extractInt.type = class org.apache.kafka.connect.transforms.ExtractField$Key
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:49:26,092] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 20:49:26,094] INFO Instantiated task jdbc-source-0 with version 5.5.1 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 20:49:26,096] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 20:49:26,096] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 20:49:26,096] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:49:26,097] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 20:49:26,097] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 20:49:26,103] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker:514)
[2020-07-30 20:49:26,110] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-jdbc-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2020-07-30 20:49:26,133] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:49:26,134] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:49:26,134] INFO Kafka startTimeMs: 1596138566133 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:49:26,143] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:81)
[2020-07-30 20:49:26,143] INFO Created connector jdbc-source (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 20:49:26,144] INFO [Producer clientId=connector-producer-jdbc-source-0] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 20:49:26,144] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:h2:/home/daniel/master-thesis/usmanager/manager/manager-master/manager-master-db;MODE=POSTGRESQL;AUTO_SERVER=TRUE
	connection.user = sa
	db.timezone = UTC
	dialect.name = PostgreSqlDatabaseDialect
	incrementing.column.name = id
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = [LOGGING_EVENT, LOGGING_EVENT_EXCEPTION, LOGGING_EVENT_PROPERTY, MONITORING*]
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = ["MANAGER-MASTER-DB"."PUBLIC"."APPS", "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES", "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE", "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."FIELDS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES", "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS", "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS", "MANAGER-MASTER-DB"."PUBLIC"."REGIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS", "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS", "MANAGER-MASTER-DB"."PUBLIC"."USERS", "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES"]
	timestamp.column.name = [lastmodified]
	timestamp.delay.interval.ms = 500
	timestamp.initial = null
	topic.prefix = 
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:347)
[2020-07-30 20:49:26,146] INFO Using JDBC dialect PostgreSql (io.confluent.connect.jdbc.source.JdbcSourceTask:98)
[2020-07-30 20:49:26,147] INFO Attempting to open connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 20:49:26,150] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:347)
[2020-07-30 20:49:26,151] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:49:26,151] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:49:26,152] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:251)
[2020-07-30 20:49:26,153] INFO Instantiated connector jdbc-sink with version 5.5.1 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:254)
[2020-07-30 20:49:26,153] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:273)
[2020-07-30 20:49:26,154] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 20:49:26,154] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:49:26,155] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2020-07-30 20:49:26,156] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:419)
[2020-07-30 20:49:26,157] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:347)
[2020-07-30 20:49:26,157] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:49:26,158] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:347)
[2020-07-30 20:49:26,158] INFO Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:434)
[2020-07-30 20:49:26,158] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:347)
[2020-07-30 20:49:26,159] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:447)
[2020-07-30 20:49:26,159] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:347)
[2020-07-30 20:49:26,159] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:453)
[2020-07-30 20:49:26,159] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:460)
[2020-07-30 20:49:26,160] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:529)
[2020-07-30 20:49:26,161] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:347)
[2020-07-30 20:49:26,161] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [USERS]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:347)
[2020-07-30 20:49:26,170] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2020-07-30 20:49:26,206] INFO Kafka version: 2.5.0 (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-07-30 20:49:26,207] INFO Kafka commitId: 66563e712b0b9f84 (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-07-30 20:49:26,207] INFO Kafka startTimeMs: 1596138566206 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-07-30 20:49:26,214] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2020-07-30 20:49:26,215] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): USERS (org.apache.kafka.clients.consumer.KafkaConsumer:974)
[2020-07-30 20:49:26,216] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:44)
[2020-07-30 20:49:26,216] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:h2:mem:manager-worker-db;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=POSTGRESQL
	connection.user = sa
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [ID]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:347)
[2020-07-30 20:49:26,219] INFO Initializing writer using SQL dialect: GenericDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:57)
[2020-07-30 20:49:26,220] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)
[2020-07-30 20:49:26,236] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: Q0bc5XN6RCuQd0z97LD1KA (org.apache.kafka.clients.Metadata:280)
[2020-07-30 20:49:26,237] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator daniel:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:797)
[2020-07-30 20:49:26,238] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 20:49:26,245] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:455)
[2020-07-30 20:49:26,246] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:552)
[2020-07-30 20:49:26,251] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 3: {connector-consumer-jdbc-sink-0-fac83cac-44ec-41d3-8b1c-4b5d72f4c49b=Assignment(partitions=[USERS-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2020-07-30 20:49:26,255] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:503)
[2020-07-30 20:49:26,258] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:273)
[2020-07-30 20:49:26,267] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1299)
[2020-07-30 20:49:26,280] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition USERS-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-07-30 20:49:26,363] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:257)
[2020-07-30 20:49:26,363] INFO WorkerSourceTask{id=jdbc-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:214)
[2020-07-30 20:49:26,364] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APPS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,369] INFO Attempting to open connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:92)
[2020-07-30 20:49:26,399] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."APP_SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,407] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,409] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,410] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CLOUD_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,411] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."COMPONENT_TYPES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,415] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,422] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,423] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_LABELS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,424] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_NAMES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,425] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_ENTITY_PORTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,426] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,426] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,428] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,428] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."CONTAINER_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,429] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,435] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOSTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,439] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,440] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."EDGE_HOST_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,441] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."FIELDS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,447] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,450] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,451] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,452] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,453] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,456] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."HOST_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,460] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."MONITORING_SERVICE_LOG_TESTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,461] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."OPERATORS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,463] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:49)
[2020-07-30 20:49:26,465] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."REGIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,470] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,483] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,484] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DECISION_VALUES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,485] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_DEPENDENCIES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,489] INFO Checking Generic dialect for existence of TABLE "USERS" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:548)
[2020-07-30 20:49:26,492] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENTS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,493] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_EVENT_PREDICTIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,494] INFO Using Generic dialect TABLE "USERS" absent (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:556)
[2020-07-30 20:49:26,496] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: null (STRING) type doesn't have a mapping to the SQL database column type (org.apache.kafka.connect.runtime.WorkerSinkTask:566)
org.apache.kafka.connect.errors.ConnectException: null (STRING) type doesn't have a mapping to the SQL database column type
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getSqlType(GenericDatabaseDialect.java:1818)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnSpec(GenericDatabaseDialect.java:1734)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.lambda$writeColumnsSpec$33(GenericDatabaseDialect.java:1723)
	at io.confluent.connect.jdbc.util.ExpressionBuilder.append(ExpressionBuilder.java:558)
	at io.confluent.connect.jdbc.util.ExpressionBuilder$BasicListBuilder.of(ExpressionBuilder.java:597)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnsSpec(GenericDatabaseDialect.java:1725)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.buildCreateTableStatement(GenericDatabaseDialect.java:1648)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:92)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-07-30 20:49:26,497] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_MONITORING" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,500] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:186)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: null (STRING) type doesn't have a mapping to the SQL database column type
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getSqlType(GenericDatabaseDialect.java:1818)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnSpec(GenericDatabaseDialect.java:1734)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.lambda$writeColumnsSpec$33(GenericDatabaseDialect.java:1723)
	at io.confluent.connect.jdbc.util.ExpressionBuilder.append(ExpressionBuilder.java:558)
	at io.confluent.connect.jdbc.util.ExpressionBuilder$BasicListBuilder.of(ExpressionBuilder.java:597)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnsSpec(GenericDatabaseDialect.java:1725)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.buildCreateTableStatement(GenericDatabaseDialect.java:1648)
	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:92)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:62)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:123)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
[2020-07-30 20:49:26,500] ERROR WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
[2020-07-30 20:49:26,500] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:107)
[2020-07-30 20:49:26,501] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:49:26,501] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,501] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions USERS-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:292)
[2020-07-30 20:49:26,501] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-fac83cac-44ec-41d3-8b1c-4b5d72f4c49b sending LeaveGroup request to coordinator daniel:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:979)
[2020-07-30 20:49:26,502] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,505] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_RULE_CONDITIONS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,511] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SERVICE_SIMULATED_METRIC" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,512] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_CONTAINER_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,513] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_HOST_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,513] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."SIMULATED_SERVICE_METRICS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,514] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."USERS" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:26,518] INFO Begin using SQL query: SELECT * FROM "MANAGER-MASTER-DB"."PUBLIC"."VALUE_MODES" (io.confluent.connect.jdbc.source.TableQuerier:164)
[2020-07-30 20:49:32,149] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-07-30 20:49:32,150] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:321)
[2020-07-30 20:49:32,156] INFO Stopped http_8083@138fe6ec{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-07-30 20:49:32,156] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-07-30 20:49:32,157] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:338)
[2020-07-30 20:49:32,157] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:100)
[2020-07-30 20:49:32,158] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 20:49:32,159] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 20:49:32,159] INFO Stopped connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 20:49:32,159] INFO Stopping task jdbc-source-0 (org.apache.kafka.connect.runtime.Worker:704)
[2020-07-30 20:49:32,159] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:313)
[2020-07-30 20:49:32,222] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2020-07-30 20:49:32,223] INFO Closing connection #1 to PostgreSql (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:49:32,223] INFO WorkerSourceTask{id=jdbc-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:424)
[2020-07-30 20:49:32,223] INFO WorkerSourceTask{id=jdbc-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:441)
[2020-07-30 20:49:32,229] INFO WorkerSourceTask{id=jdbc-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:523)
[2020-07-30 20:49:32,229] INFO [Producer clientId=connector-producer-jdbc-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1182)
[2020-07-30 20:49:32,231] INFO Stopping connector jdbc-source (org.apache.kafka.connect.runtime.Worker:358)
[2020-07-30 20:49:32,231] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:174)
[2020-07-30 20:49:32,232] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2020-07-30 20:49:32,232] INFO Closing connection #1 to Generic (io.confluent.connect.jdbc.util.CachedConnectionProvider:118)
[2020-07-30 20:49:32,232] INFO Stopped connector jdbc-source (org.apache.kafka.connect.runtime.Worker:374)
[2020-07-30 20:49:32,232] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:198)
[2020-07-30 20:49:32,233] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2020-07-30 20:49:32,233] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:219)
[2020-07-30 20:49:32,233] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:117)
[2020-07-30 20:49:32,233] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)

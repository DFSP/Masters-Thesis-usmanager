----------------------------------------------------------------------------
----------------------------------------------------------------------------
Wiley series on parallel and distributed computing) Buyya, Rajkumar_ Srirama, Satish Narayana - Fog and edge computing_ principles and paradigms-John Wiley & Sons (2019) 
----------------------------------------------------------------------------
----------------------------------------------------------------------------


Parte 1 Foundations

Capitulo 1 Internet of Things (IoT) and New Computing Paradigms

- 1.1 Introdução aos novos paradigmas relacionados com IoT: indica as possiveis utilizações do IoT, enumera as limitações do cloud-based IoT, e explica as alternativas existentes.
- 1.2 Relevant Technologies
- 1.3 Fog and Edge Computing Completing the Cloud: indica as vantagens da computação fog-edge (SCALE - Security, Cognition, Agility, Latency, Efficiency) e quais são as capacidades dos nós FEC que permitem essas vantagens (SCANC - Storage, Compute, Acceleration, Networking, Control).
- 1.4 Hierarchy of Fog and Edge Computing: indica e explica as três camadas (inner-edge, middle-edge, and outer-edge) onde podem ser colocados os nós FEC. Contém os tipos de dispositivos que podem ser usados como nós FEC.
- 1.5 Business Models: Discute três modelos de negócio identificados - X as a Service, Support Service, Application Service.
- 1.6 Opportunities and Challenges de Fog-Edge computing: Out-of-Box Experience disponiveis atualmente, exemplos de plataformas fog-edge disponiveis e System Management (adjustment é o mais relevante)


Capitulo 2 Addressing the Challenges in Federating Edge Resources

- 2.1 Introduction: problema do controlo e distribuição dos nós fec e possivel solução usando federação. Discussão de networking e gestão da federação (contém imagem que pode ser parcialmente usada)
- 2.2 Networking Challenge
- 2.3 The Management Challenge: Discovery of edge nodes, Deployment of service and applications, Migrating services, Load balancing (EaaS and ENORM frameworks). Current research: Migration of services, monitoring, etc. Future research: Coordinating management task, Developing real-time benchmarking, Facilitating rapid migration, Resource allocation/deallocation for load balancing
- 2.4 Resource challenges (Defined Edge Nodes, Unified Architectures to Account for Heterogeneity, Public Usability of Edge Nodes, Interoperability with Communication Networks, Network Slices for Edge Systems) and modeling challenges. Edge simulator
- 2.5 Conclusions

M. Satyanarayanan. Edge computing: Vision and challenges. IEEE Internet of
Things Journal, 3(5): 637–646, June 2016.
N. Wang, B. Varghese, M. Matthaiou, and D. S. Nikolopoulos. ENORM:
A framework for edge node resource management. IEEE Transactions on
Services Computing, PP(99): 1–1, September 2017.
B. Varghese, N. Wang, S. Barbhuiya, P. Kilpatrick, and D. S. Nikolopoulos.
Challenges and opportunities in edge computing. In Proceedings of the
International Conference on Smart cloud, New York, USA, November
18–20, 2016.
Z. Hao, E. Novak, S. Yi, and Q. Li. Challenges and software archi-
tecture for fog computing. IEEE Internet Computing, 21(2): 44–53,
March 2011.


Capitulo 3 Integrating IoT + Fog + Cloud Infrastructures: System Modeling and Research Challenges

- 3.1 Introdução à Cloud-to-fog-to-things (C2F2T)
- 3.2 Metodologia de pesquisa e revisão de papers
- 3.3 Técnicas de modelação de sistemas C2F2T (Analytical models, Petri Nets, Integer Linear Programs, Others)
- 3.4 Relacionamento de Modelos C2F2T com casos especificos
- 3.5 Relacionamento de Modelos C2F2T com métricas (Energy Consumption, Performance, Resource Consumption, Cost, Quality of Service, Security)
- 3.7 Conclusão


Capitulo 5 Optimization Problems in Fog and Edge Computing

- 5.1 Introdução aos problemas de otimização na fog/edge computing
- 5.2 Trabalho relacionado
- 5.3 Conceitos
- 5.4 The Case for Optimization in Fog Computing
- 5.5 Formal Modeling Framework for Fog Computing
- 5.6 Metrics
- 5.7 Optimization Opportunities along the Fog Architecture
- 5.8 Optimization Opportunities along the Service Life Cycle
- 5.9 Toward a Taxonomy of Optimization Problems in Fog Computing
- 5.10 Optimization Techniques
- 5.11 Future Research Directions


Parte 2 Middlewares


Capitulo 6 Middleware for Fog and Edge Computing: Design Issues

- 6.1 Introduction
- 6.2 Need for Fog and Edge Computing Middleware
- 6.3 Design Goals (Ad-Hoc Device Discovery, Run-Time Execution Environment, Minimal Task Disruption,  Overhead of Operational Parameters, Context-Aware Adaptive Design, Quality of Service)
- 6.4 State-of-the-Art Middleware Infrastructures
- 6.5 System Model (Embedded Sensors or Actuators, Personal Devices, Fog Servers, Cloudlets, Cloud Servers)
- 6.6 Proposed Architecture (API Code, Security, Authentication, Privacy, Encryption, Device Discovery, Middleware)
- 6.7 Case Study Example (perpetrator tracking application)
- 6.8 Future Research Directions
- 6.9 Conclusions


Capitulo 7 A Lightweight Container Middleware for Edge Cloud Architectures

- 7.1 Introduction
- 7.2 Background/Related Work (Edge Cloud Architectures, A Use Case: Modern ski resorts)
- 7.3 Clusters for Lightweight Edge Clouds (Lightweight Software – Containerization, Lightweight Hardware – Raspberry Pi Clusters)
- 7.4 Architecture Management – Storage and Orchestration (OpenStack Storage, Docker Orchestration)
- 7.5 IoT Integration
- 7.6 Security Management for Edge Cloud Architectures (Blockchain)
- 7.7 Future Research Directions
- 7.8 Conclusions


Capitulo 9 Predictive Analysis to Support Fog Application Deployment

- 9.1 Introduction
- 9.2 Motivating Example: Smart Building
- 9.3 Predictive Analysis with FogTorchΠ (seleção de deployments que satisfaçam uma certa % de QoS e até um determinado custo)
- 9.4 Motivating Example
- 9.5 Related Work (Comparing iFogSim and FogTorchΠ)
- 9.6 Future Research Directions
- 9.7 Conclusions


> Informação importante

- Monitorização ao nivel do processo, aplicação e nó

- Preciso de me procupar com o dispositivo fisico (traffic routing nodes ou dedicated nodes) onde serão colocados os containers?

- Profile of users and applications para antecipar utilização e eventos?

- User mobility implica migração de serviços

- Exemplos de aplicações: smart cities, Wireless sensor network (WSN), e-health, traffic management, smart buildings, self-driving cars, autonomous domotics systems, energy production, plants, agricultural lands, supermarkets, embedded AI

- Cloud-to-things (C2T) continuum, Cloud-to-edge/fog-to-things (C2E2T/C2F2T)

- Modelos analiticos VS Petri nets Vs Programação Linear Inteira Vs Outros
Ver se nos papers referidos no livro existe informação importante para chegar à função analitica ótima (Informação resumida na pag. 81)

- Página 90 -> Tabela importante, com resumo das aplicações apresentadas em vários artigos

- Página 92 -> Tabela importante com o uso das métricas para modelar os sitemas, apresentadas em vários artigos

- Página 138 e 139 -> Tabelas com algoritmos de otimização

- Métricas: 
	Mais ou menos fáceis de quantificar:
	- Desempenho (tempo de execução) - global
	- Utilização de recursos (CPU, Memória, Bandwidth) - individual a cada nó
	- Consumo de energia (mais importante nos end devices, menos nos edge nodes, também importane na cloud devido aos custos). Minimzar o impacto ambiental do sistema fog/edge
	- Custos financeiros (uso de energia na efge/cloud, uso de rede). Podem ser fixos ou baseados na utilização
	Dificeis de quantificar:
	- Confiabilidade
	- Segurança
	- Privacidade

- Exemplos: Numa aplicação que requer execuções/respostas rápidas, o objetivo dever
a ser limitar superiormente o tempo de resposta enquanto que o consumo de energia é limitado.
Já numa aplicação que se procupera com a bateria finita e limitada do dispositivo, deve limitar superiormente o consumo de energia, enquanto o tempo de resposta é minimizado.

- 6.3 (pag 146) Apresenta vários tipos de aplicações que podem beneficiar do middleware edge computing

- (pag 147) Sistema: é composto por ad-hoc device discovery, runtime executing environment, minimal task disruption, overhead of operational parameters, context aware adaptive design, quality of service

- Pag 149: Tabela importante com os middlewares existentes atualmente

- Modulos (pag 153):
	- Seleção de dispositivos:
		- Seleção de dispositivos baseada na sua localização e reputação de completar os pedidos/tarefas dos utilizadores
		- Real-time aplications requerem que o pedido seja efetuado num curto espaço de tempo. A seleção dos dispositivo pode ser feita com "incentivized schemes by petri et al. (ref 47, pag 163)"
		- Seleção baseada no contexto (pag 154)
	- Agendamento e gestão de recursos (monitorização da disponibilidade de user devices e vms, monitorização do trabalho atribuido aos nós e edge)
	- Monitorização do contexto e previsão. Usar time series, estocástica, ou machine learning para modelar e prever as mudanças de contexto
	- Execution management (containers, migração, etc)
	- Mobility management (os dados e serviços seguem os dispositivos móveis/users. Uso do protocolo locator/Id separation (LISP))
	- Sensor/actuators são incluidos nos user devices

- Location tracking of the users (ref 51, pag 163), user activity patterns, prediction of user environment, device usage patterns

- Pag 165, 7.1, bom resumo da necessidade da edge computing e soluções do middleware

- Segurança usando tecnologia blockchain? Pag 179

- É possivel usar Raspberry pis como edge devices, na altura dos testes? Usando Hypriot OS, que é baseado no Debian e já contém o Docker CE instalado

- Pag 170, bom resumo sobre containerização

- TOSCA service orchestration standard

- Segurança e arquitetura do swarm cluster - Página 176 "Swarm cluster architecture and security"

- Service discovery (pag 176) - rever a opção de usar service mesh (consul, istio, linkerd, red hat openshift)

- Segurança entre dispositivos/sensores/rede/service provide - tecnologia blockchain (pag 179), consensus algorithm, etc

- Registration/Tracking of devices (pag 204)

- Moreover, determining deployments of a multi-component application to a given fog infrastructure, while satisfying its functional and nonfunctional constraints, is an NP-hard problem (pag 212)

- Associação de requerimentos de software e hardware a cada microserviço: associação do tipo de hardware/software, necessidades de latencia e bandwidth, a cada tipo de microserviço. Para garantir a QoS, o gestor deve garantir esses valores (pag 214)

- Pag 215. Tabela importante que pode ser usada como base para definir os tipos de edge nodes que poderão existir

- Capitulo 9 - uma adaptação do FogTorchΠ e iFogSim pode ser usado para descobrir a melhor estrategia de deployment inicial. Para isso é preciso definir os dispositivos iniciais, bem como as ligações entre eles.

- Explorar as referencias do subcaptiulo 9.5.2 Fog Application Deployment Support, para ver se há algo interessante para incluir no sistema de gestão:
	1. [43] S. Shekhar, A. Chhokra, A. Bhattacharjee, G. Aupy and A. Gokhale, INDICES: Exploiting edge resources for performance-aware cloud-hosted services. In Proceedings of the 1st IEEE International Conference on Fog and Edge Computing, Madrid, Spain, May 14, 2017
	2. ...

- Outros capitulos interessantes para mais tarde ler: 
	Exemplos de aplicações:
		11 Fog Computing Realization for Big Data Analytics, 
		12 Exploiting Fog Computing in Health Monitoring, 
		13 Smart Surveillance Video Stream Processing at the Edge for Real-Time Human Objects Tracking, 
		14 Fog Computing Model for Evolving Smart Transportation Applications, 
		15 Testing Perspectives of Fog-Based IoT Applications, 
	Simulação de uma ambiente edge computing:
		17 Modeling and Simulation of Fog and Edge Computing Environments Using iFogSim Toolkit


----------------------------------------------------------------------------
---------------------------------------------------------------------------- 
2018.cal.microservices 
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Resumindo: A adaptação do uso de microserviços força uma mudança no desenho de cloud datacenters, em aspetos como:
 - o tempo gasto em modo kernel vs modo user
 - o racio computação vs comunicação
 - o debate entre grandes vs pequenos servidores
 - a pressão colocada no desempenho da computação single-thread
 - a pressão colocada no i-cache

1. Abstract

- Mudanças que serão necessarias na cloud para acomodar a adaptação do uso de microserviços em detrimento do uso de aplicações monoliticas. Implicações do uso de microserviços no desenho de cloud datacenters

- Exemplos de microserviços (movie renting and streaming service)


1. Introduction 

- Microservices vs monolithic applications

- Introdução ao conteudo contido no paper


2. Related Work

- Cloudsuite: has been used to study the architectural implications of cloud benchmarks

- Tail-bench: aggregates a set of interactive benchmarks

- Sirius: focuses on intelligent personal assistant workloads. has been used to study the acceleration potential of interactive ML applications

- Related work tem limitações porque foca-se exclusivamente em single-tier workloads


3. The end-to-end movie streaming service

- Descrição de dois gráficos de dependencias para representar ações (ver filmes, e adicionar review) numa aplicação de movie streaming

- Resolving performance issues requires determining which microservice is the culprit of a QoS violation: RPCs and REST requests are timestamped upon arrival toand departure from each microservice, and data is accumulated, and stored in a centralized Cassandra database. We additionally track the time spent processing network request

4. Evaluation

4.1 Scalability and Query Diversity

- 3 charts with throughput-tail latency (99th percentile) curves for representative operations of the Movie Streaming service: Browse Movie Info, Add Movie Review, and Movie Rent+Stream. Across all three request types the system saturates following queueing principles, although requests that process payments for renting a movie incur much higher latencies, and saturate at much lower load compared to other requests, due 
to the high bandwidth demands of streaming large video files

4.2 Implications in Server Design

- At low load, the frontend dominates the overall latency that a request takes.
However, at high load, overall performance is now also limited by the back-end databases and the microservices that manage them. This shows that bottlenecks shift across microservices as load increases,
hence resource management must be agile, dynamic, and able to leverage tracing information to track how per-microservice latency changes over time.

- (...) denoting that current systems are poorly provisioned for microservices-based applications. The same end-to-end service built as a monolithic Java application providing the exact same functionality, and running on a single node experiences significantly reduced front-end stalls, due to the lack of network requests, which translate to an improved IPC.

- A grande variedade de microserviços, faz com que hajam diferentes locais de bottlenecks, o que torna a otimização generalizada muito dificil, e impoe também um obstaculo na criação de otimizadores costumizados

- High number of cycles spent at kernel mode because applications like memcached and
MongoDB spend most time in the kernel to handle interrupts, process TCP packets, and activate and schedule idling interactive services. The high number of library cycles is also justified given that microservices optimize for speed of development, and hence leverage a lot of existing libraries, as opposed to reimplementing the functionality from scratch

- Microservices additionally shift the computation to communication ratio in cloud applications significantly compared to monoliths. Despite the increased pressure in the network fabric, microservices allow individual components to scale independently, unlike monoliths, improving elasticity, modularity, and abstraction

- Microservices offer an appealing target for small cores, given the limited amount of computation per microservice. But, despite the higher tail latency of the end-to-end service, microservices are much more sensitive to poor single-thread performance than traditional cloud applications. Although initially
counterintuitive, this result is not surprising, given the fact that each individual microservice must meet much stricter tail latency constraints compared to an end-to-end monolith, putting more pressure on performance predictability. Low-power machines can still be used formicroservices out of the critical path, or insensitive to frequency scaling by leveraging the per-microservice characterization


- I-cache pressure:: (...) hence we conclude that it is the simplicity of microservices which results in better i-cache locality. In comparison, the monolithic design experiences extremely high i-cache misses, due to its large code footprint, and consistent with prior studies of cloud applications


----------------------------------------------------------------------------
----------------------------------------------------------------------------
The Tail at Scale
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Resumindo: O que é a tail-tolerance e como desenhar sistemas que tolerem tail-latency.

Um pedido com um valor elevado de 99% percentile aplicado a uma resposta que involva múltiplos servidores aumenta muito a % de utilizadores que experienciam uma resposta lenta. (e.g. 99% percentile de 1 segundo, numa resposta que involva 100 servidores, 63% das respostas são a cima de 1 segundo. O cálculo é 1 - 0.99^100).

Dando um exemplo, num sistema da Google, um pedido com 10ms de latencia e um 99% percentile, o conjunto dos pedidos demora 140ms, e o valor de 95% percentile é 70ms. O que quer dizer que os 5% pedidos mais lentos são responsáveis por metade do tempo de espera. Com isto, é possivel concluir que se for possivel melhorar o tempo de resposta desses 5% de pedidos, o sistema global melhora muito o seu desempenho de serviço.

O problema está em como desenhar sistemas que tolerem tail-latency, mas que continuem globalmente responsivos.
Uma forma passa por tentar reduzir a variabilidade dos tempos de resposta, ao prioritarizar pedidos interativos, e separar um workload elevado em workloads mais leves, de forma a permitir interleaving. 
Outra forma de reduzir a variabilidade é sincronizar a execução de uma tarefa de background para executar em todas as máquinas, em vez de se distribuir o workload. A razão para isso reduzir a variabilidade é o facto de todas as máquinas ficarem ocupadas ao mesmo tempo, e portanto, os tempos de resposta continuam semelhantes. Pelo contrário, se o workload for distribuido, alguns servidores podem estar sempre ocupados com tarefas de background, contribuindo para um tail-latency sempre elevado.

No então, é muito dificil limitar a variabilidade dos tempos de resposta. Existem padrões de desenho para tornar os sistemas tail-tolerance:

1. Hedged requests. Pode ser incluido no sistema de gestão?

2. Tied requests. Pode ser incluido no sistema de gestão?

3. Micro-partition. O uso de containers são uma forma de micro-partition?

4. Selectively increase replication factors: Pode ser incluido no sistema de gestão, com a componente de previsão de acontecimentos.

5. Put slow machines on probation. Pode ser incluido no sistema de gestão, com uma especie de histórico do tempo de respostas.

6. Consider ‘good enough’ responses

7. Canary requests


----------------------------------------------------------------------------
----------------------------------------------------------------------------
an-open-source-benchmark-suite-for-microservices-and-their-hardware-software-implications-for-cloud-edge-systems
----------------------------------------------------------------------------
----------------------------------------------------------------------------

http://www.csl.cornell.edu/~delimitrou/papers/2019.asplos.microservices.pdf
http://microservices.ece.cornell.edu

- Specifically we show that, similarly to traditional cloud applications, microservices spend a large fraction of time in the kernel. Unlike monolithic services though, microservices spend much more time sending and processing network requests over RPCs or other REST APIs

- microservices significantly complicate cluster management. Even though the cluster manager can scale out individual microservices on-demand instead of the entire monolith, dependencies between microservices introduce backpressure effects and cascading QoS violations that quickly propagate through the system, making performance unpredictable. Existing cluster managers that optimize for performance and/or utilization [29, 32, 33, 36, 45, 60–62, 64, 66–68, 73, 80, 84] are not expressive enough to account for the impact each pair-wise dependency has on end-to-end performance

- Principios de desenho do DeathStarBench: Representativeness, End-to-end operation, Heterogeneity, Modularity, Reconfigurability

- Descrição, incluindo os graficos de depedencias, das 6 aplicações compostas por microserviços. IMPORTANTE para o desenvolvimento das dependencias, bem como para o complementar os testes, para além do sockshop.

- Resolving performance issues requires determining which microservice(s) is the culprit of a QoS violation, which typically happens through distributed tracing - ver o tracing distribuido que foi desenvolvido, caso seja necessário

- The ratio of resources between tiers varies significantly across end-to-end services, highlighting the need for application-aware resource management.
 
- Microservices complicate cluster management, because dependencies between tiers can introduce backpressure  effects, leading to system-wide hotspots [56, 59, 82, 85, 87]. Backpressure can additionally trick the cluster manager into penalizing or upsizing a saturated microservice, even though its saturation is the result of backpressure from another, potentially not-saturated service.

- Conversely, there are microservices with relatively low utilization and degraded performance, for example, due to waiting on a blocking/synchronous request from another, saturated tier. This highlights the need for cluster managers that account for the impact dependencies between microservices have on end-to-end performance when allocating resources - IMPORTANTE para explicar a necessidade da inclusão da deteção de dependencias no sistema
 
 - However, while the cluster manager can simply instantiate new copies of the monolith and rebalance the load, autoscaling takes longer to improve performance.This is because, as shown in Fig. 20b, the autoscaler simply upsizes the resources of saturated services - seen by the progressively darker colors of highly-utilized microservices. However, services with the highest utilization are not necessarily the culprits of a QoS violation [61], taking the system much longer to identify the correct source behind the degraded performance and upsizing it. As a result, by the time the culprit is identified, long queues have already built up which take considerable time to drain.

- Such dependencies are difficult for developers or users to describe, and furthermore, they change frequently, as old microservices are swapped out and replaced by newer services.

-In general, the more complex an application’s microservices graph, the more impactful slow servers are, as the probability that a service on the critical path will be degraded increases.


https://blog.acolyer.org/2019/05/13/an-open-source-benchmark-suite-for-microservices-and-their-hardware-software-implications-for-cloud-edge-systems/

Resumo:
The paper examines the implications of microservices at the hardware, OS and networking stack, cluster
management, and application framework levels, as well as the impact of tail latency.

- 6 exemplos de aplicações baseadas em microserviços:
	- social network
	- movie reviewing
	- e-commerce website
	- banking system
	- swarm cloud
	- swarm edge

- exemplo de diagrama de dependências entre microserviços


- microservices are much more sensitive to poor single-thread performance than traditional cloud
applications. Although initially counterintuitive, this result is not surprising, given the fact that each
individual microservice must meet much stricter tail latency constraints compared to an end-to-end
monolith, putting more pressure on performance predictability.

- For interactive, latency-critical services, where even a small improvement in tail latency is significant,
network acceleration provides a major boost in performance.

- Microservices significantly complicate cluster management. Even though the cluster manager can scale out
individual microservices on-demand instead of the entire monolith, dependencies between microservices
introduce back-pressure effects and cascading QoS violations that quickly propagate through the system,
making performance unpredictable.

- Once back-end services have high utilisation this propagates all the way to the front-end, and can put even higher pressure on microservices in the middle. There are also microservices that show relatively low utilisation
but still have degraded performance (due to blocking).
The authors draw two conclusions from their analysis here:
		- Cluster managers need to account for the impact dependencies have on end-to-end performance when allocating resources (but how will they learn this?)
		- Once microservices experience a QoS violation they need longer to recover than traditional monolithic applications, even in the presence of autoscaling mechanisms which most cloud providers employ. 
Bottleneck locations also vary with load.

- The general finding is that the more complex the interaction graph of a microservices application, the
more impactful slow services are as the probability that a service on the critical path will be degraded increases.

----------------------------------------------------------------------------
----------------------------------------------------------------------------
Event Prediction in an IoT Environment Using Naïve Bayesian Models.pdf
----------------------------------------------------------------------------
----------------------------------------------------------------------------
https://www.sciencedirect.com/science/article/pii/S1877050916301168

Nota: O modelo apresentado teve apenas uma previsão correta em 54% dos casos. A justificação dada foi que o modelo considerado era demasiado simples (A more elaborate model including additional factors such as time of day or season, impacting flight delays could also improve the prediction accuracy.)

Resumo: Architecture that employs a Bayesian event prediction model that uses historical event data generated by the IoT cloud to calculate the probability of future events


1. Introduction

- In many IoT environments event causality is not known, and thus probabilistic approaches must be used to predict the probability of subsequent events occurring, based on the observation of other events.

- In this paper we use the Bayesian approach to probabilistic reasoning 7 , in order to calculate the probability of triggered events occurring based on the probabilities of the occurrence of triggering events

2. Bayesian Networks

- Explicação do uso de Bayesian Networks na previsão de eventos

3. IoT Event Prediction Architecture

- Uso de uma arquitetura onde novos eventos acionam um novo recalculo das probabilidades das dependencias

4. Bayesian Model for Flight Delay Prediction

- We developed a Bayesian event prediction model that calculates the probability of a connecting flight departing late, given the probability of the incoming flight departing late and/or the incoming flight arriving late

5. Case Study

- Uso de dados de voos durante 1 mês

- we observe that the model assigns the highest probability to the correct delay event category in 53.8 % of the cases, which is better than random.

6. Discussion and Conclusions

- A robust event prediction model requires a sufficiently large data set about historical IoT events.

- A more elaborate model including additional factors such as time of day or season, impacting flight delays could also improve the prediction accuracy


----------------------------------------------------------------------------
----------------------------------------------------------------------------
1810.00305 Resource Management in Fog/Edge Computing: A Survey
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: bom paper para justificar o uso de edge computing

Resumo: Identify and classify the architectures, infrastructure, and underlying algorithms for managing resources in fog/edge computing

I. INTRODUCTION

- Necessidade de fog computing

- A computing model that makes use of resources located at the edge of the network is referred to as ‘edge computing’ [4], [5]. A model that makes use of both edge resources and the cloud is referred to as ‘fog computing’ [6], [7]

- Possivel imagem (parcial) (fig 2) para usar no documento dissertação
Algoritmos, Arquiteturas, Infrastruturas

- Caracteristicas dos recursos na edge

II. ARCHITECTURES

- Data control (Hierarquico)
	Atualmente, o controlo de nós é centralizado, mas vai passar a ser hierarquico, com a inclusão de gestores locais
	O texto não dá mais detalhe sobre arquiteturas de controlo hierarquicas, apenas centralizadas e distribuidas

- Data tenancy (Multi-tenancy - Multiple Application, Multiple User (MAMU))
	
	Microserviços de aplicações diferentes podem estar no mesmo nó, e vários utilizadores podem aceder aos microserviços.
	É necessário: 
		1. Sistema de Virtualização (Containers Docker)
		Virtualization makes it possible to isolate resources for individual applications, whereby users can access applications hosted in a virtualized environment. For example, different containers of multiple applications may be concurrently hosted on an edge node.
		2. Network Slicing (Vertente não considerada na dissertação)

metrics: bandwidth, latency, and
energy constraints

III. INFRASTRUCTURE

- Hardware
	1. Computing devices (Single-board computers, e.g. raspberry pi, e commodity products, e.g. desktops, laptops, smartphones)
	2. Network devices (Gateway and routers, Wifi APs, Edge racks)
- Software (Apps executam dentro de Containers)
	1. VMs/Containers que executam nos nós edge e cloud
	2. Network virtualization
- Middleware
	- Hierarchical Fog/Edge Computing: A Hierarchical fog/edge computing platform provides middleware that exploits both conventional cloud computing and recent fog/edge computing paradigms. Tasks that require prompt reaction are processed in fog/edge nodes whereas complex or long-term analysis tasks are performed at more powerful cloud nodes [138], [139], [140].
	- Cloud Orchestration Management: 
	Ver referências para possivel trabalho relacionado:
	[112] R. Morabito and N. Beijar, “Enabling Data Processing at the Network Edge Through Lightweight Virtualization Technologies,” in IEEE International Conference on Sensing, Communication and Networking, 2016, pp. 1–6.
	[147] M. Selimi, A. M. Khan, E. Dimogerontakis, F. Freitag, and R. P. Centelles, “Cloud services in the guifi. net community network,” Computer Networks, vol. 93, pp. 373–388, 2015.
	[148] A. M. Khan and F. Freitag, “On Edge Cloud Service Provision with Distributed Home Servers,” in Cloud Computing Technology and Science (CloudCom), 2017 IEEE International Conference on. IEEE, 2017, pp. 223–226.
	[93] B. Amento, B. Balasubramanian, R. J. Hall, K. Joshi, G. Jung, and K. H. Purdy, “FocusStack: Orchestrating Edge Clouds Using Location-Based Focus of Attention,” in Edge Computing (SEC), IEEE/ACM Symposium on. IEEE, 2016, pp. 179–191.
	[150] D. Santoro, D. Zozin, D. Pizzolli, F. De Pellegrini, and S. Cretti, “Foggy: A Platform for Workload Orchestration in a Fog Computing Environment,” in IEEE International Conference on Cloud Computing Technology and Science, 2017, pp. 231–234.
	[151] M. Vögler, J. Schleicher, C. Inzinger, S. Nastic, S. Sehic, and S. Dustdar, “LEONORE–Large-Scale Provisioning of Resource-constrained IoT Deployments,” in IEEE Symposium on Service-Oriented System Engineering, 2015, pp. 78–87.
	[152] S. Nastic, S. Sehic, D.-H. Le, H.-L. Truong, and S. Dustdar, “Provisioning Software-Defined IoT Cloud Systems,” in International Conference on Future Internet of Things and Cloud, 2014, pp. 288–295.
	[153] S. Nastic, H.-L. Truong, and S. Dustdar, “A Middleware Infrastructure for Utility-based Provisioning of IoT Cloud Systems,” in IEEE/ACM Symposium on Edge Computing, 2016, pp. 28–40.

IV. ALGORITHMS

- Benchmarking:
	Benchmarking is a de facto approach for capturing the performance (of entities such as memory, CPU, storage, network, etc) of a computing system.
	The majority of edge benchmarking research evaluates power, CPU, and memory performance of edge processors
	Ver referências:
	[163] B. Varghese, L. T. Subba, L. Thai, and A. Barker, “DocLite: A Docker-Based Lightweight Cloud Benchmarking Tool,” in 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, 2016, pp. 213–222.
	[164] Z. Kozhirbayev and R. O. Sinnott, “A Performance Comparison of Container-based Technologies for the Cloud,” Future Generation Computer Systems, vol. 68, pp. 175 – 182, 2017

- Load-balacing
	1. Particle swarm optimization
	Ver referências:
	[171] X. He, Z. Ren, C. Shi, and J. Fang, “A Novel Load Balancing Strategy of Software-defined Cloud/Fog Networking in the Internet of Vehicles,” China Communications, vol. 13, no. 2, pp. 140–149, 2016.
	[172] K. E. Parsopoulos, M. N. Vrahatis et al., “Particle Swarm Optimization Method for Constrained Optimization Problems,” Intelligent Technologies–Theory and Application: New Trends in Intelligent Technologies, vol. 76, no. 1, pp. 214–220, 2002.
	2. Cooperative load balancing
	Ver referências:
	[173] R. Beraldi, A. Mtibaa, and H. Alnuweiri, “Cooperative Load Balancing Scheme for Edge Computing Resources,” in 2nd International Conference on Fog and Mobile Edge Computing. IEEE, 2017, pp. 94–100
	3. Graph-based balancing
	Ver referências:
	[174] S. Ningning, G. Chao, A. Xingshuo, and Z. Qiang, “Fog Computing Dynamic Load Balancing Mechanism Based on Graph Repartitioning,” China Communications, vol. 13, no. 3, pp. 156–164, 2016.
	4. Breadth-first search
	Ver referências:
	[175] D. Puthal, M. S. Obaidat, P. Nanda, M. Prasad, S. P. Mohanty, and A. Y. Zomaya, “Secure and Sustainable Load Balancing of Edge Data Centers in Fog Computing,” IEEE Communications Magazine, vol. 56, no. 5, pp. 60–65, 2018.

- Placement (Talvez seja o ponto mais relevante para esta dissertação)
	- Dynamic condition-aware techniques
	[177] S. Wang, R. Urgaonkar, T. He, K. Chan, M. Zafer, and K. K. Leung, “Dynamic service placement for mobile micro-clouds with predicted future costs,” IEEE Transactions on Parallel and Distributed Systems, vol. 28, no. 4, pp. 1002–1016, 2017
	- Iterative techniques
		- Iterative over resources
		- Iterative over the problem spaces
		[180] O. Skarlat, M. Nardelli, S. Schulte, and S. Dustdar, “Towards qos-aware fog service placement,” in Fog and Edge Computing (ICFEC), 2017 IEEE 1st International Conference on. IEEE, 2017, pp. 89–96.

----------------------------------------------------------------------------
----------------------------------------------------------------------------
GDWXT6-08031490 Where Resources meet at the Edge
----------------------------------------------------------------------------
----------------------------------------------------------------------------
Versão completa: K. Toczé and S. Nadjm-Tehrani, “Resource management at the edge: A survey”

Notas: Referências muito relevantes 25, 31, 32, 33 (como trabalho relacionado), e 26 como otimização baseada em custo

Resumo: We begin by a brief description of the edge paradigm, the most generic edge architecture, and the terminology associated to it. Then, we propose and elaborate on a preliminary taxonomy for edge resource management, together with a substantial review of works in the area. Finally, we identify some research challenges.

I. INTRODUCTION

- Explicação da necessidade da computação edge/fog, devido ao aumento de dispositivos, aumento de transferencia de dados, aplicações com QoS restritas, etc.

- In order to achieve this and to make edge computing a reality and a success, there is a need for efficient resource management at the edge.

- However, there are more areas in resource management than just offloading and this paper aims at providing a brief overview of the current work done in those areas, focusing on work where the resources are at the edge or where the resource management is performed at the edge. Therefore, work considering direct interactions from a device to a cloud [18] or from the cloud to the edge [19] are not considered. É EXATAMENTE O MESMO QUE O NOSSO SISTEMA CONSIDERA

II. TERMINOLOGY

- Figura 1 pode ser usada na dissertação para explicar a hierarquia edge/cloud computing

- edge devices are named for example
cloudlets[21], [22] or micro datacenters [23] and they can be
located for example in shops, companies or co-located with
the base stations of the telecom access network

- The edge device usually has relatively high computational power, though it remains less powerful than a conventional datacenter used in the cloud computing paradigm. In the literature, such edge devices are named for example cloudlets[21], [22] or micro datacenters [23] and they can be located for example in shops, companies or co-located with the base stations of the telecom access network

III. TAXONOMY OF EDGE RESOURCE MANAGEMENT
Possivel uso parcial da figura 2 na dissertação

A. Resource type
	1) Physical (nós)
		- Commonly considered resources are computational or storage resources (such as CPU), needed to execute the task. Other common resources are communication resources, such as bandwidth or spectrum.
		- The storage resources that can be needed for applications with more data.
		- The next resource is energy, mobile devices (and especially smartphones) are constrained by their limited battery life, and there is a rising awareness about the need to use less energy.
		- Wireless bandwidth between edge devices
	2) Virtual (containers)
		- First, virtual resources can be represented through the use of virtual machines (VMs).
		- Another approach is to abstract the physical resources into virtual resources and manage those instead

B. Objective
	1) Resource estimation
		- This is the ability to estimate how much resource will be needed to complete a task, or carry a load. 
		No caso dos containers, já é considerada a memória que é expectável que usem (em MB). Será possivel fazer o mesmo para outro tipo de métricas?
	2) Resource allocation
		- The next area is to actually allocate resources so that the task can be executed. This problem can be tackled from different perspectives: where to allocate, when to allocate, and how much to allocate.
		Several researchers approach the resource allocation problem by studying where different virtual entities such as applications or VMs should be created and executed and how they
		can be moved during execution if the new location is better.
		For example, Tärneberg et al. [32] study application placement
		[27] Y. Liu, M. J. Lee, and Y. Zheng, “Adaptive multi-resource allocation for cloudlet-based mobile cloud computing system,” IEEE Transactions on Mobile Computing, vol. 15, no. 10, pp. 2398–2410, Oct 2016.
		[25] F. Z. Yousaf and T. Taleb, “Fine-grained resource-aware virtual network function management for 5G carrier cloud,” IEEE Network, vol. 30, no. 2, pp. 110–115, March 2016.
		[31] L. Gu, D. Zeng, S. Guo, A. Barnawi, and Y. Xiang, “Cost efficient resource management in fog computing supported medical cyber-physical system,” IEEE Transactions on Emerging Topics in Computing, vol. 5, no. 1, pp. 108–119, Jan 2017
		[33] J. Plachy, Z. Becvar, and E. C. Strinati, “Dynamic resource allocation exploiting mobility prediction in mobile edge computing,” in 2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC), Sept 2016, pp. 1–6.
		PREVISÂO DE MOVIMENTAÇÃO DOS UTILIZADORES
	3) Resource sharing
		Pouco relevante para esta dissertação
	4) Resource optimization
		- Latency
		- Energy
		- Communication cost
		- Financial cost

C. Resource use
	1) Functional properties
	Resources are used in order to get the requested service, i.e. for satisfying functional properties. For example, resources can be used for video stream services [39] or crowd sensing applications [37]
	2) Non-functional properties
	For example, Fan et al. [29] consider the cost of VM migration
	Overhead of the mechanisms that provide the properties enforced by edge computing such as privacy [40] or context adaptation [41]

IV. DISCUSSION

- There is not a lot of work on the resource footprint of edge algorithms dealing with aspects such as security, privacy or context adaptation. It is especially important to research on the resource overhead for providing those properties, since a too high overhead can signify a technology which is not usable in practice.

----------------------------------------------------------------------------
----------------------------------------------------------------------------
08700543 PESS-MinA: A Proactive Stochastic Task Allocation Algorithm for FaaS Edge-Cloud environments
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: o paper foca-se na demonstração de um algoritmo de atribuição de tarefas a recursos. Não é diretamente relevante ao nosso sistema

Resumo: We propose a new allocation algorithm called PESS-MinA based on our novel modular model for FaaS Edge-Cloud environments. In contradiction to widely-used Max-Min and Min-Min algorithms which are both reactive and deterministic, this algorithm is based on stochastic score, and thus provides proactivity considerations.

A FaaS Cloud fulfills this promise by implementing two
main functionalities: Resource Auto-scaling and Task to
Resource Allocation. The first one provisions hardware
resources while the latter assigns a task to the provided VM
and allows it to use a specified amount of resource

CloudSim para simulação de testes


----------------------------------------------------------------------------
----------------------------------------------------------------------------
p25-jindal Performance Modeling for Cloud Microservice Applications
----------------------------------------------------------------------------
----------------------------------------------------------------------------

https://github.com/ansjin/terminus

Notas: o paper foca-se em encontrar o valor do nº máximo de pedidos (MSC) que um microserviço consegue suportar sem violar o seu service level objective (SLO). Esse valor pode ser útil para decidir se o microserviço deve ser replicado. No entanto, pode não ser assim tão util na edge, devido às máquinas edge serem heterogeneas, com capacidades diferentes (cpu, ram, bandwidth, etc)

Resumo:
Continuously increasing load or a sudden load spike may yield a violation of a service level objective (SLO). To characterize the behavior of a microservice application which is appropriate for the user, we define a MicroService Capacity (MSC) as a maximal rate of requests that can be served without violating SLO.
The paper addresses the challenge of identifying MSC individually for each microservice. Finding individual capacities of microservices ensures the flexibility of the capacity planning for an application. This challenge is addressed by sandboxing a microservice and building its performance model.

Automating the
forecasting procedure and the performance modeling of microser-
vices allows to automate capacity planning. In particular, predictive
autoscaling incorporates capacity planning.

- virtualização (vms e containers)

- microservices

common SLOs for interactive web applications are con-
sidered in the paper - (1) requests success rate being higher than
the threshold and (2) the response time for 90% of requests being
lower than the threshold.

Further, resource requests, limits and replicas count are used to allocate
the desired amount of resources

The maximal number of successfully processed user requests
per second for the given service such that no SLO is violated is
called Microservice Capacity (MSC)

- contém 4 microserviços usados para testar

----------------------------------------------------------------------------
----------------------------------------------------------------------------
1-s2.0-S0167739X18331868-main
----------------------------------------------------------------------------
----------------------------------------------------------------------------

FRAMES - load balacing fog resource management scheme to implmenent fog-to-fog collaboration promoting offload of incoming request

Notas: o paper foca-se maioritariamente em colaboração fog-to-fog (peer-to-peer), para distribuir o workload desejado pelos nós da arquitetura

1. introdução
- refere-se ao desenvolvimento das tecnologias de telecomunicação e como isso está ligado à adoção da computação cloud. a qual coincide com a adoção de tenicas como capital expenditures (capex), operating expenditures (opex) e o algoritmo nfv baseado em 'spectral clusteting theory'
- aparecimento de dispositivos iot (50 mil milhões em 2022) que aumenta muito o volume de dados transmitidos na rede. Embora a computação cloud seja muito útil para armazenamento e processamento de dados, não é adequada ao uso de dispositivos iot devido ao peso colocado na rede de comunicações e à latencia elevada, resultando num consumo de energia alto.
- descrição de fog. serve de complemento à cloud. pode fornecer serviços que a computação cloud não consegue, como 1. serviços sensiveis à latencia (healthcare, online gaming), 2. serviços geo-distribuidos, 3. serviços moveis (veiculos), 4. sistemas de controlo distribuidos em larga escala (distribuição de energia inteligente, sistema de gestão luzes de trafego)

1.1 problema
os nós edge, onde são alojados os serviços, ficam mais perto dos clientes, comparativamente aos centros de dados cloud, o que ajuda a diminui a latencia percetida dos seus utilizadores. Mas, os nós podem ficar congestionados devido ao grande numero de pedidos, ao exceder a sua capacidade. 

2. trabalho relacionado
Investigação em fog - heterogeneidade das maquinas, elasticidade devido ao aumento crescente de iot, federação
- "application knowledge of Com-
plex Event Processing (CEP) can reduce the required bandwidth
of Virtual Machines (VMs) during their migration"
- Abedin et al

3. f2f collaboration model
- "There are
two approaches to model interactions among fog nodes. First, the
centralised model, which relies on a central node that controls
the offload interaction among the fog nodes. Second, in which
each fog node runs a protocol to distribute their updated state
information to the neighbouring nodes. Then, each fog node holds
a dynamically updated list of best nodes that can serve the
offloaded tasks. We envisage that the distributed model is more
suitable for scenarios where things are mobile objects (i.e., In-
ternet of moving things [41]) as to support the mobility and
flexibility of data acquisition"
- "During the registration process, all device information and
capabilities of the device are required, such as, device CPU
clock, storage size, network capacity, MAC addresses identi-
fier alongside with the IP address assigned by the network
which will be used to identify the node."
- when/where to distribute workload


4. f2f coordination model
- distribuição de workload entre fog/cloud nodes baseada em probabilidades
- distinção entre heavy e light services
- calculos/formulas para determinar os valores necessários à realização da distribuição de workload. e.g. service delay (transmission delay, propagation delay, computationonal delay), fog workload, averager delay in a fog node, problem formulation and constraints, offloading model)
- formulação e restrições do problema
- quando o pedido nao pode ser satisfeito antes do service deadline, este é executado noutro fog/cloud nó

5. evaluation
- benchmark contra Random Walk Algorithm (RWA) and Neighbouring Fogs Algorithm (NFA):
1. Random Walk Algorithm (RWA) [46,47], which imposes
that arriving service requests are assigned to a nearest fog
node from source; if fog is congested it will offload the
service randomly to another fog node. In this scenario, we
assume that each fog node within the domain has the same
probability of being selected.
2. Neighbouring Fogs Algorithm (NFA) [48], which is imposes
that congested fog will offload the overload to the nearest
fog node with bigger capacity.


----------------------------------------------------------------------------
----------------------------------------------------------------------------
1-s2.0-S016412121730256X-main
----------------------------------------------------------------------------
----------------------------------------------------------------------------

2. monitoring levels
"To adapt edge computing applications to the changing execu-
tion environment and ensure application QoS requirements con-
tinue to be satisfied, it is necessary to employ a comprehensive
monitoring system able to address the whole spectrum of re-
quirements, pertaining to different levels including (1) the un-
derlying infrastructures (e.g. VM’s computing resources, etc.), (2)
edge computing platforms (e.g. Docker containers, etc.), (3) net-
work connections between individual application components and
(4) application-specific measurements (e.g. service response time,
etc.)."
- metrics: cpu, memory, disk, network usage

diferença entre vms e containers (2.1 e 2.2)

2.1 vm level monitoring (cpu, memory, disk, network) can be virtualized. with multiple containers/nodes sharing the resources of a physical machine.
metrics:
- CPU usage shows the amount of actively used CPU as a percent-
age of total available CPU in a VM. If the processor utilization
reaches 100% and the CPU run queues start filling up, the sys-
tem has run out of available processing capacity and adaptation
action must be taken at that point – or, preferably, before that
point, in anticipation.
- Memory usage indicates the percentage of memory that is used
on the selected machine.
- Disk usage refers to the amount of data read or written by a
VM. Or it can also indicate the percentage of used drive space.
Adding additional storage to the VM and allocating it to the ap-
propriate partition can often resolve disk space issues.
- Network usage is the volume of traffic on a specific network
interface of a VM, including external and internal data traffic.
- Wood et al. (2008) developed a mathematical model to esti-
mate resource overhead for a VM. The proposed model can be
adopted for approximating virtualized resource requirements (es-
pecially CPU)
- It was mentioned that if CPU, memory, and storage are overloaded, 
then the virtual servers will not be able to perform their normal function
- Caglar and Gokhale (2014) presented an autonomous, intelli-
gent resource management tool called iOverbook usable in het-
erogeneous and virtualized environments.

2.2 container level monitoring
metrics:
rx_bytes 				B/s 	Bytes received by the container
rx_packets 				Pckt/s 	Packets received by the container
tx_bytes 				B/s 	Bytes sent by the container
tx_packets 				Pckt/s 	Packets sent by the container
cpu_usage 				Float 	%CPU usage of the container
memory_usage 			KB 		%memory usage of the container
io_service_bytes_read 	B/s 	Bytes read from block device by the container
io_service_bytes_write 	B/s  	Bytes written to block device by the container
monitoring tools: 
Docker Built-in Tool
cAdvisor
cAdvisor + InfluxDB + Grafana
Prometheus
DUCP
Scout

- cAdvisor vs prometheus: However, both may not properly pro-
vide turnkey scalability themselves, capable of handling large num-
ber of monitored containers.
- Docker Universal Control Plane (DUCP) is a tool to manage, de-
ploy, configure and monitor distributed applications built using
Docker containers

2.3 link quality monitoring
- The idea that some application services are deployed on the
nodes at the edge of the network and others on centralized dat-
acenters has raised serious concerns about the network quality of
links between these services across an edge computing framework
4 types of connections:
 	1. Communications between a cloud datacenter and an edge node
 	2. Communications between edge nodes
 	3. Communications between/within cloud datacenters
 	4. Communications between IoT object/user and edge nodes
metrics (Lampe et al., 2013; Chen et al.,2014; Samimi et al., 2007; Mohit, 2010; Hsu and Lo, 2014; Taherizadeh et al., 2016a; Cervino et al., 2011 ):
Network throughput, which is the average rate of successful
data transfer through a network connection.
Network delay, which specifies how long a packet takes to
travel across a link from one endpoint or node to another. This
metric can also mean Round-Trip Time (RTT) which is the time
elapsed from the propagation of a message to a remote place
to its arrival back at the source.
Packet loss, which is when one or more packets of data travel-
ing across a network fail to reach their destination.
Jitter, which is the variation in the end-to-end delay of sequen-
tial received packets. This network parameter is extremely im-
portant for real-time applications, e.g. oil exploration or con-
nected vehicle application, as jitter impacts the size of the as-
sociated data stream buffers.

2.4. Application-level monitoring

metrics: 
response time
application throughput
a Deep Packet Inspection (DPI) monitoring tool was used to measure frames per second, dropped frames and video quality (video streaming apps)

These processes
have the same parent process and hence by listing a list of pro-
cess IDs (PIDs) for the monitored application, it is possible to sum
up the resource consumption of all processes belonging to the
application and calculate total resource consumption at a certain
point in time. (Mastelic et al. (2012))

importância do uso de metricas a nivel da aplicação:
The authors claim that monitoring only
infrastructure-level metrics such as memory and bandwidth with-
out taking into account how application performance is behaving
(application-level monitoring) at runtime would complicate the re-
source provisioning problem due to the lack of detailed measure-
ment (Rao et al. (2011))

"Islam et al. (2012) developed a proactive cloud resource man-
agement approach in which linear regression and neural networks
have been applied to predict and satisfy future resource demands.
The research problem in this work actually is to analyze time se-
ries monitoring data to extract a prediction model and other char-
acteristics of the monitoring data. The proposed performance pre-
diction model estimates upcoming resource utilization (e.g. aggre-
gated percentage of CPU usage of all running VM instances) at run-
time and is capable of launching additional VMs to maximize ap-
plication performance."

desafios na monitorização na edge:
1. mobility management
2. scalability and resource availability at the edge
3. prior knowledge
4. data management
5. coorindated decentralization
6. saving expense, time and energy
7. interoperability and avoiding vendor lock-in
8. optimal resource scheduling among edge nodes
9. fault tolerance
10. proactive computing
11. replication of services
12. container security
13. non specific edge nodes

3. taxonomia de requerimentos da monitorização em cenário de edge computing
...
live migration: Liaqat, M., Ninoriya, S., Shuja, J., Ahmad, R.W., Gani, A., 2016. Virtual machine mi-
gration enabled cloud resource management: a challenging task. Distrib. Parallel
Clust. Comput. 1–7 . https://arxiv.org/pdf/1601.03854 .

4. conclusão
However, their native scaling approaches are princi-
pally based on CPU usage; no matter for example how workload
intensity or application performance is behaving


----------------------------------------------------------------------------
----------------------------------------------------------------------------
1908.01153
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: algoritmo de placement de aplicações baseada em 3 objetivos de otimização (completion time, energy consumption, economic cost)

- The authors in [24] investigated a two-level scheduling
model that divides the Fog devices into multiple geographical
clusters (Yan Sun, Fuhong Lin, and Haitao Xu. Multi-objective optimization
of resource scheduling in fog computing using an improved nsga-ii.
Wireless Personal Communications, pages 1–17, 2018.)
- contém vários algoritmos úteis para quando for feita a implementação de algoritmos de gestão (pag 2, 3)
- usadas 3 aplicações relacionadas com medicina que podem ser uteis para nosso caso de estudo, mas não estão implementadas
- created elaborate scenarios based on a simulated fog environment (https://github.com/vindem/sleipnir)

----------------------------------------------------------------------------
----------------------------------------------------------------------------
1910.07660
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: paper foca-se na identificação de desafios para o desenvolvimento de aplicações compostas por microserviços como sistemas auto adaptativos. Aborda questões relevantes para incluir num sistema adaptativo, como design space, control loop deployment, continuous delivery e testing.

A self-adaptive system can monitor its behavior and change
its configuration or architecture at run time to preserve
or enhance its quality attributes (e.g., performance, relia-
bility, and security) under uncertain operating conditions
(e.g., varying workloads, errors, and security threats)
A. Filieri et al., “Control strategies for self-adaptive software
systems,” ACM Transactions on Autonomous and Adaptive Systems
(TAAS), vol. 11, no. 4, p. 24, 2017.

- enhance microservice quality attributes using techniques such
as planners (e.g., to select the best possible adaptation
strategy for each microservice), machine learning (e.g., to
learn new adaptation strategies from past adaptation re-
sults), reasoning under uncertainty (e.g., to cope with noisy
monitoring data), and multi-objective optimization (e.g., to
cater for multiple, possibly conflicting microservice require-
ments)

- beneficios do uso de microserviços

- descrição de uma aplicação composta por microserviços (face recognition surveillance cameras):
We envision multiple self-adaptation scenarios for this
example application. For instance:
	- The video processing service may need to dynamically change its number of deployed instances in response to load variations;
	- The video recognition service may need to dynamically change its container image, e.g., to switch to a less accurate version, under extreme load conditions;
	- The video playback service may need to dynamically
	change its quality attributes, e.g., frame rate, to cope
	with latency fluctuations;

A typical self-adaptation
control loop consists of four main activities, namely Mon-
itor, Analyze, Plan, and Execute, all sharing a common
Knowledge base, usually referred to as the MAPE-K refer-
ence model [1].
Control loops can be designed and deployed
according to different control strategies, from a single cen-
tralized control component managing the whole system, to
multiple control components managing different parts of the
system and organized in a hierarchical or fully decentralized
manner [2]. 

For instance, control loops for each of
the video surveillance application services could be de-
ployed in a fully decentralized fashion, which would in-
crease their overall reliability and scalability. However, this strategy would also make it harder to enforce application-
wide adaptation constraints, as this would require each
control loop to coordinate its actions with the other control
loops, thus reducing their decision autonomy. Similarly,
deploying control loops at the infrastructure level would
help to promote a better separation between business and
management services at run time, thus facilitating their
reuse. However, this would also make them much harder
to customize for specific adaptation needs, e.g., managing
the expected accuracy of ML services, as most control loops
provided at the infrastructure level support only a restricted
set of adaptation models and mechanisms

would be by extending the management API provided by current container
orchestration tools, like Kubernetes. For instance, one could
easily build on the rollout and rollback features provided by
Kubernetes to develop a self-adaptive service fallback mech-
anism that could be customized to meet different adaptation
requirements

From a control loop deployment perspective, as discussed
in the context of challenges C5 and C6, a microservice
system’s control components should ideally be deployed in
a fully decentralized fashion, with each microservice being
managed by its own local controller. However, this solution
would make it harder for the local controllers to monitor
and manage application-wide quality attributes. Having a
centralized controller dedicated to managing application-
level quality concerns would be a more straightforward
solution in that regard, but this would also have the down-
side of creating a single point of failure and ultimately
could compromise the application’s overall availability. In
practice, microservice developers may choose from a variety
of intermediate solutions between those two extremes, e.g.,
by logically grouping services according to their business
and/or quality affinity, and then having those services be-
ing collectively managed by independent yet application-
aware group controllers organized in a hierarchical or fully
decentralized structure.
Another important issue is the decision about whether
microservice developers should have any responsibility in
developing and managing control components, as sug-
gested above. Having control components explicit in the
design and development of a self-adaptive microservice sys-
tem may contribute to further increase the system’s overall
complexity.

importante: 5.1, 5.2, 5.3, 5.4


----------------------------------------------------------------------------
----------------------------------------------------------------------------
1912.00595
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: Aborda assuntos relacionados com Iot, edge e cloud computing.
Contém exemplos de aplicações edge relevantes para incluir nos nossos casos de estudo, mas são apenas descrições:

1. Manufacturing industry
2. Supply chain management
3. food industry
4. distributed synchronization services
5. healthcare
6. agriculture


----------------------------------------------------------------------------
----------------------------------------------------------------------------
1912.05058 Modelling and Simulation Environment for Self-Adaptive and Self-Aware Cloud Architectures
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: Útil para explicar o uso de um sistema self-adaptable (capitulo 2.2)
É um simulador de um sistema adaptativo, no entanto, pode ser útil para ver como é implementada a simulação, e aplicar conceito no nosso sistema

Resumo: this paper presents a novel modelling and sim-
ulation environment for self-adaptive and self-aware cloud architectures

1. Introdução

- Aparecimento de sistema self-adaptable, self-aware, etc 
- Necessidade de haver um sistema de simulação
 
2.1 Trabalho relacionado

- CloudSim, GreenCloud, MDCSim, iCanCloud, etc

2.2 Self adaptivity e self awareness

- MAPE-K loop

3. arquitetura sad/saw-cloudsim

- The Self-Adaptation layer is added on top of the cloud
core architecture, to model the adaptation controller of a self-adaptive software
system. Researchers and practitioners, willing to design an adaptation technique
or study the efficiency of an existing one, would need to implement their tech-
niques in this layer

- MAPE-K adaptation process

4. implementação


----------------------------------------------------------------------------
----------------------------------------------------------------------------
2006.00876 Algorithms for Computing in Fog Systems:
principles, algorithms, and Challenges
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: bom paper como referencia para fog computing. Foca-se maioritariamente em algoritmos fog, onde existe comunicação direta entre nós (peer-to-peer) e partilha de workload entre eles

Resumo: Introdução a fog computing e algoritmos fog

1. Introdução 
fog computing can be seen in various applications and
services that do not work well in the cloud paradigm
which include applications that are latency dependent such
as gaming, geo-distributed such as pipeline monitoring,
fast mobile applications such as connected vehicles and
large-scale distribution such as smart grid [2]. Other
applications where fog computing can prove to be highly
effective include augmented reality, content delivery and
caching and analytics on big data

2. concepts

- Cyber Foraging

Systems that are in place, that help mobile computers
in computational and storage tasks, are called surrogates; where these systems are not administrated by an authority
and are considered untrusted. For example, a mobile
device will detect a surrogate in their operating
environment and negotiate with the surrogate for their
resources

- Cloudlet

cloudlets are
trusted devices that are connected to the Internet which
provide a large array of resources available to nearby
mobile devices

- Similarities and differences

All the proposed technologies have the same
characteristics in common: to reduce latency, network
constraints and provide the user with a product or service
instantaneously

3. computing algorithms

Are designed related to:
heterogeneity - components can be diverse in its application
QoS management - fog systems provide improved user experience by reducing latency and processing time
scalability - implemented on a great horizontal and vertical spectrum with regards to the number of users to the number of applications. This factor should be dynamic as there may
not always be a need for a large number of fog
components in low demand situations
mobility - components can be moved and relocated if necessary while in operating conditions to perform a function (e.g. vehicles acting as fog nodes)
federation - systems need to be diverse and be able to be managed by different operators with different requirements
interoperability - components need to be interoperable with other devices from other infrastructures such as cloud and IoT systems

- utility-based scheme

- Two approaches to creating clusters included
forming a cluster to perform a task with the lowest latency
and the other to reduce the amount of power usage

- convex optimization approaches

- The local resource coordinator is selected by the nodes
in the domain through factors that include connectivity,
CPU performance, and energy usage. The coordinator then assigns tasks to
each node to maximize the resource usage of each node.
This proves to reduce latencies as well as improve energy
efficiency (referência [14])

- By adjusting the number of nodes available and
threshold values, it was proven that the lowest latency
policy was the most effective. [17]

- a dynamic load balancing method is effectively
implemented to handle resources and reduce the
consumption of node migration that is introduced by
system changes [22]


----------------------------------------------------------------------------
----------------------------------------------------------------------------
6421607 Dynamic Resource Allocation for Load Balancing in Fog Environment
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: Foca-se na distribuição de carga de aplicações iot pelos nós disponiveis (fog e cloud). É basicamente um optimization problem with multiple constraints.
Usa os requisitos das aplicações, juntamente com as caracteristicas dos nós edge/fog/cloud, para atribuir as tasks aos melhor nós uniformemente

Resumo: a dynamic resource allocation method, named DRAM, for load balancing in fog environment is proposed in this paper. 
We present a system framework for
IoT applications in fog environment and conduct the load-
balance analysis for various types of computing node;
A corresponding resource allocation method in the fog
environment is designed through static resource allocation
and dynamic service migration to achieve the load balance for
the fog computing systems;
And adequate experimental analysis is conducted to verify the performance of our proposed method

Notas:

- iot, incompatibilidade com cloud computing, e como o fog computing pode ser a solução, juntamente com tecnologias de virtualização (vms, containers)

- The resource allocation for the IoT applications should
take into account both the centralized and the geodistributed
computing nodes, and the resource schedulers and managers should choose the appropriate computing nodes to host the
fog services combined in the IoT applications through the
design of resource allocation strategies.

- In the cloud environment, the main
goal of resource allocation is to optimize the number of
active physical machines (PMs) and make the workloads
of the running PMs distributed in a balanced manner, to
avoid bottlenecks and overloaded or low-loaded resource
usage [14–17]. In the fog environment, resource allocation
becomes more complicated since the applications could be
responded to by the computing nodes both in fog and in
clouds

- we can find that the final objective
solved in this paper is an optimization problem with multiple
constraints

- When
allocating resource units for a fog service, the computing
node with the least and enough spare space is selected.
Besides, some workloads from the computing nodes with
higher resource usage are migrated to the computing nodes
with low resource usage

- The computing nodes
with the requested type, which have spare space, which could
be detected by Algorithm 2, are chosen as the candidate
resources to be provided for the fog services

- weather forecasting and traffic monitoring

- In the ever-expanding
data volume, cloud computing is difficult to provide efficient,
low-latency computing services, and fog computing is pro-
posed to complement the above shortage of cloud computing

- Compared to the remote cloud computing center, fog
computing is closer to the Internet of Things devices and
sensors. Fog computing can quickly solve lightweight tasks
with fast response. In the era of big data, with cloud comput-
ing expansion, fog computing is widely used in the medical,
transportation, and communication fields, to name a few


----------------------------------------------------------------------------
----------------------------------------------------------------------------
08354433 Microservices The Journey So Far and Challenges Ahead
----------------------------------------------------------------------------
----------------------------------------------------------------------------

container engines: LXC linuxcontainers.org, Docker www.docker.com, rkt coreos.com/rkt

service discovery: ZooKeeper zookeeper.apache.org, Eureka github.com/Netflix/eureka, etcd coreos.com/etcd, Synapse github.com/airbnb/synapse, Consul www.consul.io

Container orchestration: Mesos mesos.apache.org, Kubernetes kubernetes.io, Docker Swarm docs.docker.com/engine/swarm , Amazon Elastic Container Service aws.amazon.com/ecs, Nomad www.nomadproject.io

Sidecar: SmartStack nerds.airbnb.com/smartstack-service-discovery-cloud, Prana github.com/Netflix/Prana, Envoy www.envoyproxy.io

----------------------------------------------------------------------------
----------------------------------------------------------------------------
08498141
----------------------------------------------------------------------------
----------------------------------------------------------------------------

figura 1 boa para explicar a complexidade de sistemas auto adaptativos, devido à mudança constante de workload

Thereby,
auto-adjustment allows dynamically considering the trade-off
between adaptation speed and adaptation quality.

figura 2 Augmenting the MAPE loop with auto-adjustment


----------------------------------------------------------------------------
----------------------------------------------------------------------------
Resourceoptimizationofcontainerorchestration_acasestudyinmulti-cloudmicroservices-basedapplications_SpringerLink

Resource optimization of container orchestration: a case study in multi­-cloud microservices-­based applications
----------------------------------------------------------------------------
----------------------------------------------------------------------------

https://link.springer.com/epdf/10.1007/s11227-018-2345-2?author_access_token=F9dgH1yPMwXD92_awClLeve4RwlQNchNByi7wbcMAY6lT8kHbKxLlbWW7i_qHoyoEzJikzK87KNimWfSBHd9IxzWNKPz6ZzycaFqDZsW9aQsTQux-q0r8ptda8h32Y5CE8rDkXrFF_tb1Et0ry0CRg%3D%3D

Notas: muito relacionado com o nosso sistema. define formulas para otimizar a gestão de containers e cloud providers

Resumo:
An approach to optimize the deployment of microservices-based applications using
containers in multi-cloud architectures is presented. The optimization objectives are
three: cloud service cost, network latency among microservices, and time to start a
new microservice when a provider becomes unavailable. The decision variables are:
the scale level of the microservices; their allocation in the virtual machines; the
provider and virtual machine type selection; and the number of virtual machines


The system administrator has two management tools: one for the orchestration of the microservices and a second one for the VM management. Real examples of those tools are OpenNebula [52], for the automatization and management of VMs in multicloud providers, and HashiCorp Nomad, for the container orchestration. These tools allow the system administrator to easily deploy the application in any of the three cloud providers, first by deciding the number and type of the VMs to create and second by deciding the scale level of the containers and the VMs where they are allocated in.

The system administrator is concerned with the risk of increasing the application execution time due to the additional network latency among microservices allocated in different providers. However, he is also interested in the reduction of deployment cost and in the improvement of the application availability by considering several cloud providers. The solution is easily affordable if only one of those objectives is considered. For example, the application availability is easily maximized by creating instances of the three microservices (or their containers) in two providers, but this solution will also increase the deployment cost. In contrast, the VM and container allocation plan is not so simple if the three objectives are considered together, even for the small architecture size of the example. An external pluggable solution is required to be able to manage concurrently the VM and container management tools, since those tools are not coordinated between them. By gathering information from both components, an optimization process can be conducted, resulting in a VM and container orchestration plan that can be returned to the management tools in order to change the allocation of both VMs and containers. Thus, the decision-making process is automated and, for example, the deployment cost can be reduced without damaging the application performance or availability.

referências: 33, 1, 10, 47, 20, 50, 21

provn - The cloud provider with identification n
Ln,n′ - Latency time between providers’ data centers
vmtn,i - VM type with identification i in cloud provider n
Ravailn,i - Available computation resources in virtual machine type i
Ctotal - Total cost of the cloud services
Cn,i - User cost for the virtual machine type i
vmijn,i - Instance of a virtual machine of type i in provider n
msx,y - Microservice with identification y for application x
msizx,y - Instance z for a microservice identified as y for the application x
Rreqx,y - Computational resources required by a running container of microservice y
Rstrx,y - Computational resources required by a stored container of microservice y
MSconsx,y - The set of microservices consumed by the microservice y 
Tstartx,y - Time to start a container of the microservice y
Tdwnldx,y - Time to download the container image of microservice y
TrepairTotal - repair time for all the microservices in the systems
Trepairvm[msizx,y] - Total repair time for the microservice instance z in case of a VM failure
Trepairprovider[msizx,y] - Total repair time for the microservice instance z in case of a provider failure
U [vmijn,i] - Resource usage of the VM instance j of type i in provider n
alloc[msizx,y] - Function that returns the allocation of the microservice instance z
store[msizx,y] - Function that returns the VM where the microservice instance z is stored


- We propose optimizing: (i) the cost of the cloud services for the cloud user; (ii) the latency overhead due to the allocation of the related microservices in different cloud providers or VM instances; (iii) and, finally, the time that any microservice is unavailable due to a failure of one VM, or one cloud provider

- We propose achieving this optimization by managing: (a) the VM types and consequently the cloud providers; (b) the number of VM instances for each type; (c) the scale level of each microservices; and (d) the container allocations in the VMs.


----------------------------------------------------------------------------
----------------------------------------------------------------------------
p93-klinaku CAUS: An Elasticity Controller for a Containerized Microservice
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Resumo: 
We propose a novel heuristic adaptation process which enables elasticity for a particular containerized microservice. The proposed method consists of two mechanisms that complement each other. One part reacts to changes in load intensity by scaling container instances depending on their processing capability. The other mechanism manages additional containers as a buffer to handle unpredictable workload changes

Notas: muito relacionado com o nosso sistema. só não tem em conta a migração baseada na origem dos pedidos

- Microservices facing unpredictable workloads need to react fast and match the 
supply as closely as possible to the demand in order to guarantee
quality objectives and to keep costs at a minimum
- Current state-of-the-art approaches, that react on conditions which reflect the
need to scale, are either slow or lack precision in supplying the
demand with the adequate capacity
- Therefore, we propose a novel
heuristic adaptation process which enables elasticity for a particu-
lar containerized microservice. The proposed method consists of
two mechanisms that complement each other. 
One part reacts to
changes in load intensity by scaling container instances depending
on their processing capability. The other mechanism manages ad-
ditional containers as a buffer to handle unpredictable workload
changes

3. adaptation process

Figure 1: The MAPE-K loop of the elasticity controller

For the load intensity, on which
scaling decisions are based, we chose the rate at which the queue
has increased over the last minute. We denote this value with λ. The
average rate over the last minute is computed based on a time-series
that contains samples of the total number of published events at a
certain point in time. In our prototype implementation, we obtain
the chosen signal from Prometheus by using the rate function 6


In our prototype implementation, we obtain
the chosen signal from Prometheus by using the rate function 6
over a one-minute interval
https://prometheus.io/docs/prometheus/latest/querying/functions/#rate

We argue that this signal reflects the
necessity to scale. Having provisioned resources with a predictive
technique for units of hours, we aim to rely on a reactive mecha-
nism which bases its scaling decisions based on a signal that reflects
the load in fine-grained time units. By computing the rate over the
last minute, the signal represents the right magnitude of trends
that change in minute timescales. In cases where load spikes occur
and they level off to a new value, as it is shown in Figure 2a the
intensity reflects the right magnitude with around one minute delay.
However, when spikes of a short wavelength occur the signal used
for load intensity filters them out. As it is shown in Figure 2b, the
curve marked with triangles depicts the computed instant increase,
as a difference between two consecutive queue length samples. As
seen, the curve marked with squares which represents the average
rate over the last minute filters out the short wavelength spike that
occurred.


----------------------------------------------------------------------------
----------------------------------------------------------------------------
Improving_microservice-based_applications_with_run: Improving microservice-based applications with runtime placement adaptation
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Resumo: 
In this paper, we first identify the runtime aspects of microservice execution that impact the placement of microservices in a μApp. We then review the challenges of reconfiguring a μApp based on these aspects. Our main contribution is an adaptation mechanism, named REMaP, to manage the placement of microservices in an μApp automatically. To achieve this, REMaP uses microservice affinities and resource usage history. We evaluate our REMaP prototype and demonstrate that our solution is autonomic, lowers resource utilization, and can substantially improve μApp performance

- However, existing tools for μApps, like Kubernetes, provide minimal ability to influence the placement and utilization of a μApp deployment.

- In the bin-
packing problem, objects of different sizes must be packed
into a finite number of bins of volume V in a way that
minimizes the number of bins needed. This approach is
a combinatorial NP-Hard problem. In our context, the
objects to be packed are the microservices and the bins are
hosts of the cluster.
Unlike the classical bin-packing problem, the place-
ment of microservices in a cluster cannot consider only
one dimension, but the microservices affinities and their
resources usage, e.g., CPU, memory, disk, and so on.
Therefore, our problem is a multi-dimensional variation
of bin-packing [21] that is exponentially harder to solve.
The formal statement of the microservice placement
problem is stated as follows:
Given a set of hosts H 1 , H 2 , · · · , H m and a set of
μApps P 1 , P 2 , · · · , where P i is a set of n microservices
m P i ,1 , m P i ,2 , · · · m P i ,n linked by the affinity function A :
m P i → m P i . Find an integer number of hosts H and a H-
partition H 1 ∪ · · · ∪ H B of the set {1, · · · , n} such that the

of the multi-attributes microservices m P i ,j fits on H k for
all k = 1, · · · , B, i = 1, · · · , P, and j = 1, · · · , n P i . A solu-
tion is optimal if it has minimal B and maximum affinity
score for all H k .

There are several approaches surveyed in [20, 22] to
compute this optimization in an offline way. At runtime,
the best strategies are approximations calculated through
heuristics and evolutionary algorithms to achieve a quasi-
optimal solution.

- Existing management tools have simple primitives used
to move a microservice across different hosts. However,
due to a limitation of existing operating systems and
frameworks, it is not possible to live-migrate processes
(microservices) between machines. As a workaround, the
movement of a microservice can be emulated by a three-
step sequence:
1. Instantiate the microservice replica at the new
location,
2. Wait for the microservice to become ready, i.e., load
its libraries and be able to handle requests, and
3. Remove the microservice replica from the previous
location.

- Hence, in addition to the minimization of resource usage, we also aim to
maximize the affinity score of the selected hosts, i.e., placing the maximum number of highly-related microservices
together.

- We define affinity between
two microservices using the number of messages and the
amount of data exchanged between them. We use a ratio
(weight) in the affinity calculation to steer the Analyzer
execution

types of microservices a and b as A a,b and calculate it as
follows:
A a,b = (ma,b / m) * (w + da,b / d) * (1 - w)
where,
– m is the total of messages exchanged by all
microservices,
– ma,b is the number of messages exchanged between
microservices a and b,
– d is the total amount of data exchanged by all
microservices,
- da,b is the amount of data exchanged between
microservices a and b,
– w is the weight, such that {w ∈ R | 0 ≤ w ≤ 1}, used
to define which variable is the most important to
compute the affinity, i.e., number of messages or
amount of data exchanged.

- can compute an optimization for
μApps using async communication since, like data stores,
the messaging middleware (e.g., RabbitMQ) is wrapped
into a container. In this case, the analyzer can identify
which microservices have a high communication rate and
may co-locate them with the middleware

- We propose two Planners to compute the placement of
microservices during an adaptation: the Heuristic-based
Affinity Planner (HBA) and the Optimal Affinity Planner
(OA). Both planners compute a new placement for μApps
by reducing this problem to a multi-objective bin-packing
problem

- As this problem is NP-Hard, we know that for
large μApps an optimal approach is infeasible. Hence,
we implement the heuristic version (HBA) to achieve
approximate solutions for large μApps and the optimal
version (OA) to achieve an optimal solution for small
μApps.

- The heuristic planner (HBA) reorganizes the placement
of microservices that make up a μApp in a cluster

- Planner OA optimizes the placement of μApps. The optimization is calculated by using a SAT
solver [23]


----------------------------------------------------------------------------
----------------------------------------------------------------------------
ucc19 - Microservices-based IoT Application Placement within
Heterogeneous and Resource Constrained Fog Computing
Environments
----------------------------------------------------------------------------
----------------------------------------------------------------------------

Notas: multi level, hierarchical Fog architecture where each
Fog computing node is responsible for processing application place-
ment requests.

Resumo: we propose a
decentralized microservices-based IoT application placement policy
for heterogeneous and resource constrained Fog environments. The
proposed policy utilizes the independently deployable and scalable
nature of microservices to place them as close as possible to the data
source to minimize latency and network usage. Moreover, it aims
to handle service discovery and load balancing related challenges
of the microservices architecture

1. introdução 

- boa explicação sobre a necessidade de ser preciso edge computing, devido ao nº crescente de dispositivos iot, impossibilidade de ser usada a cloud, etc

- boa explicação sobre a associação entre microserviços, containers, e dispositivos fog

2. trabalho relacionado

- Taneja et al. [18] present a resource aware module mapping algorithm for placement of distributed applications within Fog environments. 
This work tries to optimize resource utilization by sorting application modules and nodes based on required resources and available capacity respectively and mapping sorted modules to resources. 
The proposed algorithm is compared with Cloud-only placement to depict the reduction of end-to-end latency in the Fog placement approach. 
This work defines a hierarchical Fog architecture where each Fog node is connected with a node in immediate upper layer of the hierarchy. 
Horizontal connections among Fog nodes of the same level are not defined. Moreover, placement is managed through a centralized approach.
(Mohit Taneja and Alan Davy. 2017. Resource aware placement of IoT application
modules in Fog-Cloud Computing Paradigm. In Proceedings of the 2017 IFIP/IEEE
Symposium on Integrated Network and Service Management (IM). IEEE, 1222–1228.)

- Gupta et al. [9] propose a centralized edge-ward module placement algorithm for placing distributed applications modeled as Directed Acyclic Graphs (DAG). 
Their algorithm commences the placement of application modules starting from lower level Fog nodes and move upwards the hierarchy until a node with enough resources is met. 
But, their proposed algorithm supports only the vertical scaling of modules and does not consider horizontal connections among Fog nodes within the same Fog level.
(Harshit Gupta, Amir Vahid Dastjerdi, Soumya K Ghosh, and Rajkumar Buyya. 2017. iFogSim: A toolkit for modeling and simulation of resource management
techniques in the Internet of Things, Edge and Fog computing environments. Software: Practice and Experience 47, 9 (2017), 1275–1296.)

- Latency aware placement of application modules within Fog environments is presented in R. Mahmud et al [14]. 
This work, proposes a decentralized module placement method that considers service access delay, 
service delivery time and internodal communication delay when placing application modules within Fog environments.
In this approach, distributed applications consisting of interdependent modules are placed and forwarded vertically and horizontally 
to satisfy the latency requirements of the application while optimizing resource usage. But horizontal scaling of modules within Fog layer, microservices architecture and related challenges are not considered in this work.
(Redowan Mahmud, Kotagiri Ramamohanarao, and Rajkumar Buyya. 2018. Latency-aware application module management for fog computing environments. 
ACM Transactions on Internet Technology (TOIT) 19, 1 (2018), 9.)

- Filip et al. [6] present a centralized placement approach for Edge-Cloud scheduling of microservices using Bag of Tasks (BoT) model where each task consists of one or more microservices. 
In the proposed architecture, nano data centers are used as Edge resources. Scheduling engine receives jobs and assigns them to VMs in the nano data centers or Cloud. 
Their scheduling policy places all microservices of a certain job within the same processing element or move it towards the Cloud, based on resource availability.
(Ion-Dorinel Filip, Florin Pop, Cristina Serbanescu, and Chang Choi. 2018. Microservices scheduling model over heterogeneous cloud-edge environments as support for iot applications. 
IEEE Internet of Things Journal 5, 4 (2018), 2672–2681)

- A centralized throughput aware placement algorithm for microservices-based Fog applications is presented in [5]. 
The proposed system consists of Edge servers that are grouped together based on their geographical regions. 
In this work, application microservices are placed within the region that contains respective IoT device or within the neighboring region. 
A greedy algorithm is presented for mapping these microservices onto Edge servers with sufficient computational resources while ensuring that bandwidth of the involved
links can satisfy the throughput requirements of the application microservices.
(Francescomaria Faticanti, Francesco De Pellegrini, Domenico Siracusa, Daniele Santoro, and Silvio Cretti. 2019. 
Cutting Throughput with the Edge: App-Aware Placement in Fog Computing. 
In Proceedings of the 2019 6th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/ 2019 5th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom). 196–203.)

- Santoro et al. [16] implement a framework and a software platform for orchestration of microservices-based IoT application workloads. 
In the proposed architecture, applications are modeled as a collection of microservices distributed via container images. 
The proposed architecture consists of IoT device layer, Edge gateways, Edge cloudlets and Cloud. 
Microservice deployment requests are sent towards a negotiator that accepts or rejects the requests. 
Orchestrator calculates the most suitable device to deploy microservices of the accepted requests, based on requirements (CPU, RAM, bandwidth, etc.) defined in deployment requests.
(Daniele Santoro, Daniel Zozin, Daniele Pizzolli, Francesco De Pellegrini, and Silvio Cretti. 2017. Foggy: a platform for workload orchestration in a fog computing environment. 
In Proceedings of the 2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom). IEEE, 231–234.)

3. System model and problem formulation

- Since microservices that
make up an application have data dependencies amongst them, an
IoT application is depicted as a Directed Acyclic Graph (DAG) [18].
In this model, each microservice is represented by vertices of the
DAG, whereas edges between vertices represent data dependencies
among microservices.

- In the server-side load balancing
approach, a dedicated load balancer component lies between client
microservices and server side microservices. This component is
responsible for service discovery and directing the requests accord-
ing to a load balancing policy. However, in a highly distributed environment such as Fog, using such centralized load balancing
approach is not efficient

- Due to the highly distributed and hierarchical nature of the Fog
architecture server-side service discovery is not suitable. In both
approaches having a separate centralized service registry adds an
extra overhead to the load balancing and routing process. Moreover,
in the above described client-side approach, client would have to
communicate with service registry before every API call which
results in a large number of messages flowing among them.

- Weighted Round Robin method for load balancing where
weighting is done based on the amount of resources allocated for
each available service instance


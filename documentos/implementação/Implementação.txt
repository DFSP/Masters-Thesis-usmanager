Implementação



. Sistema de gestão dinâmico

O sistema de gestão está dividido em 3 componentes principais (manager-hub, manager-master, manager-worker), 2 módulos (manager-database, manager-services), e 6 componentes de apoio (registration-server/registration-client/loadbalancer/basic-auth-proxy/prometheus/request-location-monitor)
Foi desenvolvido um dockerfile para cada componente, para que possam ser lançados num contentor.
Os módulos estão disponiveis através de releases no github.

. Componentes principais

	Manager hub

    É uma aplicação web que permite visualizar o progresso do sistema e interagir manualmente com o mesmo.
    Comunica com o manager-master através de rest, com troca de mensagens em formato json, para obter/adicionar/alterar/remover entidades da base de dados, 
    bem como acionar ações manualmente.
    Usa react-redux para gerir o estado dos dados.
    Completamente funcional, com login/logout usando basic authentication, feedback de ações e erros ao utilizador, esquema de cores escuro/claro, filtro de dados, etc.
    Inclui componentes genericos como listas, botões, form, modals, etc, para ser fácil a sua extensão no futuro.

    	. Funcionalidades 
    
    	 Página principal

    	 	Mapa mundo interativo com a informação sobre os contentores, nós e hosts incluidos no sistema. Atualiza automáticamente a cada 15 segundos 

    	 Aplicações

    	 	Listagem das aplicações registadas
    	 	Adicionar/remover/alterar aplicações
    	 	Ver os dados individuais de cada aplicação
    	 	Associar serviços/regras/métricas simuladas a uma aplicação
    	 	Ver as regras e métricas genéricas que são aplicadas a todas as aplicações

    	 Serviços
    	 	
    	 	Listagem de todos os serviços
    	 	Adicionar/remover/alterar serviços
    	 	Ver os dados individuais de cada serviço
    	 	Associar aplicações/regras/métricas simuladas a um serviço
    	 	Ver as regras e métricas genericas que são aplicadas a todos os serviços
    	 	Adicionar dependencias e previsões

    	 Contentores

    	 	Listagem de todos os contentores a executar no sistema
    	 	Lançar novos contentores num host especifico ou numa coordenada. Ao lançar numa coordenada, é incluido um mapa interativo onde o utilizador pode escolher a localização. O algoritmo a executar no manager-master escolhe o host melhor para lançar esse serviço
    	 	Ver os dados individuais de cada contentor
    	 	Associar regras/métricas simuladas a um contentor
    	 	Ver as regras e métricas genericas que são aplicadas a todos os contentores

    	 Hosts 

    	 	cloud

    	 		Listagem de todas as instâncias cloud registadas no sistema
    	 		Parar/terminar/iniciar instâncias dependendo do seu estado
    	 		Iniciar novas instâncias usando um mapa interativo
    	 		Ver os dados individuais de cada instância
    	 		Associar regras/métricas simuladas
    	 		Ver as regras e métricas genericas aplicadas a todos os hosts
    	 		Executar commandos ssh às instâncias a executar
    	 		Carregar ficheiros através do protocolo sftp
    	 		Ativar uma sincronizar da base de dados

    	 	edge

    	 		Listagem das máquinas edge no sistema
    	 		Adicionar/remover máquinas
    	 		Ver os dados individuais de cada máquina
				Associar regras/métricas simuladas
				Ver as regras e métricas genericas aplicadas a todos os hosts
    	 		Executar commandos ssh às instâncias a executar
    	 		Carregar ficheiros através do protocolo sftp

    	 Nós

    	 	Adicionar/remover nós do swarm
    	 	Alterar o estado dos nós (active, pause, drain)
    	 	Alterar a função de nós (manager, worker)
    	 	Ver as labels associadas

    	 Regiões

    	 	Ver as regiões definidas pelo sistema

    	 Regras

    	 	Contém páginas para definir condições e regras aplicadas a hosts/aplicações/serviços/contentores.
    	 	Permitir fazer a listagem, adicionar/atualizar/remover condições ou regras, associar as condições às regras, 
    	 	e, caso não sejam do tipo genérica, associar as regras aos hosts/aplicações/serviços/contentores

    	Métricas simuladas

    		Contém páginas para listar e definir as métricas simuladas. Caso não seja do tipo generica, permite fazer associações aos hosts/aplicações/serviços/contentores.

    	Balanceamento de carga

    		Listar e gerir os contentores contendo serviços de balanceamento de carga
    		Parar e lançar novos contentores em regiões ou hosts especificos

    	Servidores de registo

    		Listar e gerir os contentores contendo servidores de registo
    		Parar e lançar novos contentores em regiões ou hosts especificos

    	Gestores locais

    		Listar e gerir os contentores contendo gestores locais
    		Parar e lançar novos contentores em regiões ou hosts especificos
    		Permite fazer atribuir hosts a workers

    	Secure shell

    		Execução de comandos ssh aos hosts no sistema
    		Carregamento de ficheiros através do protocolo sftp

    	Logs

    		Listagem das logs obtidas do manager master

	Manager master

	É o componente que faz a gestão principal do sistema.
	É o ponto de entrada para o lançamento e configuração inicial do sistema:
		- preenche a base de dados com os valores necessários
		- configura o docker swarm inicial, sendo o host onde é lançado o nó principal
		- lança e configura as instâncias na cloud iniciais, juntado-as ao swarm
		- aloca um elastic ip por cada região, para ser associado ao registration server
		- inicia a monitorização dos containers de todo o sistema
		- inicia o thread para sincronização das instâncias cloud da base de dados
		- configura o serviço symetricds, que é usado para sincronizar a informação entre as base de dados do manager e os workers, e vice-versa
	Durante a execução:
		- monitoriza os containers, obtendo as suas métricas e aplicando as regras para decidir se deve migrar contentores entre regiões
		- executa os pedidos do manager hub
	Ao terminar:
		- pára o serviço symetricds
		- pára a monitorização
		- pára todos os worker managers, que por sua vez executam os passos para terminar a sua execução
		- termina todos os containers no docker swarm
		- termina todas as instâncias cloud
		- Dealoca os endereços ip elasticos da amazon
		- Destroi o seu docker swarm
		- Apaga a informação na base de dados sobre a monitorização

	Manager worker

		Executam numa dada região geográfica, não comunicam diretamente com o manager, apenas alteram e observam alterações na base de dados. A sincronização da base de dados feita pelo symetricds permite que haja uma comunicação indireta.

		- monitoriza os hosts, obtendo as suas métricas e aplicando regras para decidir se deve diminuir ou aumentar o número de hosts a executar
		- monitoriza os containers, obtendo as suas métricas e aplicando as regras para decidir se deve replicar/migrar contentores
		- monitoriza o estado dos contentores e, em caso de falha, tenta restaurá-los num local próximo

. Módulos

	Manager database

		Modulo maven que define as entidades e tabelas da base de dados partilhada pelo manager master e manager worker.

	Manager services

		Modulo maven que define os serviços partilhados pelo manager master e manager worker.

. Componentes de apoio

	Basic auth proxy

		Protege o api do docker com basic authentication

	Registration server

		O registration server integra o eureka da netflix para permitir registar e descobrir serviços. É executado um em cada região onde estão a executar serviços.
		É executado num contentor e só pode ser lançado uma instância cloud. Isto porque o ip de todos os registration servers deve ser conhecido no inicio da execução do sistema, para que consigam comunicar entre si. Foram usados elastic ips da amazon para obter 1 ip estático por região.

	Registration client

		Aplicação desenvolvida em go contendo um servidor http com 4 métodos.
		Regista o serviço no Registration Server, e obtém os endpoints dos outros serviços, também registados no servidor.
		Envia periodicamente a monitorização para o serviço location-request-monitor
		Usa as distâncias entre os endpoints no algoritmo de escolha. Existem 3 tipos de algoritmos possiveis: sempre o serviço mais próximo, alternar aleatoriamente entre um certo número de serviços mais próximos, e alternar entre serviços com a procura a uma certa distância e duplicando caso não encontre nenhum
		A aplicação executa no contentor do microserviço, expondo ao mesmo uma api com 4 métodos:
			**Post**  /register                     		| Regista o serviço no servidor eureka
			**Get**   /services/{service}/endpoint          | Obtém o endpoint mais próximo do serviço {service}
			**Get**   /services/{service}/endpoint?number=x | Obtém um endpoint aleatório para o serviço {service} entre os x serviços mais perto
			**Get**   /services/{service}/endpoint?range=d  | Obtém um endpoint aleatório para o serviço {service} entre x serviços começando a procura à distância de 100km
			**Get**   /services/{service}/endpoints         | Obtém todos os endpoints registados em nome do serviço {service}
			**Post**  /metrics                              | Adiciona uma nova monitorização deste endpoint. Request body: {service, latitude, longitude, count}

		Registration client apis

			Apis em Java, Go, C++, C#, Python e Node que é incluida num microserviço implementado na respetiva linguagem. É o código que traz as funcionalidades do registration client para dentro do microserviço. As apis estão disponiveis através do github, onde depois são descarregadas, instaladas na imagem docker e importadas no código dos microserviços.




	Request location monitor

	Load balancer

	Prometheus

		Node exporter


Algoritmos de migração e eplicação de contentores

Algoritmos de escolha do melhor host para executar um contentor





. Funcionalidades


	Aplicações

	Serviços

	Regras

	Métricas simuladas

	Sincronização


Gestão de instâncias cloud e máquinas edge



Migração/replicação de contentores docker baseando-se nos pedidos do serviços, carga dos nós, metricas, regras, etc



Manager-master: é o componente principal que faz toda a gestão do sistema (hosts/containers/métricas/workers/etc) e contém a base de dados principal. Define endpoints rest para comunicar com o manager-hub

Manager-worker: é o componente de gestão secundário que pode ser lançado mais próximo da edge. Gere um subconjunto de hosts/containers que é atribuído pelo manager-master (federação). Sincroniza com o manager-master através da sua própria base de dados (sincronizada pelo symmetric-ds, abordado mais à frente)

Manager-database: é um módulo maven que contém a definição das tabelas de base de dados e entidades que são usadas pelo manager-master e manager-worker

Manager-services: é outro módulo maven que contém os serviços partilhados entre o manager-master e manager-worker


- O symmetric-ds é um serviço que permite sincronizar as base de dados entre os manager-workers e o manager-master. Detecta mudanças no transaction log das base de dados e comunica através de rest para efetuar a sincronização. É open-source https://github.com/JumpMind/symmetric-ds, foi a melhor solução que encontrei para a sincronização. Também foi considerado e testado o uso de kafka juntamente com kafka jdbc connector, mas não era possível detectar os deletes

- O sistema agora gere edge hosts, e não apenas instâncias cloud. Para adicionar um novo edge host, é preciso indicar a password de acesso, mas após isso é configurado uma conjunto de chaves privada/pública que permite aceder ao edge host sem ser usada a password

- As instâncias cloud agora podem ser lançadas em qualquer região suportada pela amazon, um total de 21 regiões. Antes apenas era possível numa única zona (us-east-2 que fica em ohio (usa))

- Agora existem 5 tipos de regras: host genericas (aplicadas a todos os hosts), host individial (aplicado a um só host), serviço genericas (aplicadas a todos os serviços), serviço individual (aplicada a um só tipo de serviço), app individual (aplicadas aos serviços de uma app)

- Os componentes de apoio (registration-server (eureka-server) e load-balancer) agora podem ser lançados em 7 localizações diferentes, mais ou menos 1 por continente

- Todos os componentes têm dockerfiles atualizados para gerar a respectiva docker image. O repositório no github foi ligado ao docker-hub-usmanager para que as imagens sejam atualizadas automaticamente após cada commit

- Incluí outra app que tem 4 microserviços novos. Vou tentar encontrar mais até à altura dos testes

- O docker swarm do manager-master agora permite múltiplos manager nodes. E cada manager-worker tem o seu próprio docker swarm. Com múltiplos docker swarms no sistema, foi preciso guardar a informação dos containers e hosts na base de dados. A informação entre docker swarms e base de dados é sincronizada no ciclo de monitorização dos containers (que acontece a cada 30 segundos). A sincronização da base de dados com as instancias cloud na aws é atualizada no ciclo de monitorização dos hosts

- Introduzi um pouco de paralelismo na inicialização/finalização do sistema, por exemplo no lançamento das instâncias cloud iniciais. Ainda vou tentar paralelizar a inicialização de uma app, mas é mais complexo devido às dependências entre os seus microserviços
A recolha dos dados de monitorização (pedidos ao request-location-monitor) também deixou de ser sequencial e passou a ser paralelo. A diferença de tempo pode ser substancial, por exemplo com 100 nós, se cada demorar em média 100 ms, passou de um processamento de 10 segundos para 100ms + overhead de alguns ms

- O manager-master ao terminar agora para o sistema todo (termina containers, termina instâncias cloud, destrói o docker swarm, etc). Os manager-workers fazem o mesmo para o seu subsistema

- Os algoritmos de seleção estão a ser alterados para deixarem de usar comparações entre strings continentes/regiões/cidades e passarem a usar coordenadas (latência/longitude). Com as coordenadas é possível calcular a distância e obter uma escolha mais genérica e acertada. Para a escolha do melhor host é usado um algoritmo (de geometria esférica) que calcula a distância entre as 2 coordenadas com até cerca de 10km de erro. Para a escolha do local de migração de containers, é usado um algoritmo que calcula a coordenada média com peso, ou seja, tem em conta o nº de acessos ao serviço e a sua proveniência
A comparação entre continentes/regiões/cidades era muito limitado porque havia um nº fixo de regiões/continentes, e bastava os nomes estarem em línguas diferentes (e.g. lisboa/lisbon) para que o algoritmo deixasse de ser eficaz
O próximo passo é usar a latência em vez de distância, mas vou deixar isso como trabalho futuro, porque é uma alteração mais complexa, que requer ainda mais mudanças no sistema

- Também alterei o componente request-location-monitor para suportar vários tipos de algoritmos de seleção, todos relacionados com a distância. Dependendo do microserviço, este pode escolher qual o melhor para o seu uso. A escolha é feita através de queries no url, e.g. http://...?closest (escolhe o serviço mais próximo) ou http://...?range=100&factor=2 (tenta encontrar algum serviço a menos de 100km, depois duplica por 2, para 200, 400, 800km, etc)

- Ainda não comecei a escrever o documento final, mas cada componente no repositório git tem um readme.md a descrever o que faz, o que pode ajudar. Também analisei alguma bibliografia, incluindo aquela que recomendou na última reunião. Guardei as partes importantes e alguns resumos para ajudar na escrita do documento

- Monitorização de hosts e serviços quase completamente paralela, desde a recolha de métricas, ao processamento das decisões. A parte da execução não é paralela porque a escolha para onde migrar/replicar os containers depende de outras escolhas

- Foram adicionadas regras e métricas simuladas ao nível de uma aplicação. São aplicadas a todos os containers que pertençam a essa aplicação.
A ordem de aplicação das regras/métricas é a seguinte: aplicação -> serviço -> container. Ou seja, regras/métricas ao nível do container sobrepõem as definidas ao nível do serviço, e as de serviço sobrepõem aquelas definidas ao nível da aplicação.

- Agora é possível ter containers e instâncias cloud exteriores ao sistema a executar no mesmo ambiente que o usmanager. Os recursos geridos pelo usmanager são isolados através de uma tag us-manager=true

- Existe agora a definição de regiões, mais um menos 1 por continente. Cada recurso (container/host) é associada a uma região automaticamente através da proximidade ao seu ponto central. Isto permite ter associações entre recursos -> componentes de apoio (abordado no próximo ponto)

- Os componentes de apoio (load-balancer, registration-server, worker-manager) estão disponíveis 1 por região. Isso é feito dinamicamente ao ser detectada a presença de containers numa dada região. Se não forem detectados containers numa região, existe um delay de 5 minutos antes de remover o componente para evitar paragens/reinícios num curto espaço de tempo

- O node exporter agora é configurado automaticamente em cada host. Este processo é feito na fase de configuração do nó, após este entrar no docker swarm.
Isto necessitou que fosse desenvolvido código para executar comandos bash/ssh assíncronos, que executam em background, porque o node exporter é executado como um processo e não como container

- Adição de uma nova query ao prometheus, node_filesystem_avail_bytes. Esta métrica será usada para ver se o host tem espaço suficiente no disco para extrair a imagem docker que poderá receber, caso tenha que executar um container desse serviço.
O código também foi otimizado para ser mais fácil adicionar novas métricas no futuro

- As instâncias cloud agora são baseadas numa imagem (ami) configurada com:
  - Imagens docker dos containers que executam em cada máquina (docker-api-proxy, prometheus, request-location-monitor)
  - Scripts para instalar e remover o docker e node_exporter
Isto permite que a configuração de uma instância cloud nova seja muito mais rápida. Porque não precisa de transferir as imagens docker e os scripts necessários, que são bastantes megabytes, o que demorava vários minutos até ser transferido tudo e acabar a configuração

- O loadbalancer foi atualizado para deixar de usar a região na escolha do servidor, visto que todos as réplicas registadas estão na sua região.
Aproveitei também para adicionar suporte a vários serviços e não apenas um. Isso é gerido pelo load-balancer-api, que configura dinamicamente o ficheiro de configuração do nginx. O api teve portanto que ser alterado para suportar as novas operações.
Também foi atualizado para usar a versão 2 do maxmind geoip, visto que a versão 1 foi descontinuada e as bases de dados já não são atualizadas. Este passo deu bastante trabalho porque o módulo geoip2 já não vem incluido com o nginx, então tem que ser instalado como um módulo dinamico (https://github.com/leev/ngx_http_geoip2_module)

- Adicionadas novas apps:
Online-boutique (11 microservices), é o conjunto de microserviços que a google usa para mostrar as capacidades do kubernetes, serviços gcloud, etc.
Conjunto de apps deathstarbenchmark. Contém 3 apps: hotel reservation (8 microservices), mediamicroservices (13 microservices), socialnetwork (12 microservices)
Estas apps ainda não foram modificadas para funcionarem corretamente no sistema

- Adicionados os clients api para registar serviços implementados em c# e c++, feito com o swagger codegen. Isto foi preciso porque as apps referidas no ponto anterior contém microserviços implementadas nessas linguagens. E até agora só tínhamos microserviços em java e go

- Paralelização da configuração de uma instância cloud nova

- Possibilidade de lançar manualmente um container usando localização. O algoritmo escolhe o host com capacidade mais próximo

- Sistema de recuperação de containers. É executado na fase de monitorização de serviços. Deteta containers que falharam e tenta restaurá-los no local mais próximo possível. A recuperação é abortada se for detetado que o container falhou 3 ou mais vezes nos últimos 10 minutos. Isto para evitar que um container estragado continue a ser reiniciado num loop infinito








Sincronização da base de dados
No manager:
 - Instâncias cloud são sincronizadas com a amazon (intervalo de 45 segundos)
Nos workers:
 - Contentores e nós são sincronizados com o seu respetivo docker swarm (intervalo de 10 segundos)

Paralelização das operações. Qualquer operação que envolva execuções a api externas são feitas em paralelo com o uso das apis Future e Executor do java
No master está configurado com 5 threads e 100 queue size. Nos workers com 2 threads e 100 queue size.
Inclui:
 - Amazon (instâncias/elastic ips)
 - Load balancers (obtenção dos servidores registados de cada serviço em cada região)
 - Location request monitor (obtenção da informação relativa aos pedidos externos dos serviços)
 - Prometheus (obtenção das métricas dos nós)
 - Sincronização manual dos contentores e nós geridos pelos manager workers

Alocação, associação, desassociação e dealocação de elastic ips da amazon. 
Um elastic ip é um ipv4 estático que pode ser associado a uma instância ec2. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html
É alocado 1 por cada região no inicio da execução do sistema e são atribuidos às instâncias que executam os registration servers.
Os ips têm que ser conhecidos no inicio da execução do sistema porque o eureka server não permite alterar dinamicamente a sua configuração, e portanto o conjunto de ips atribuidos aos registration servers não pode ser dinamico. 
Com esta configuração, a lista de ips conhecidos à partida são passados aos registration servers através do ambiente do respetivo contentor. O que permite que exista partilha da informação sobre os  serviço registados em cada registration server. https://github.com/Netflix/eureka/wiki/Deploying-Eureka-Servers-in-EC2

